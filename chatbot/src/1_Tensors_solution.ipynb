{"cells":[{"cell_type":"markdown","id":"Q4vVUgxxDJD0","metadata":{"id":"Q4vVUgxxDJD0"},"source":["# <font color = 'dodgerred'>**Solution HW1** </font>\n","\n"]},{"cell_type":"code","source":["import torch\n","import time"],"metadata":{"id":"9EkUQ3dGQpJ_"},"id":"9EkUQ3dGQpJ_","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"2Voz6l5RUZRh","metadata":{"id":"2Voz6l5RUZRh"},"source":["# <font color = 'dodgerred'>**Q1 : Create Tensor (1/2 Point)**\n"," Create a torch Tensor of shape (5, 3) which is filled with zeros. Modify the tensor to set element (0, 2) to 10 and element (2, 0)  to 100."]},{"cell_type":"code","execution_count":null,"id":"bI9MwTaamhis","metadata":{"id":"bI9MwTaamhis"},"outputs":[],"source":["my_tensor = torch.zeros((5,3))"]},{"cell_type":"code","execution_count":null,"id":"7QEV2uq-yqZK","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":572,"status":"ok","timestamp":1706029142947,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"7QEV2uq-yqZK","outputId":"05662e9e-3861-4a0c-917a-b57ed6bb7721"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([5, 3])"]},"metadata":{},"execution_count":10}],"source":["my_tensor.shape"]},{"cell_type":"code","execution_count":null,"id":"yAxzhRVGyr6c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706029144070,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"yAxzhRVGyr6c","outputId":"1f5959ae-2fc3-4924-aa2f-d8db932d8f23"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.]])"]},"metadata":{},"execution_count":11}],"source":["my_tensor"]},{"cell_type":"code","execution_count":null,"id":"FCM4GuyWylQV","metadata":{"id":"FCM4GuyWylQV"},"outputs":[],"source":["# Manually set the value at the first row and third column to 10,\n","# and the value at the third row and first column to 100 in the tensor named \"my_tensor\".\n","my_tensor[0,2], my_tensor[2,0] = 10, 100"]},{"cell_type":"code","execution_count":null,"id":"ZdEftwuPyyx9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1706029196480,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"ZdEftwuPyyx9","outputId":"58e8c68e-1e71-47ef-e96a-98d2b82360db"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[  0.,   0.,  10.],\n","        [  0.,   0.,   0.],\n","        [100.,   0.,   0.],\n","        [  0.,   0.,   0.],\n","        [  0.,   0.,   0.]])"]},"metadata":{},"execution_count":13}],"source":["my_tensor"]},{"cell_type":"markdown","id":"6Eoa6rF80ZTa","metadata":{"id":"6Eoa6rF80ZTa"},"source":["# <font color = 'dodgerred'>**Q2: Reshape tensor (1/2 Point)**\n","You have following tensor as input:\n","\n","```x=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])```\n","\n","Using only reshaping functions (like view, reshape, transpose, permute), you need to get at the following tensor as output:\n","\n","```\n","tensor([[ 0,  4,  8, 12, 16, 20],\n","        [ 1,  5,  9, 13, 17, 21],\n","        [ 2,  6, 10, 14, 18, 22],\n","        [ 3,  7, 11, 15, 19, 23]])\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"id":"LDBQ6kYZ-JRV","metadata":{"id":"LDBQ6kYZ-JRV"},"outputs":[],"source":["x=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])"]},{"cell_type":"code","execution_count":null,"id":"b6_SdFS-ke_x","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706029208196,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"b6_SdFS-ke_x","outputId":"03050efe-85e4-4262-c287-c4339b5fc210"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0,  4,  8, 12, 16, 20],\n","        [ 1,  5,  9, 13, 17, 21],\n","        [ 2,  6, 10, 14, 18, 22],\n","        [ 3,  7, 11, 15, 19, 23]])"]},"metadata":{},"execution_count":15}],"source":["x = x.view(6, 4).permute(1,0)\n","x"]},{"cell_type":"markdown","id":"dnw5qi7A4ysc","metadata":{"id":"dnw5qi7A4ysc"},"source":["# <font color = 'dodgerred'>**Q3: Slice tensor (1 Point)**\n","\n","- Slice the tensor x to get the following\n",">- last row of x\n",">- fourth column of x\n",">- first three rows and first two columns - the shape of subtensor should be (3,2)\n",">- odd valued rows and columns"]},{"cell_type":"code","execution_count":null,"id":"STbUdF0J5IBD","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":534,"status":"ok","timestamp":1706029217619,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"STbUdF0J5IBD","outputId":"82052242-f342-4008-b77c-c7b0ddcb7a05"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1,  2,  3,  4,  5],\n","        [ 6,  7,  8,  8, 10],\n","        [11, 12, 13, 14, 15]])"]},"metadata":{},"execution_count":16}],"source":["x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 8, 10], [11, 12, 13, 14, 15]])\n","x"]},{"cell_type":"code","execution_count":null,"id":"mgHPm0qP5ZU3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":239,"status":"ok","timestamp":1706029219833,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"mgHPm0qP5ZU3","outputId":"373fb3db-1b8b-4586-cf19-3c30f56b1ef9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 5])"]},"metadata":{},"execution_count":17}],"source":["x.shape"]},{"cell_type":"code","execution_count":null,"id":"hzQRs79A5JGd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":586,"status":"ok","timestamp":1706029238984,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"hzQRs79A5JGd","outputId":"2b70bcdc-749b-492a-86d9-e8adf8fce626"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([11, 12, 13, 14, 15])"]},"metadata":{},"execution_count":18}],"source":["# Student Task: Retrieve the last row of the tensor 'x'\n","# Hint: Negative indexing can help you select rows or columns counting from the end of the tensor.\n","# Think about how you can select all columns for the desired row.\n","last_row = x[-1,:]\n","last_row"]},{"cell_type":"code","execution_count":null,"id":"-mb_Et866ZEW","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706029240127,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"-mb_Et866ZEW","outputId":"c10837a1-9959-4476-e49e-5cb53f49d784"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 4,  8, 14])"]},"metadata":{},"execution_count":19}],"source":["# Student Task: Retrieve the fourth column of the tensor 'x'\n","# Hint: Pay attention to the indexing for both rows and columns.\n","# Remember that indexing in Python starts from zero.\n","fourth_column = x[:, 3]\n","fourth_column"]},{"cell_type":"code","execution_count":null,"id":"c2VaG6Y16jsA","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1706029242949,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"c2VaG6Y16jsA","outputId":"848b2fdc-82bf-41d4-e320-2a11053ae56a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1,  2],\n","        [ 6,  7],\n","        [11, 12]])"]},"metadata":{},"execution_count":20}],"source":["# Student Task: Retrieve the first 3 rows and first 2 columns from the tensor 'x'.\n","# Hint: Use slicing to extract the required subset of rows and columns.\n","first_3_rows_2_columns = x[0:3, 0:2]\n","first_3_rows_2_columns"]},{"cell_type":"code","execution_count":null,"id":"RHnSUpxs7O82","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":571,"status":"ok","timestamp":1706029260984,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"RHnSUpxs7O82","outputId":"769b8fdf-6013-4f36-b852-fd23098227b6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1,  3,  5],\n","        [11, 13, 15]])"]},"metadata":{},"execution_count":21}],"source":["# Student Task: Retrieve the rows and columns with odd-indexed positions from the tensor 'x'.\n","# Hint: Use stride slicing to extract the required subset of rows and columns with odd indices.\n","odd_valued_rows_columns = x[0::2, 0::2]\n","odd_valued_rows_columns"]},{"cell_type":"markdown","id":"uSj20OEuf6bf","metadata":{"id":"uSj20OEuf6bf"},"source":["#  <font color = 'dodgerred'>**Q4 -Normalize Function (1 Point)**<font>\n","\n","Write the function that normalizes the columns of a matrix. You have to compute the mean and standard deviation of each column. Then for each element of the column, you subtract the mean and divide by the standard deviation."]},{"cell_type":"code","execution_count":null,"id":"8L9JBFNilkWt","metadata":{"id":"8L9JBFNilkWt"},"outputs":[],"source":["# Given Data\n","x = [[ 3,  60,  100, -100],\n","     [ 2,  20,  600, -600],\n","     [-5,  50,  900, -900]]"]},{"cell_type":"code","execution_count":null,"id":"iRrhopVBl-0q","metadata":{"id":"iRrhopVBl-0q"},"outputs":[],"source":["# Convert to PyTorch Tensor and set to float\n","X = torch.tensor(x)\n","X= X.float()"]},{"cell_type":"code","execution_count":null,"id":"S2MaocHxmEQJ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":697,"status":"ok","timestamp":1706029287835,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"S2MaocHxmEQJ","outputId":"158441ec-752b-4de7-dbe3-63266b33ac33"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 4])\n","torch.float32\n"]}],"source":["# Print shape and data type for verification\n","print(X.shape)\n","print(X.dtype)"]},{"cell_type":"code","execution_count":null,"id":"rPgb1L9RmQAU","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":568,"status":"ok","timestamp":1706029295039,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"rPgb1L9RmQAU","outputId":"3964a884-7848-4498-9b19-50a791da2c22"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([  4.3589,  20.8167, 404.1452, 404.1452])"]},"metadata":{},"execution_count":25}],"source":["# Compute and display the mean and standard deviation of each column for reference\n","X.mean(axis = 0)\n","X.std(axis = 0)"]},{"cell_type":"code","execution_count":null,"id":"qnM87Db1mqFH","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1706029296730,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"qnM87Db1mqFH","outputId":"9200c665-5c62-4f45-9025-8448cae0285e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([  4.3589,  20.8167, 404.1452, 404.1452])"]},"metadata":{},"execution_count":26}],"source":["X.std(axis = 0)"]},{"cell_type":"markdown","source":["- Your task starts here\n","- Your normalize_matrix function should take a PyTorch tensor x as input.\n","- It should return a tensor where the columns are normalized.\n","- After implementing your function, use the code provided to verify if the mean for each column in Z is close to zero and the standard deviation is 1."],"metadata":{"id":"Yy2hehDnJHOq"},"id":"Yy2hehDnJHOq"},{"cell_type":"code","execution_count":null,"id":"mwq8qnqFlu9V","metadata":{"id":"mwq8qnqFlu9V"},"outputs":[],"source":["def normalize_matrix(x):\n","  # Calculate the mean along each column (think carefully , you will take mean along axis = 0 or 1)\n","  mean = x.mean(axis=0)\n","\n","  # Calculate the standard deviation along each column\n","  std = x.std(axis=0)\n","\n","  # Normalize each element in the columns by subtracting the mean and dividing by the standard deviation\n","  y = (x - mean) / std\n","\n","  return y  # Return the normalized matrix\n","\n"]},{"cell_type":"code","execution_count":null,"id":"m027Qcgwm9OL","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706029308218,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"m027Qcgwm9OL","outputId":"d87eef19-f580-4778-f17f-52a2d25dc0be"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.6882,  0.8006, -1.0722,  1.0722],\n","        [ 0.4588, -1.1209,  0.1650, -0.1650],\n","        [-1.1471,  0.3203,  0.9073, -0.9073]])"]},"metadata":{},"execution_count":28}],"source":["Z = normalize_matrix(X)\n","Z"]},{"cell_type":"code","execution_count":null,"id":"78-0D3KfnHel","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":586,"status":"ok","timestamp":1706029313912,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"78-0D3KfnHel","outputId":"61380972-307e-4d4d-c2ab-b1b25d4e802a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0.0000e+00,  4.9671e-08,  3.9736e-08, -3.9736e-08])"]},"metadata":{},"execution_count":29}],"source":["Z.mean(axis = 0)"]},{"cell_type":"code","execution_count":null,"id":"NETkPmz8n142","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1706029314817,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"NETkPmz8n142","outputId":"0779ac11-deca-44dc-cfa3-078d7fe3bdc2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 1., 1., 1.])"]},"metadata":{},"execution_count":30}],"source":["Z.std(axis = 0)"]},{"cell_type":"markdown","metadata":{"id":"cc32848b"},"source":["# <font color = 'dodgerred'>**Q5: In-place vs. Out-of-place Operations (1 Point)**\n","\n","1. Create a tensor `A` with values `[1, 2, 3]`.\n","2. Perform an in-place addition (use `add_` method) of `5` to tensor `A`.\n","3. Then, create another tensor `B` with values `[4, 5, 6]` and perform an out-of-place addition of `5`.\n","\n","**Print the memory addresses of `A` and `B` before and after the operations to demonstrate the difference in memory usage. Provide explanation**\n"],"id":"cc32848b"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"19b3aaa5","executionInfo":{"status":"ok","timestamp":1706029402212,"user_tz":360,"elapsed":222,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"15c5dfe7-8562-4867-9f0c-0f13dccb94ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original memory address of A: 134234535193120\n","Memory address of A after in-place addition: 134234535193120\n","A after in-place addition: tensor([6, 7, 8])\n","Original memory address of B: 134237874698208\n","Memory address of B after out-of-place addition: 134234535018256\n","B after out-of-place addition: tensor([ 9, 10, 11])\n"]}],"source":["A = torch.tensor([1, 2, 3])\n","print('Original memory address of A:', id(A))\n","A.add_(5)\n","print('Memory address of A after in-place addition:', id(A))\n","print('A after in-place addition:', A)\n","\n","B = torch.tensor([4, 5, 6])\n","print('Original memory address of B:', id(B))\n","B = B.add(5)\n","print('Memory address of B after out-of-place addition:', id(B))\n","print('B after out-of-place addition:', B)\n"],"id":"19b3aaa5"},{"cell_type":"markdown","source":["**Provide Explanation for above question here :**\n","\n","The in-place addition operation `add_` modifies the tensor 'A' directly, without creating a new tensor. Therefore, the memory address before and after the operation remains the same, indicating that the original tensor is modified in place.\n","\n","The out-of-place addition operation `add` creates a new tensor with the result of the addition. As a result, the memory address of 'B' changes after the operation, indicating that a new tensor is created and assigned to the variable 'B'. The original tensor is not modified, and the result is stored in a new memory location.\n","\n","In summary, in-place operations modify the original tensor and do not change the memory address, while out-of-place operations create a new tensor and hence have a different memory address post-operation."],"metadata":{"id":"aXi2TsYVElqy"},"id":"aXi2TsYVElqy"},{"cell_type":"markdown","metadata":{"id":"f57bb1a4"},"source":["# <font color = 'dodgerred'>**Q6: Tensor Broadcasting (1 Point)**\n","\n","1. Create two tensors `X` with shape `(3, 1)` and `Y` with shape `(1, 3)`. Perform an addition operation on `X` and `Y`.\n","2. Explain how broadcasting is applied in this operation by describing the shape changes that occur internally."],"id":"f57bb1a4"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"699f9bd9","executionInfo":{"status":"ok","timestamp":1706029415611,"user_tz":360,"elapsed":221,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"31cebe52-4d49-437b-f4ae-c65403ffe5f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original shapes: torch.Size([3, 1]) torch.Size([1, 3])\n","Result: tensor([[2., 3., 4.],\n","        [2., 3., 4.],\n","        [2., 3., 4.]])\n","Result shape: torch.Size([3, 3])\n"]}],"source":["X = torch.ones(3, 1)\n","Y = torch.arange(1, 4).view(1, 3)\n","print('Original shapes:', X.shape, Y.shape)\n","result = X + Y\n","print('Result:', result)\n","print('Result shape:', result.shape)\n","\n"],"id":"699f9bd9"},{"cell_type":"markdown","source":["**Provide Explanation for above question here :**\n","\n","In the code snippet, tensors `X` and `Y` are added using broadcasting. Here's an explanation of how broadcasting works in this operation:\n","\n","1. **Initial Shapes**:\n","   - `X` has a shape of `(3, 1)`.\n","   - `Y` has a shape of `(1, 3)`.\n","\n","2. **Broadcasting Rules**:\n","   - PyTorch compares the shapes of `X` and `Y` element-wise from the last dimension:\n","     - The dimensions are compatible because one of the dimensions is `1`.\n","   - The tensor with the smaller dimension (in this case, both `X` and `Y`) is virtually expanded to match the larger dimension without copying data.\n","   \n","3. **Internal Shape Adjustment**:\n","   - `X` is broadcasted to a shape of `(3, 3)` by repeating its columns.\n","   - `Y` is broadcasted to a shape of `(3, 3)` by repeating its rows.\n","\n","4. **Result of the Addition**:\n","   - The element-wise addition is performed on the broadcasted tensors.\n","   - The resulting tensor has a shape of `(3, 3)`, and each element is the sum of the corresponding elements of `X` and `Y`.\n","\n","5. **Conclusion**:\n","   - Broadcasting allows for element-wise operations on tensors of different shapes by virtually expanding the smaller tensor along the dimension with size `1`.\n","   - The operation does not involve actual data duplication, which makes broadcasting memory-efficient."],"metadata":{"id":"IblSVvPhE4-E"},"id":"IblSVvPhE4-E"},{"cell_type":"markdown","metadata":{"id":"e1f2667e"},"source":["# <font color = 'dodgerred'>**Q7: Linear Algebra Operations (1 Point)**\n","\n","1. Create two matrices `M1` and `M2` of compatible shapes for matrix multiplication. Perform the multiplication and print the result.\n","2. Then, create two vectors `V1` and `V2` and compute their dot product.\n"],"id":"e1f2667e"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d45aed18","executionInfo":{"status":"ok","timestamp":1706029432036,"user_tz":360,"elapsed":314,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"4a4d910a-430e-4f81-b4c3-de73800a5941"},"outputs":[{"output_type":"stream","name":"stdout","text":["Matrix multiplication result: tensor([[19, 22],\n","        [43, 50]])\n","Dot product: tensor(32)\n"]}],"source":["M1 = torch.tensor([[1, 2], [3, 4]])\n","M2 = torch.tensor([[5, 6], [7, 8]])\n","product = torch.matmul(M1, M2)\n","print('Matrix multiplication result:', product)\n","\n","V1 = torch.tensor([1, 2, 3])\n","V2 = torch.tensor([4, 5, 6])\n","dot_product = torch.dot(V1, V2)\n","print('Dot product:', dot_product)\n"],"id":"d45aed18"},{"cell_type":"markdown","metadata":{"id":"fe24a961"},"source":["# <font color = 'dodgerred'>**Q8: Manipulating Tensor Shapes (1 Point)**\n","\n","Given a tensor `T` with shape `(2, 3, 4)`, demonstrate how to\n","1. reshape it to `(3, 8)` using view,\n","2. reshape it to `(4, 2, 3` using reshape,\n","3. transpose the first and last dimensions using permute.\n","4. explain what is the difference between reshape and view"],"id":"fe24a961"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0644b861","executionInfo":{"status":"ok","timestamp":1706029457298,"user_tz":360,"elapsed":329,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"2d941531-c981-4d83-adfa-bf114ff35f50"},"outputs":[{"output_type":"stream","name":"stdout","text":["T_view shape: torch.Size([3, 8])\n","T_reshape shape: torch.Size([4, 2, 3])\n","T_permute shape: torch.Size([4, 3, 2])\n"]}],"source":["T = torch.rand(2, 3, 4)\n","T_view = T.view(3, 8)\n","print('T_view shape:', T_view.shape)\n","\n","T_reshape = T.reshape(4, 2, 3)\n","print('T_reshape shape:', T_reshape.shape)\n","\n","T_permute = T.permute(2, 1, 0)\n","print('T_permute shape:', T_permute.shape)"],"id":"0644b861"},{"cell_type":"markdown","source":["**Provide Explanation for above question here :**\n","The `view` and `reshape` methods in PyTorch are used to change the shape of a tensor without changing its data. Here's the concise explanation of the difference between them:\n","\n","- **`view`**:\n","  - Returns a new tensor with the same data as the original tensor but with a different shape.\n","  - Requires the new shape to be compatible with the original shape and the tensor to be contiguous in memory. If the tensor is not contiguous, it will fail (you may need to call `.contiguous()` before calling `.view()`).\n","  - It's a low-level method in terms of memory usage, as it operates directly on the original tensor's memory.\n","\n","- **`reshape`**:\n","  - Also returns a new tensor with the same data as the original tensor but with a different shape.\n","  - Similar to `view`, but it can work with non-contiguous tensors by internally handling the memory layout if necessary.\n","  - If the tensor is contiguous, `reshape` and `view` will provide the same functionality. If not, `reshape` may return a copy of the tensor to ensure the requested shape.\n","\n","In summary, while both `view` and `reshape` can be used to change the shape of a tensor, `reshape` is more flexible as it can handle non-contiguous tensor layouts, potentially at the cost of performance if a copy of the tensor needs to be made."],"metadata":{"id":"Lb-861xsFXHN"},"id":"Lb-861xsFXHN"},{"cell_type":"markdown","metadata":{"id":"1d5c7b62"},"source":["# <font color = 'dodgerred'>**Q9: Tensor Concatenation and Stacking (1 Point)**\n","\n","Create tensors `C1` and `C2` both with shape (2, 3).\n","1. Concatenate them along dimension 0 and then along dimension 1. Print the shape of the resulting tensor.\n","2. Afterwards, stack the same tensors alomng dimension 0  and print the shape of the resulting tensor.\n","3. What is the difference between stacking and concatinating."],"id":"1d5c7b62"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"69e2f7e0","executionInfo":{"status":"ok","timestamp":1706029683717,"user_tz":360,"elapsed":227,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"a28911b9-d3d3-4d73-8a75-4570c581a450"},"outputs":[{"output_type":"stream","name":"stdout","text":["Concatenated along dimension 0: torch.Size([4, 3])\n","Concatenated along dimension 1: torch.Size([2, 6])\n","Stacked tensor shape: torch.Size([2, 2, 3])\n"]}],"source":["C1 = torch.rand(2, 3)\n","C2 = torch.rand(2, 3)\n","concatenated_dim0 = torch.cat((C1, C2), dim=0)\n","print('Concatenated along dimension 0:', concatenated_dim0.shape)\n","\n","concatenated_dim1 = torch.cat((C1, C2), dim=1)\n","print('Concatenated along dimension 1:', concatenated_dim1.shape)\n","\n","stacked = torch.stack((C1, C2), dim = 0)\n","print('Stacked tensor shape:', stacked.shape)\n"],"id":"69e2f7e0"},{"cell_type":"markdown","source":["**Explain the diffrence between concatinating and stacking here**\n","\n","The difference between stacking and concatenating tensors in PyTorch lies in how the tensors are combined:\n","\n","- **Concatenating**:\n","  - Combines tensors along an existing dimension.\n","  - The tensors to be concatenated must have the same shape in all dimensions except for the dimension along which they are concatenated.\n","  - The result has the same number of dimensions as the original tensors, but the size of the dimension along which they are concatenated is the sum of the sizes of that dimension in each tensor.\n","  - Example: Concatenating two tensors of shape `(2, 3)` along dimension 0 results in a tensor of shape `(4, 3)`, and along dimension 1 results in a tensor of shape `(2, 6)`.\n","\n","- **Stacking**:\n","  - Combines tensors along a new dimension.\n","  - All tensors to be stacked must have the same shape.\n","  - The resulting tensor has one more dimension than the original tensors, with the size of the new dimension equal to the number of tensors stacked.\n","  - Example: Stacking two tensors of shape `(2, 3)` results in a tensor of shape `(2, 2, 3)`, with the new dimension added at the beginning (dimension 0).\n","\n","In summary, concatenation merges tensors along an existing axis, while stacking adds a new dimension to the tensor and stacks along it.\n"],"metadata":{"id":"kmKpP5xBCCgt"},"id":"kmKpP5xBCCgt"},{"cell_type":"markdown","metadata":{"id":"0d0ed971"},"source":["# <font color = 'dodgerred'>**Q10: Advanced Indexing and Slicing (1 Point)**\n","\n","1. Given a tensor `D` with shape (6, 6), extract elements that are greater than 0.5.\n","2. Then, extract the second and fourth rows from `D`.\n","3. Finally, extract a sub-tensor from the top-left 3x3 block."],"id":"0d0ed971"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"df969523","executionInfo":{"status":"ok","timestamp":1706029776825,"user_tz":360,"elapsed":4,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"1d3b89dc-46a0-4b06-9c31-eabaf7921eec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Elements greater than 0.5:\n"," tensor([0.5396, 0.7725, 0.6434, 0.8961, 0.8104, 0.5072, 0.5674, 0.6243, 0.5790,\n","        0.8664, 0.7271, 0.7643, 0.6259, 0.5248, 0.5040, 0.6147, 0.8272, 0.5612,\n","        0.5318, 0.6680])\n","\n","Second and fourth rows:\n"," tensor([[0.8104, 0.1105, 0.5072, 0.2074, 0.5674, 0.6243],\n","        [0.2082, 0.0723, 0.6259, 0.4635, 0.3203, 0.5248],\n","        [0.8272, 0.0098, 0.5612, 0.4052, 0.5318, 0.6680]])\n","\n","Top-left 3x3 block:\n","  tensor([[0.5396, 0.2973, 0.7725],\n","        [0.8104, 0.1105, 0.5072],\n","        [0.4442, 0.4958, 0.5790]])\n"]}],"source":["D = torch.rand(6, 6)\n","print('Elements greater than 0.5:\\n', D[D > 0.5])\n","\n","second_fourth_rows = D[1::2]\n","print('\\nSecond and fourth rows:\\n', second_fourth_rows)\n","\n","top_left_3x3 = D[:3, :3]\n","print('\\nTop-left 3x3 block:\\n ', top_left_3x3)\n"],"id":"df969523"},{"cell_type":"markdown","metadata":{"id":"c081cf3b"},"source":["# <font color = 'dodgerred'>**Q11: Tensor Mathematical Operations (1 Point)**\n","\n","1. Create a tensor `G` with values from 0 to π in steps of π/4.\n","2. Compute and print the sine, cosine, and tangent logarithm and the exponential of `G`."],"id":"c081cf3b"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4c22b354","executionInfo":{"status":"ok","timestamp":1706029809717,"user_tz":360,"elapsed":439,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"97667823-cf8f-462b-a220-053e361442ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["G: tensor([0.0000, 0.7854, 1.5708, 2.3562])\n","Sine of G: tensor([0.0000, 0.7071, 1.0000, 0.7071])\n","Cosine of G: tensor([ 1.0000e+00,  7.0711e-01, -4.3711e-08, -7.0711e-01])\n","Tangent of G: tensor([ 0.0000e+00,  1.0000e+00, -2.2877e+07, -1.0000e+00])\n","Natural logarithm of G: tensor([   -inf, -0.2416,  0.4516,  0.8570])\n","Exponential of G: tensor([ 1.0000,  2.1933,  4.8105, 10.5507])\n"]}],"source":["G = torch.arange(0, torch.pi, torch.pi/4)\n","print('G:', G)\n","print('Sine of G:', torch.sin(G))\n","print('Cosine of G:', torch.cos(G))\n","print('Tangent of G:', torch.tan(G))\n","print('Natural logarithm of G:', torch.log(G))\n","print('Exponential of G:', torch.exp(G))\n"],"id":"4c22b354"},{"cell_type":"markdown","metadata":{"id":"629eb94b"},"source":["# <font color = 'dodgerred'>Q12: **Tensor Reduction Operations (1 Point)**\n","\n","1. Create a 3x2 tensor `H`.\n","2. Compute the sum of `H`. Print the result and shape after taking sun.\n","3. Then, perform the same operations along dimension 0 and dimension 1, printing the results and shapes.\n","4. What do you observe? How the shape changes?"],"id":"629eb94b"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"729d7275","executionInfo":{"status":"ok","timestamp":1706029822916,"user_tz":360,"elapsed":553,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"43b9d84d-2e6c-46bd-82d8-ccd1b9ee5ac3"},"outputs":[{"output_type":"stream","name":"stdout","text":["H: tensor([[0.1203, 0.5088],\n","        [0.2164, 0.0863],\n","        [0.2356, 0.6817]])\n","\n","Shape of original Tensor H torch.Size([3, 2])\n","\n","Sum of H: tensor(1.8491)\n","Shape after Sum of H: torch.Size([])\n","\n","Sum of H along dimension 0: tensor([0.5723, 1.2768])\n","Shape after sum of H along dimension 0: torch.Size([2])\n","\n","Sum of H along dimension 1: tensor([0.6291, 0.3027, 0.9172])\n","Shape after sum of H along dimension 1: torch.Size([3])\n"]}],"source":["H = torch.rand(3, 2)\n","print('H:', H, end = \"\\n\\n\")\n","print('Shape of original Tensor H', H.shape, end = \"\\n\\n\")\n","\n","print('Sum of H:', torch.sum(H))\n","print('Shape after Sum of H:', torch.sum(H).shape,  end = \"\\n\\n\")\n","\n","print('Sum of H along dimension 0:', torch.sum(H, dim=0),)\n","print('Shape after sum of H along dimension 0:', torch.sum(H, dim=0).shape,  end = \"\\n\\n\")\n","\n","print('Sum of H along dimension 1:', torch.sum(H, dim=1))\n","print('Shape after sum of H along dimension 1:', torch.sum(H, dim=1).shape)\n"],"id":"729d7275"},{"cell_type":"markdown","source":["**Provide your observations on shape changes here**\n","\n","Observations regarding how the shape changes after each operation:\n","\n","1. **Original Tensor `H` (3x2):**\n","   - The tensor `H` is initialized with random values and has a shape of (3, 2). This means it has 3 rows and 2 columns.\n","\n","2. **Sum of `H`:**\n","   - When you compute `torch.sum(H)`, it calculates the sum of all elements in the tensor. As a result, it returns a single scalar value (a 0-dimensional tensor).\n","   - **Observation**: The shape changes from (3, 2) to an empty shape `torch.Size([])`, indicating a scalar.\n","\n","3. **Sum of `H` along dimension 0:**\n","   - When you compute `torch.sum(H, dim=0)`, it sums the elements of `H` along dimension 0 (vertically). This means each element in the resulting tensor is the sum of the elements in each column of `H`.\n","   - **Observation**: The shape changes from (3, 2) to (2,), indicating that the sum is computed for each column, and the number of rows is reduced to a single value representing the sum of each column.\n","\n","4. **Sum of `H` along dimension 1:**\n","   - When you compute `torch.sum(H, dim=1)`, it sums the elements of `H` along dimension 1 (horizontally). This means each element in the resulting tensor is the sum of the elements in each row of `H`.\n","   - **Observation**: The shape changes from (3, 2) to (3,), indicating that the sum is computed for each row, and the number of columns is reduced to a single value representing the sum of each row.\n","\n","In summary, when you perform a sum operation along a specific dimension, the shape of the tensor is reduced along that dimension. For a sum over the entire tensor, you get a scalar. For a sum along a specific dimension, that dimension is collapsed to a single value representing the sum along that dimension. The overall rank of the tensor (number of dimensions) is reduced by one for each dimension you sum over."],"metadata":{"id":"hfmDz2OLb8M2"},"id":"hfmDz2OLb8M2"},{"cell_type":"markdown","metadata":{"id":"4a156a3a"},"source":["# <font color = 'dodgerred'>**Q13: Working with Tensor Data Types (1 Point)**\n","\n","1. Create a tensor `I` of data type float with values `[1.0, 2.0, 3.0]`.\n","2. Convert `I` to data type int and print the result.\n","3. Explain in which scenarios it's necessary to be cautious about the data type of tensors."],"id":"4a156a3a"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"af427555","executionInfo":{"status":"ok","timestamp":1705999099480,"user_tz":360,"elapsed":256,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"40bd20fc-b7cd-4ed1-f7c0-579133901c7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["I: tensor([1., 2., 3.])\n","I converted to int: tensor([1, 2, 3], dtype=torch.int32)\n"]}],"source":["# Solution for Q16\n","I = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float)\n","print('I:', I)\n","I_int = I.to(dtype=torch.int)\n","print('I converted to int:', I_int)\n","# Explanation: It's necessary to be cautious about data types when converting between float and int\n","# to avoid losing precision or information.\n"],"id":"af427555"},{"cell_type":"markdown","source":["**Your explanations here**\n","\n","Being cautious about the data type of tensors is important in various scenarios due to the different ways that data types handle numerical precision, computation speed, and memory usage. Here are some specific scenarios where careful consideration of tensor data types is crucial:\n","\n","1. **Loss of Precision**: When converting from a type with higher precision to lower precision (e.g., from `float` to `int`), there is a loss of precision. Decimal parts are truncated when converting from a floating-point type to an integer type. This can lead to loss of critical information, especially if the decimals are significant for the calculations or results.\n","\n","2. **Overflow and Underflow**: Certain operations might result in numbers that are too large or too small to be represented in the chosen data type, leading to overflow or underflow. For instance, integer types have a fixed range, and exceeding this range might result in unexpected behavior or errors.\n","\n","3. **Compatibility with Frameworks and Functions**: Some PyTorch functions and neural network layers expect inputs of a certain data type. Using the incorrect data type might lead to errors or inefficient computation. For example, convolution layers in a neural network might expect floating-point weights for gradient computations.\n","\n","4. **Memory Usage**: Data types with higher precision (like `float64`) consume more memory compared to lower precision types (like `float32` or `int`). In scenarios where you're working with large datasets or models, choosing a more memory-efficient data type can be crucial to prevent running out of memory, especially on limited resources like GPUs.\n","\n","5. **Computational Speed**: Operations on lower precision data types are generally faster. This is particularly noticeable when working with GPUs, as they are optimized for high-speed computation on lower precision data. For instance, using `float16` (half-precision) can speed up training neural networks without significantly affecting the model's performance.\n","\n","6. **Gradient Computations in Neural Networks**: When training neural networks, the choice of data type affects the precision of gradient computations. Lower precision might lead to inadequate learning or failure to converge, as small gradient values may vanish or not be captured accurately.\n","\n","7. **Quantization and Model Deployment**: When deploying models, especially on edge devices with limited resources, model quantization (reducing the precision of the model's parameters) is common. Careful management of data types is crucial here to balance between performance, accuracy, and resource usage.\n","\n","In each of these scenarios, understanding the implications of tensor data types helps in making informed decisions, ensuring the accuracy of computations, and optimizing performance and resource usage."],"metadata":{"id":"9yyx5WtweGNP"},"id":"9yyx5WtweGNP"},{"cell_type":"markdown","metadata":{"id":"2TU6l0nC3EfW"},"source":["# <font color = 'dodgerred'>**Question 14. Speedtest for vectorization - 1.5 Points** </font>\n","\n","Your goal is to measure the speed of linear algebra operations for different levels of vectorization.\n","\n","1. Construct two matrices $A$ and $B$ with Gaussian random entries of size $1024 \\times 1024$.\n","1. Compute $C = A B$ using matrix-matrix operations and report the time. (Hint: Use torch.mm)\n","1. Compute $C = A B$, treating $A$ as a matrix but computing the result for each column of $B$ one at a time. Report the time. (hint use torch.mv inside a for loop)\n","1. Compute $C = A B$, treating $A$ and $B$ as collections of vectors. Report the time. (Hint: use torch.dot inside nested for loop)"],"id":"2TU6l0nC3EfW"},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkKjtX0HH2wz"},"outputs":[],"source":["## Solution 1\n","torch.manual_seed(42)\n","A= torch.randn((1024, 1024))\n","B= torch.randn((1024, 1024))"],"id":"wkKjtX0HH2wz"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1706029965994,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"kSMH_j5OD2ZB","outputId":"3b06ce7b-b364-4c25-a796-e6ce46ff7999"},"outputs":[{"output_type":"stream","name":"stdout","text":["Matrix by matrix: 0.01940011978149414 seconds\n"]}],"source":["## Solution 2\n","start=time.time()\n","\n","C = torch.mm(A, B)\n","\n","print(\"Matrix by matrix: \" + str(time.time()-start) + \" seconds\")"],"id":"kSMH_j5OD2ZB"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":237,"status":"ok","timestamp":1706029969080,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"-tU8yGBP-Crk","outputId":"7966f200-34c9-4bbf-811f-540ca65ae47c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Matrix by vector: 0.2724425792694092 seconds\n"]}],"source":["## Solution 3\n","C= torch.empty(1024,1024)\n","start = time.time()\n","\n","for i in range(C.shape[1]):\n","  C[:,i] = torch.mv(A, B[:,i])\n","\n","print(\"Matrix by vector: \" + str(time.time()-start) + \" seconds\")"],"id":"-tU8yGBP-Crk"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14959,"status":"ok","timestamp":1706029985806,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"MFgJCFf6DUFK","outputId":"84414600-ba2a-48ab-d09c-22ad07359bdc"},"outputs":[{"output_type":"stream","name":"stdout","text":["vector by vector: 14.729571104049683 seconds\n"]}],"source":["## Solution 4\n","C= torch.empty(1024,1024)\n","start = time.time()\n","\n","for i in range(C.shape[0]):\n","  for j in range(C.shape[1]):\n","    C[i,j] = torch.dot(A[i], C[j])\n","\n","print(\"vector by vector: \" + str(time.time()-start) + \" seconds\")"],"id":"MFgJCFf6DUFK"},{"cell_type":"markdown","metadata":{"id":"TtYsJM4mJNdE"},"source":["# <font color = 'dodgerred'>**Question 4 : Redo Question 14 by using GPU - 1.5 Points**"],"id":"TtYsJM4mJNdE"},{"cell_type":"markdown","metadata":{"id":"fxJ1UlTf3Efb"},"source":["<font size = 4, color = 'dodgerred'> **Using GPUs**\n","\n","How to use GPUs in Google Colab<br>\n","In Google Colab -- Go to Runtime Tab at top -- select change runtime type -- for hardware accelartor choose GPU"],"id":"fxJ1UlTf3Efb"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1706029985806,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"_6ilpofMIe1e","outputId":"feb00861-4113-4ce2-ca70-f81785026ede"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["# Check if GPU is availaible\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"id":"_6ilpofMIe1e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4XMhjifbJcu0"},"outputs":[],"source":["## Solution 1\n","torch.manual_seed(42)\n","A= torch.randn((1024, 1024),device=device)\n","B= torch.randn((1024, 1024),device=device)"],"id":"4XMhjifbJcu0"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1706029986097,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"pn-ZKI7sK9Oh","outputId":"0f768ff0-3704-469d-9aea-528a8756359e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Matrix by matrix: 0.10759520530700684 seconds\n"]}],"source":["## Solution 2\n","start=time.time()\n","\n","C = torch.mm(A, B)\n","\n","print(\"Matrix by matrix: \" + str(time.time()-start) + \" seconds\")"],"id":"pn-ZKI7sK9Oh"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1706029986098,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"GcHPGEitLL8i","outputId":"6b2acf4f-4ebf-4487-c886-4dfe8821d513"},"outputs":[{"output_type":"stream","name":"stdout","text":["Matrix by vector: 0.08559584617614746 seconds\n"]}],"source":["## Solution 3\n","C= torch.empty(1024,1024, device = device)\n","start = time.time()\n","\n","for i in range(C.shape[1]):\n","  C[:,i] = torch.mv(A, B[:,i])\n","\n","print(\"Matrix by vector: \" + str(time.time()-start) + \" seconds\")"],"id":"GcHPGEitLL8i"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32694,"status":"ok","timestamp":1706030018790,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"wZ5LWSa2Lrdw","outputId":"4269bf43-47d5-4d8b-9504-2a17100f4298"},"outputs":[{"output_type":"stream","name":"stdout","text":["vector by vector: 32.53825521469116 seconds\n"]}],"source":["## Solution 4\n","C= torch.empty(1024,1024, device = device)\n","start = time.time()\n","\n","for i in range(C.shape[0]):\n","  for j in range(C.shape[1]):\n","    C[i,j] = torch.dot(A[i], C[j])\n","\n","print(\"vector by vector: \" + str(time.time()-start) + \" seconds\")"],"id":"wZ5LWSa2Lrdw"}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":5}