{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gaGkLWaZZv45"},"source":["# <font color = 'pickle'>**Lecture : Intoduction to PyTorch Tensors**\n"]},{"cell_type":"markdown","metadata":{"id":"sa1p0E6Sduea"},"source":["# <font color = 'pickle'>**Importing PyTorch Library**"]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mYXb-ESsbaut","executionInfo":{"status":"ok","timestamp":1705969617368,"user_tz":360,"elapsed":320,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"79c0b7f4-3a9b-4b30-eac8-059b39be418a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Jan 23 00:26:55 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"chOMYVyUVNof","executionInfo":{"status":"ok","timestamp":1705969648269,"user_tz":360,"elapsed":3937,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"source":["import torch\n","import numpy as np"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IHWosHogaREo"},"source":["# <font color = 'pickle'>**Tensors**\n","\n","- Tensors are the basic building blocks of any deep learning network.\n","\n","- They are used to represent all the different types of data be it images, sound files, text data etc.\n","\n","- Tensors are **order N-matrix**.\n","\n","\n","If N=1, tensor will basically be a **vector**.\n","If N=2, tensor will be a **2-d matrix**.\n","\n","Why Tensors and not NumPy arrays?\n","\n","- NumPy only supports CPU computation.\n","- Tensor class supports automatic differentiation."]},{"cell_type":"markdown","metadata":{"id":"EJREGcIPcIm2"},"source":["**Let us start by importing PyTorch library and understand some of the basic functions on tensors.**"]},{"cell_type":"markdown","source":["## <font color = 'pickle'>**Scalar**\n","- rank-0 tensor\n"],"metadata":{"id":"_9Va52NtqsTo"}},{"cell_type":"code","metadata":{"id":"agJbamRzhFh8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705969699105,"user_tz":360,"elapsed":305,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"1250c828-e2e1-4fe1-d38e-a5daafa1436c"},"source":["t = torch.tensor(1.)\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')\n","print(\"\\nnumber of dimensions:\", t.dim(), sep = \"\\n\")\n","print(\"\\nData Type:\", t.dtype, sep = \"\\n\")"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.)\n","\n","size:\n","torch.Size([])\n","\n","number of dimensions:\n","0\n","\n","Data Type:\n","torch.float32\n"]}]},{"cell_type":"markdown","source":["## <font color = 'pickle'>**Vector**\n","- rank-1 tensor"],"metadata":{"id":"i8JRukoprQvo"}},{"cell_type":"code","source":["t = torch.tensor([1., 2])\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')\n","print(\"\\nnumber of dimensions:\", t.dim(), sep = \"\\n\")\n","print(\"\\nData Type:\", t.dtype, sep = \"\\n\")"],"metadata":{"id":"Jklq36xT3doI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705969733077,"user_tz":360,"elapsed":424,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"9e184873-7b54-40a7-de8e-d21908b03e0f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 2.])\n","\n","size:\n","torch.Size([2])\n","\n","number of dimensions:\n","1\n","\n","Data Type:\n","torch.float32\n"]}]},{"cell_type":"markdown","source":["## <font color = 'pickle'>**Matrix**\n","- rank 2 tensor\n","\n","Matrices are 2-d arrays with size `n x m`. Here, n: number of rows and m: number of columns.\n","\n","If `m = n`, then the matrix is known as a `square matrix`.\n","\n","Precisely, matrices can be represented as:\n","$$\\mathbf{X}=\\begin{bmatrix} x_{11} & x_{12} & \\cdots & x_{1n} \\\\ x_{21} & x_{22} & \\cdots & x_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{m1} & x_{m2} & \\cdots & x_{mn} \\\\ \\end{bmatrix}$$\n","<br>\n","\n"],"metadata":{"id":"SMnpra7xr6Kf"}},{"cell_type":"code","source":["t = torch.tensor([\n","     [1., 2, 3],\n","     [4, 5, 6]\n","    ])\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')\n","print(\"\\nnumber of dimensions:\", t.dim(), sep = \"\\n\")\n","print(\"\\nData Type:\", t.dtype, sep = \"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705969804055,"user_tz":360,"elapsed":306,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"4ca3d65b-3b7e-4af0-dc26-8f697e4e1123","id":"oonCHYyJr6Kg"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 2., 3.],\n","        [4., 5., 6.]])\n","\n","size:\n","torch.Size([2, 3])\n","\n","number of dimensions:\n","2\n","\n","Data Type:\n","torch.float32\n"]}]},{"cell_type":"markdown","source":["\n","<img src = \"https://drive.google.com/uc?export=view&id=1822fQJQuXtzZ7DmO86pUXUj4aU9ZLor_\" width =600 >"],"metadata":{"id":"Hblncsd7wbVz"}},{"cell_type":"markdown","source":["## <font color = 'pickle'>**Higher Order Tensors**"],"metadata":{"id":"7USc0U8XsMbI"}},{"cell_type":"markdown","source":["\n","### <font color = 'pickle'>**rank-3 tensor**"],"metadata":{"id":"uuHbPHclzl7Z"}},{"cell_type":"code","source":["t = torch.tensor([\n","    [[1 , 2], [3,4]],\n","    [[5, 6], [7,8]],\n","    [[5, 6], [7,8]]\n","                  ])\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')\n","print(\"\\nnumber of dimensions:\", t.dim(), sep = \"\\n\")\n","print(\"\\nData Type:\", t.dtype, sep = \"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705969866785,"user_tz":360,"elapsed":588,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"a3c775be-91b7-43de-bdac-9872f7e031f7","id":"QkAU2DQbsMbO"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[1, 2],\n","         [3, 4]],\n","\n","        [[5, 6],\n","         [7, 8]],\n","\n","        [[5, 6],\n","         [7, 8]]])\n","\n","size:\n","torch.Size([3, 2, 2])\n","\n","number of dimensions:\n","3\n","\n","Data Type:\n","torch.int64\n"]}]},{"cell_type":"markdown","source":["<img src = \"https://drive.google.com/uc?export=view&id=184X0Qjn0lwuJRSFoh_lEmR9v7yF7GxaA\" width =600 >\n","\n","Image source: https://dev.to/sandeepbalachandran/machine-learning-going-furthur-with-cnn-part-2-41km"],"metadata":{"id":"H2oM6QJXyF7m"}},{"cell_type":"markdown","source":["### <font color = 'pickle'>**rank-4 tensor**"],"metadata":{"id":"ojkafKrazhSM"}},{"cell_type":"code","source":["t1 = torch.stack((t,t))\n","print(t1)\n","# print(t)\n","print(\"\\nsize:\",t1.shape, sep ='\\n')\n","print(\"\\nnumber of dimensions:\", t1.dim(), sep = \"\\n\")\n","print(\"\\nData Type:\", t1.dtype, sep = \"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RuHWcJh-zw8L","executionInfo":{"status":"ok","timestamp":1705970055149,"user_tz":360,"elapsed":2,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"faa95e6e-2b2d-47cc-852c-b086f81ed6a9"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[[1, 2],\n","          [3, 4]],\n","\n","         [[5, 6],\n","          [7, 8]],\n","\n","         [[5, 6],\n","          [7, 8]]],\n","\n","\n","        [[[1, 2],\n","          [3, 4]],\n","\n","         [[5, 6],\n","          [7, 8]],\n","\n","         [[5, 6],\n","          [7, 8]]]])\n","\n","size:\n","torch.Size([2, 3, 2, 2])\n","\n","number of dimensions:\n","4\n","\n","Data Type:\n","torch.int64\n"]}]},{"cell_type":"markdown","source":["<img src = \"https://drive.google.com/uc?export=view&id=189RzBY0oYuih-dZjNAT79FIVf3ZUp_HY\" width =600 >"],"metadata":{"id":"mH1bd2kaz5ZO"}},{"cell_type":"markdown","source":[" # <font color = 'pickle'> **Python list**"],"metadata":{"id":"vCIviyFV2m04"}},{"cell_type":"code","source":["scalar = 4\n","type(scalar)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3POAdGGfA12q","executionInfo":{"status":"ok","timestamp":1705970130128,"user_tz":360,"elapsed":317,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"63cda578-1d46-4a34-c36b-89388e1e5186"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["int"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["my_list = [[1., 2], [3,4]]\n","type(my_list)"],"metadata":{"id":"r8p1P6rC3IXq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705970135814,"user_tz":360,"elapsed":645,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"05f075f5-6c09-4744-930f-d5bc1d02dbed"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["list"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["my_tensor = torch.tensor([[1., 2], [3,4]])\n","type(my_tensor)\n","print(t)\n","print(\"\\nData Typr:\", my_tensor.dtype, sep = \"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1czex0Vr3MG-","executionInfo":{"status":"ok","timestamp":1705970144824,"user_tz":360,"elapsed":572,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"052d1212-6cb3-4661-f1d4-c3bffb5d7750"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[1, 2],\n","         [3, 4]],\n","\n","        [[5, 6],\n","         [7, 8]],\n","\n","        [[5, 6],\n","         [7, 8]]])\n","\n","Data Typr:\n","torch.float32\n"]}]},{"cell_type":"markdown","source":["# <font color = 'pickle'>**Difference between list and Array/tensor**</font>\n","\n","| <font size =5> Python List                       | <font size =5>Tensor/Array                     |\n","|-----------------------------------|----------------------------------|\n","| <font size =5>Mixed types allowed               | <font size =5>Same type required               |\n","|<font size =5> Elements can be added or removed  | <font size =5>Elements cannot be added or removed               \n","| <font size =5>Basic Python operations           | <font size =5>Supports mathematical operations                \n","|<font size =5>Numerical Computtaions are slow    |<font size =5>Numerical Computtaions are fast\n"],"metadata":{"id":"yfg-TJpb2rmg"}},{"cell_type":"markdown","source":["\n"],"metadata":{"id":"XfoOIXQK6Bm-"}},{"cell_type":"markdown","metadata":{"id":"Y6zZfHhsu92j"},"source":["# <font color = 'pickle'>**Conversion to other Python Objects**"]},{"cell_type":"code","metadata":{"id":"bHmHIXZB-Aw7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705970273792,"user_tz":360,"elapsed":4,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"338df9da-3b77-4421-da0a-c79c858ced9b"},"source":["# Initializing a tensor\n","t = torch.arange(10)\n","print(t)\n","\n","# Converting tensor t to numpy array using numpy() mehod\n","arr = t.numpy()\n","\n","# Converting numpy array to tensor T using tensor() method\n","T = torch.tensor(arr)\n","\n","# Printing data type of arr and T\n","print(type(arr), type(T), T.type(), sep='\\n')"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","<class 'numpy.ndarray'>\n","<class 'torch.Tensor'>\n","torch.LongTensor\n"]}]},{"cell_type":"markdown","metadata":{"id":"K8ot3be10F1e"},"source":["We can also use torch.from_numpy() and torch.as_tensor() to convert numpy array to PyTorch Tensor. However, with these methods, the PyTorch tensor and the source NumPy array share the same memory. This means that changes to one affect the other. However, the torch.tensor() function always makes a copy."]},{"cell_type":"code","metadata":{"id":"ZxfeLiNH1dB4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705970368338,"user_tz":360,"elapsed":569,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"f1fdf9dd-28ce-4c88-e908-e0dcb80ad22c"},"source":["my_ndarray = np.arange(10)\n","t_from_numpy = torch.from_numpy(my_ndarray)\n","t_as_tensor = torch.as_tensor(my_ndarray)\n","t_Tensor = torch.tensor(my_ndarray)\n","\n","print(f\"tensor craeted using torch.from_numpy before changing np array: {t_from_numpy}\")\n","print(f\"tensor craeted using torch.as_tensor before changing np array : {t_as_tensor}\")\n","print(f\"tensor craeted using torch.tensor before changing np array    : {t_Tensor}\")\n","\n","# change numpy array\n","my_ndarray[2] = 1000\n","\n","print()\n","print(f\"tensor craeted using torch.from_numpy after changing np array: {t_from_numpy}\")\n","print(f\"tensor craeted using torch.as_tensor after changing np array : {t_as_tensor}\")\n","print(f\"tensor craeted using torch.tensor after changing np array    : {t_Tensor}\")"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor craeted using torch.from_numpy before changing np array: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","tensor craeted using torch.as_tensor before changing np array : tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","tensor craeted using torch.tensor before changing np array    : tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","\n","tensor craeted using torch.from_numpy after changing np array: tensor([   0,    1, 1000,    3,    4,    5,    6,    7,    8,    9])\n","tensor craeted using torch.as_tensor after changing np array : tensor([   0,    1, 1000,    3,    4,    5,    6,    7,    8,    9])\n","tensor craeted using torch.tensor after changing np array    : tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"]}]},{"cell_type":"code","metadata":{"id":"g-ti5Hum-kCF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705970512646,"user_tz":360,"elapsed":287,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"ddeefb87-e054-4d1f-8fd0-67ef991a51d4"},"source":["# Initializing a size-1 tensor\n","t = torch.tensor([10.5])\n","\n","# Printing tensor\n","print(t)\n","\n","# Accessing element of tensor using item function\n","# item returns the value of the tensor as python number\n","# works only for tensors with single element\n","\n","print(t.item())"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([10.5000])\n","10.5\n"]}]},{"cell_type":"code","source":["# we an also conver the tensor to python list\n","t = torch.tensor([10, 2])\n","print(t)\n","print(t.tolist())"],"metadata":{"id":"BOV1rjjaRlck","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705970554352,"user_tz":360,"elapsed":338,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"56186f41-5b34-4802-df7a-97417f056d71"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([10,  2])\n","[10, 2]\n"]}]},{"cell_type":"markdown","source":["# <font color = 'pickle'>**Changing Shape of Tensors**"],"metadata":{"id":"5geGEsHIJZCb"}},{"cell_type":"code","source":["t = torch.arange(10)\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XfHgBWm1J5rg","executionInfo":{"status":"ok","timestamp":1705970567464,"user_tz":360,"elapsed":607,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"c53ffd1c-7e8d-4dc8-f408-84f82d76b7f2"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","\n","size:\n","torch.Size([10])\n"]}]},{"cell_type":"code","source":["t = t.view(5,2)\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FHsBhOyaKooN","executionInfo":{"status":"ok","timestamp":1705970593052,"user_tz":360,"elapsed":470,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"5253c7e0-8a36-4e48-fefc-d51174fd19fd"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 1],\n","        [2, 3],\n","        [4, 5],\n","        [6, 7],\n","        [8, 9]])\n","\n","size:\n","torch.Size([5, 2])\n"]}]},{"cell_type":"code","source":["t = t.view(-1,5)\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZQG9qWI2LVU2","executionInfo":{"status":"ok","timestamp":1705970636354,"user_tz":360,"elapsed":297,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"d9b51c82-5d65-4ed4-e6a3-59e752e0ffff"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 1, 2, 3, 4],\n","        [5, 6, 7, 8, 9]])\n","\n","size:\n","torch.Size([2, 5])\n"]}]},{"cell_type":"markdown","metadata":{"id":"pmZk-J0TzwE0"},"source":["# <font color = 'pickle'>**Changing datatype of Tensors**\n","When creating tensor we can pass the dtype as an argument. We can also change the datatype of tensors using to() and type() mehods. For a list of dtypes visit https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"]},{"cell_type":"code","metadata":{"id":"PoYt1PzhzuQw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705970747005,"user_tz":360,"elapsed":293,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"04ea7752-466d-498a-f31a-e4940fd0e2c5"},"source":["x = torch.tensor([8, 9, -3], dtype=torch.int)\n","\n","# we can use type() method or to() method to change the datatype\n","print(f\"Old: {x.dtype}\")\n","\n","# change the datatype to int64 using type() method\n","x = x.type(dtype=torch.int64)\n","print(f\"New: {x.dtype}\")\n","\n","# change the datatype to int32 using t0() method\n","x = x.to(dtype=torch.int32)\n","print(f\"Newer: {x.dtype}\")"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Old: torch.int32\n","New: torch.int64\n","Newer: torch.int32\n"]}]},{"cell_type":"markdown","metadata":{"id":"l1ugpAlJ6Lks"},"source":["# <font color = 'pickle'>**Saving Memory - inplace operations**"]},{"cell_type":"markdown","metadata":{"id":"Yhk6tvq_6THn"},"source":["In-place operation are operations that change the content of a given Tensor without making a copy.\n","\n","Operations that have a `_` suffix are in-place. For example: `.add_()`. Operations like += or *= are also inplace operations.\n","\n","We can also perform in-place opaeration usng the notation `Z[:] = <expression>`.\n","\n","As in-place operations do not make a copy, they can save memory. However, we need to use them carefully. They can be problematic when computing derivatives because of an immediate loss of history. We will learn about derivatives and computation graphs in coming lectures."]},{"cell_type":"code","source":["a = torch.tensor(10)\n","print(a)\n","print(id(a))\n","a += 1\n","print(a)\n","print(id(a))\n","a = a + 1\n","print(a)\n","print(id(a))\n","\n"],"metadata":{"id":"p2ArQFtpxgPH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705970943090,"user_tz":360,"elapsed":280,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"2d1dca1b-5a00-4803-e342-ea6d81e2945e"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(10)\n","139798185933888\n","tensor(11)\n","139798185933888\n","tensor(12)\n","139798195989744\n"]}]},{"cell_type":"code","source":["b = torch.tensor(10)\n","print(b)\n","print(id(b))\n","b.add_(1) # B += 1 memory efficient\n","print(b)\n","print(id(b))\n","b = b.add(1) # b = b +1 this is not\n","print(b)\n","print(id(b))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qxbDy9WmHOxD","executionInfo":{"status":"ok","timestamp":1705971003452,"user_tz":360,"elapsed":293,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"6db06d78-3442-479b-a591-7473b97548ae"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(10)\n","139798186083088\n","tensor(11)\n","139798186083088\n","tensor(12)\n","139798186082928\n"]}]},{"cell_type":"markdown","metadata":{"id":"X-nm1NrfbvIU"},"source":["## <font color = 'pickle'>**1) Checking gpu**"]},{"cell_type":"code","source":["torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Lgo7iCxy-Yr","executionInfo":{"status":"ok","timestamp":1705971022635,"user_tz":360,"elapsed":313,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"6800e93e-9d13-4f95-fdf3-eeb0f38b3882"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"1TFzHz0Thbjt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705971050563,"user_tz":360,"elapsed":401,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"73b2a2bd-ab42-4559-b29d-26ff8d4149c1"},"source":["# check if gpu is availaible\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}]},{"cell_type":"code","metadata":{"id":"W_drUk69u5ky","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705971060434,"user_tz":360,"elapsed":288,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"5981bf92-7862-4006-e4b7-37d681fdcf47"},"source":["# create a tensor\n","X = torch.tensor([1, 2, 3, 4])\n","x"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 8,  9, -3], dtype=torch.int32)"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"qiYTr-LTvAeT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705971068132,"user_tz":360,"elapsed":642,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"fa31291b-0697-4605-8a4b-082c528d4aa9"},"source":["# check the device attribute of the tensor\n","X.device"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"lTU-niOavIgd","executionInfo":{"status":"ok","timestamp":1705971083670,"user_tz":360,"elapsed":307,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"source":["# move the tensor to gpu\n","X = X.to(device=0)"],"execution_count":28,"outputs":[]},{"cell_type":"code","source":["X.device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I8XlGGYXbxZy","executionInfo":{"status":"ok","timestamp":1705971085486,"user_tz":360,"elapsed":4,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"cbb3c397-ca89-4524-d6f2-9c43ab2e0735"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"___FrjxyzYJ-","executionInfo":{"status":"ok","timestamp":1705971126768,"user_tz":360,"elapsed":604,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"6a53f4f4-814a-4b8f-983c-6e115320eb87"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"TahnwIQOvWZV","executionInfo":{"status":"ok","timestamp":1705971138172,"user_tz":360,"elapsed":304,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"source":["# it is more efficient to create the tensor on gpu directly\n","Y = torch.tensor([1, 2, 3], device=device)"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"YWQv_dEwviH7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705971141104,"user_tz":360,"elapsed":723,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"c931a632-8f6a-4b2f-bba5-7c1c2424e7b6"},"source":["# check the device attribute of the tensor\n","Y.device"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"mJyoahoFNVtP"},"source":["## <font color = 'pickle'>**2) Memory allocation of in-place operations**"]},{"cell_type":"code","metadata":{"id":"pqdfZVuhw5PG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705971279137,"user_tz":360,"elapsed":1340,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"2ad2563e-bf01-448b-8b8c-be377288afa5"},"source":["# create tensor\n","t1 = torch.randn(10000, 10000, device=\"cpu\")\n","\n","# move tensor to gpu\n","t1 = t1.to(device)\n","print(t1.device)\n","\n","# we can use id() function to get memory location of tensor\n","print(f\"initial memory location of tensor t1 is : {id(t1)}\")\n","\n","x = t1\n","print(f\"initial memory location of x is : {id(x)}\")\n","\n","# Waits for everything to finish running\n","torch.cuda.synchronize()\n","\n","# initial memory allocated\n","start_memory = torch.cuda.memory_allocated()\n","\n","# inplace operation\n","t1 += 0.1\n","t1.add_(0.1)\n","# since the operation was inplace when we update t1 it will update x as well\n","print(x == t1)\n","\n","print(f\"final memory location of tensor t1 is: {id(t1)}\")\n","print(f\"final location of x is : {id(x)}\")\n","\n","# totall memory allocated after function call\n","end_memory = torch.cuda.memory_allocated()\n","\n","# memory allocated because of function call\n","memory_allocated = end_memory - start_memory\n","print(memory_allocated / 1024**2)"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n","initial memory location of tensor t1 is : 139798207795008\n","initial memory location of x is : 139798207795008\n","tensor([[True, True, True,  ..., True, True, True],\n","        [True, True, True,  ..., True, True, True],\n","        [True, True, True,  ..., True, True, True],\n","        ...,\n","        [True, True, True,  ..., True, True, True],\n","        [True, True, True,  ..., True, True, True],\n","        [True, True, True,  ..., True, True, True]], device='cuda:0')\n","final memory location of tensor t1 is: 139798207795008\n","final location of x is : 139798207795008\n","0.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ti7xX-TSLp_3"},"source":["From the above example wecan see that both x and t1 has same memory location. When we ue in-place operation on t1, it also updates x"]},{"cell_type":"markdown","metadata":{"id":"yyQSz0iiNsgw"},"source":["## <font color = 'pickle'>**3) Memory allocation of out-of-place operations**"]},{"cell_type":"code","metadata":{"id":"d3NPUZxTxcmL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705971295497,"user_tz":360,"elapsed":1231,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"d69910dd-6d1d-45e1-bb1e-c5a1e8f0eaed"},"source":["# create tensor\n","t2 = torch.randn(10000, 10000, device=\"cpu\")\n","\n","# move tensor to gpu\n","t2 = t2.to(device)\n","print(t2.device)\n","\n","# we can use id() function to get memory location of tensor\n","print(f\"initial memory location of tensor t2 {id(t2)}\")\n","\n","y = t2\n","print(f\"final memory location of y is : {id(y)}\")\n","\n","# Waits for everything to finish running\n","torch.cuda.synchronize()\n","\n","# initial memory allocated\n","start_memory = torch.cuda.memory_allocated()\n","\n","# out-place opertaions\n","t2 = t2 + 0.1\n","\n","# since the operation was not inplace when we update t2 it will not update y\n","print(y == t2)\n","\n","# we can use id() function to get memory location of tensor\n","print(f\"final memory location of tensor t2 {id(t2)}\")\n","print(f\"final memory location of y is : {id(y)}\")\n","\n","# totall memory allocated after function call\n","end_memory = torch.cuda.memory_allocated()\n","\n","# memory allocated because of function call\n","memory_allocated = end_memory - start_memory\n","print(memory_allocated / 1024**2)"],"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n","initial memory location of tensor t2 139798205891904\n","final memory location of y is : 139798205891904\n","tensor([[False, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        ...,\n","        [False, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False]], device='cuda:0')\n","final memory location of tensor t2 139798186182992\n","final memory location of y is : 139798205891904\n","382.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"9kli0fOuMGCM"},"source":["From the above example we can see that initially both y and t2 has same memory location. After running t2 = t2 + 0.1, we will find that id(t2) points to a different location. That is because Python first evaluates t2 + 0.1, allocating new memory for the result and then makes t2 point to this new location in memory. Since we have not done in-place operation, updating t2 does not effect y. y still points to the same memory location."]},{"cell_type":"markdown","source":["# <font color = 'pickle'>**Linear Algebra**"],"metadata":{"id":"7aJjhm1lMQ3m"}},{"cell_type":"markdown","source":["## <font color = 'pickle'>**Dot product**\n","Dot product of 2 vectors x and y  is given by the summation of product of elements at the same position.\n","\n","If we have 2 vectors x: [1, 2, 3, 4] and y: [1, 1, 2, 1]\n","\n","(x.y) will be 1x1 + 2x1 + 3x2 + 4x1 = 13"],"metadata":{"id":"E5a8iDgbMajD"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KD4_9ZUxQ2Hp","executionInfo":{"status":"ok","timestamp":1692660879835,"user_tz":300,"elapsed":98,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"07550bb6-a54b-4959-ec1b-11f2f2ca1298"},"source":["# Initializing 2 tensors\n","x = torch.Tensor([0, -1, 1, 0])\n","y = torch.Tensor([0, 1, 1, 0])\n","\n","# Performing Dot product\n","torch.dot(x, y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.)"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0NRdtGrvSGCj","executionInfo":{"status":"ok","timestamp":1692660895267,"user_tz":300,"elapsed":75,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"416815c1-9e7e-4257-d5d0-e0572fe08f29"},"source":["# Dot Product is equal to sum of products at the same position, thus the expression below will give similar result\n","torch.sum(x * y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.)"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["# Initializing 2 tensors\n","x = torch.Tensor([1, 0, 0, 1])\n","y = torch.Tensor([1, 0, 0, 1])\n","\n","# Performing Dot product\n","torch.dot(x, y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cp5-XFyNZKSt","executionInfo":{"status":"ok","timestamp":1705971506828,"user_tz":360,"elapsed":666,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"be9d0976-5859-42ec-f1ee-82309dd5358e"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.)"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","source":["## <font color = 'pickle'>**Dot product vs. for Loop in Python**"],"metadata":{"id":"BS-6BCNSOPLx"}},{"cell_type":"code","source":["import time\n","n = 1000000\n","a = torch.arange(n)\n","b = torch.arange(n)\n","\n","def pytorch_dot(x, y):\n","    return x.dot(y)"],"metadata":{"id":"olJVVgxAOx8j","executionInfo":{"status":"ok","timestamp":1705971528069,"user_tz":360,"elapsed":610,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["%timeit pytorch_dot(a,b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-HGjTW_FSE5l","executionInfo":{"status":"ok","timestamp":1705971572987,"user_tz":360,"elapsed":8383,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"59743f71-33fb-42b0-ac70-5fde441f8ce5"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["998 µs ± 261 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"]}]},{"cell_type":"code","source":["x = [1,2]\n","y = [3,4]\n","list(zip(x,y))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PHTeeIIddLkz","executionInfo":{"status":"ok","timestamp":1705971599064,"user_tz":360,"elapsed":850,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"77bc7f30-d69d-4b98-b51f-fafde69337d9"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(1, 3), (2, 4)]"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["a1 = a.tolist()\n","b1 = b.tolist()\n","def plain_python(x, y):\n","    output = 0\n","    for x_j, y_j in zip(x, y):\n","        output += x_j * y_j\n","    return output\n"],"metadata":{"id":"0K2kSXHeSYbU","executionInfo":{"status":"ok","timestamp":1705971642956,"user_tz":360,"elapsed":321,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["%timeit plain_python(a,b)"],"metadata":{"id":"Lo8KxBAQS5kX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705971707987,"user_tz":360,"elapsed":59044,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"63678211-a50d-4f12-a1a1-79d4c127db2a"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["7.25 s ± 423 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"]}]},{"cell_type":"markdown","source":["**Output 1:**\n","\n","921 µs ± 182 µs per loop: This tells you that the code took an average of 921 microseconds (µs) to run for each loop, with a standard deviation of 182 microseconds. The standard deviation gives an indication of the variability in the timing across different runs, which can be affected by other processes running on the computer at the same time.\n","\n","(mean ± std. dev. of 7 runs, 1000 loops each): This part provides details about how the timing was measured. The code was run 7 times, and each of those runs consisted of 1000 loops. The mean and standard deviation were calculated from these 7 runs.\n","\n","**Output 2:**\n","6.78 s ± 392 ms per loop: This tells you that the code took an average of 6.78 seconds to run for each loop, with a standard deviation of 392 milliseconds. Since 1 second equals 1000 milliseconds, this standard deviation is less than half a second.\n","\n","(mean ± std. dev. of 7 runs, 1 loop each): Similar to Output 1, this part tells you that the code was run 7 times, and each of those runs consisted of just 1 loop. The mean and standard deviation were calculated from these 7 runs.\n","\n","**In comparison, Output 1 suggests a much faster execution time (in the order of microseconds) compared to Output 2 (in the order of seconds).**\n","\n","\n","\n","\n","\n"],"metadata":{"id":"Rk8Kt4uJSBOp"}},{"cell_type":"markdown","metadata":{"id":"bUzXoOeG_rfX"},"source":["## <font color = 'pickle'>**Operations on Metrices**"]},{"cell_type":"code","source":["# Creating 2 matrices\n","\n","# First matrix\n","A = torch.arange(0, 25).reshape(5, 5)\n","\n","# Second matrix : copy of A\n","B = A.clone()\n","\n","print(A)\n","print(B)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"46A32IAZV0ky","executionInfo":{"status":"ok","timestamp":1705971775910,"user_tz":360,"elapsed":307,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"78534bce-bfab-4289-ffcb-b26d49941fd4"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0,  1,  2,  3,  4],\n","        [ 5,  6,  7,  8,  9],\n","        [10, 11, 12, 13, 14],\n","        [15, 16, 17, 18, 19],\n","        [20, 21, 22, 23, 24]])\n","tensor([[ 0,  1,  2,  3,  4],\n","        [ 5,  6,  7,  8,  9],\n","        [10, 11, 12, 13, 14],\n","        [15, 16, 17, 18, 19],\n","        [20, 21, 22, 23, 24]])\n"]}]},{"cell_type":"markdown","source":["### <font color = 'pickle'>**Addition of 2 matrices**"],"metadata":{"id":"D7ENj3dtV9yR"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0VFFcFWq5ZJZ","executionInfo":{"status":"ok","timestamp":1705971787790,"user_tz":360,"elapsed":614,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"a1b8681e-6629-4d52-8950-f99b81e82805"},"source":["# Addition of 2 matrices\n","A + B"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0,  2,  4,  6,  8],\n","        [10, 12, 14, 16, 18],\n","        [20, 22, 24, 26, 28],\n","        [30, 32, 34, 36, 38],\n","        [40, 42, 44, 46, 48]])"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["### <font color = 'pickle'>**Subtraction of 2 matrices**"],"metadata":{"id":"nbOc6KIXWD_N"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oycCtJiY5jPM","executionInfo":{"status":"ok","timestamp":1705971790581,"user_tz":360,"elapsed":3,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"1e48f1fd-e432-4f94-a845-74741e1a42e1"},"source":["# Subtraction of 2 matrices\n","A - B"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0]])"]},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","source":["### <font color = 'pickle'>**Multiplying Matrices with Scalars**"],"metadata":{"id":"dogJlUvrWYhx"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ISBiKAus-AKu","executionInfo":{"status":"ok","timestamp":1705971809749,"user_tz":360,"elapsed":297,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"250b4c23-da2f-41f6-c83f-5dbf734c204e"},"source":["# Each element of matrix can be aded or multiplied by a scalar (broadcasting)\n","# This operation will not change the shape of a matrix or a Tensor\n","a = 2\n","print(a + A)\n","print()\n","print(a * A)"],"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 2,  3,  4,  5,  6],\n","        [ 7,  8,  9, 10, 11],\n","        [12, 13, 14, 15, 16],\n","        [17, 18, 19, 20, 21],\n","        [22, 23, 24, 25, 26]])\n","\n","tensor([[ 0,  2,  4,  6,  8],\n","        [10, 12, 14, 16, 18],\n","        [20, 22, 24, 26, 28],\n","        [30, 32, 34, 36, 38],\n","        [40, 42, 44, 46, 48]])\n"]}]},{"cell_type":"markdown","source":["### <font color = 'pickle'>**Transpose of a Matrix**"],"metadata":{"id":"l_iPeyTzWlXA"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sHAObZ7g5nhj","executionInfo":{"status":"ok","timestamp":1705971818776,"user_tz":360,"elapsed":636,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"b936294a-8f28-4e48-d059-eddbce15019d"},"source":["# Transpose of a matrix : Elements of the rows and columns get interchanged a[i][j] becomes a[j][i]\n","# Transpose is a special case of permute\n","A.T"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0,  5, 10, 15, 20],\n","        [ 1,  6, 11, 16, 21],\n","        [ 2,  7, 12, 17, 22],\n","        [ 3,  8, 13, 18, 23],\n","        [ 4,  9, 14, 19, 24]])"]},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","source":["### <font color = 'pickle'>**Hadamard product**"],"metadata":{"id":"tsAkl0FDWQ-T"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7aua8kx79WOV","executionInfo":{"status":"ok","timestamp":1705971835289,"user_tz":360,"elapsed":716,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"e089aed8-226b-430d-8cfd-b4dff7d71afd"},"source":["# Elementwise multiplication of two metrices is called Hadamard product\n","A * B"],"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[  0,   1,   4,   9,  16],\n","        [ 25,  36,  49,  64,  81],\n","        [100, 121, 144, 169, 196],\n","        [225, 256, 289, 324, 361],\n","        [400, 441, 484, 529, 576]])"]},"metadata":{},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"Ys0KKwAFTHpk"},"source":["### <font color = 'pickle'>**Matrix Multiplication**\n","\n","Matrix multiplication is a binary operation on 2 matrices which gives us a matrix which is the product of the 2 matrices.\n","\n","If we are given 2 matrices $A$ of shape $(m * n)$ and $B$ of shape $(q * p)$, **we can perform matrix multiplication only when $n = q$** and the resultant product matrix will have shape $(m * p)$.\n","\n","Suppose we are given 2 matrices $A (m * n)$ and $B (n * p)$:\n","\n","$$\\mathbf{A}=\\begin{bmatrix}\n"," a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n"," a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n"," a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n","\\end{bmatrix},\\quad\n","\\mathbf{B}=\\begin{bmatrix}\n"," b_{11} & b_{12} & \\cdots & b_{1p} \\\\\n"," b_{21} & b_{22} & \\cdots & b_{2p} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n"," b_{n1} & b_{n2} & \\cdots & b_{np} \\\\\n","\\end{bmatrix}$$\n","\n","Then after performing matrix multiplication, the resultant matrix C = AB will be:\n","\n","$$\\mathbf{C}=\\begin{bmatrix}\n"," c_{11} & c_{12} & \\cdots & c_{1p} \\\\\n"," c_{21} & c_{22} & \\cdots & c_{2p} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n"," c_{mp} & c_{mp} & \\cdots & c_{mp} \\\\\n","\\end{bmatrix}$$\n","\n","Here, $c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + ... a_{in}b_{b_nj} = \\sum_{k = 1}^n a_{ik}b_{kj}$\n","\n","for, $i = 1,....m$ and $j = 1,...p$\n","\n","\n","Thus, each element of C, $c_{ij}$ is obtained by dot product of $i^{th}$ row of $A$ and $j^{th}$ column of $B$.\n","\n","**Example** :\n","  1. Let $A$ be a matrix of (4, 3) dimensions.\n","  2. Let $B$ be another matrix of (3, 2) dimensions.\n","  3. Let us denote denote the matrix multiplication of $A$ and $B$ with $C$.\n","  5. Then the dimension of $C$ = (number of rows of $A$,number of columns of $B$)\n","     \n","    dimension of  $C$ = (4, 2)\n","\n","The figure given below will give a good example of matrix multplication :\n","\n","<img src = \"https://drive.google.com/uc?view=export&id=176DF50XdtwkqU5wvxtWuD75sRDHvSJBf\" width =\"250\"/>\n","\n","We can perform matrix multiplication in the following way using PyTorch:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KAGYwiI4SykF","executionInfo":{"status":"ok","timestamp":1705971951595,"user_tz":360,"elapsed":672,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"17525990-d485-4153-e0e8-0bfbe7702736"},"source":["# Initializing 2 matrices\n","A = torch.arange(0, 10, dtype=float).reshape(2, 5)\n","B = torch.ones(5, 2, dtype=float)\n","\n","# Matrix-Matrix Multiplication using mm function of PyTorch\n","torch.mm(A, B)"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[10., 10.],\n","        [35., 35.]], dtype=torch.float64)"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["A @ B"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2BaJkVZc1gGE","executionInfo":{"status":"ok","timestamp":1705971953912,"user_tz":360,"elapsed":616,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"508bf39b-0ad5-4ed3-c0b6-3ecb9aaf08aa"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[10., 10.],\n","        [35., 35.]], dtype=torch.float64)"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["A * B"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":181},"id":"Bi0LdqQL1iwQ","executionInfo":{"status":"error","timestamp":1705971964550,"user_tz":360,"elapsed":627,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"729808be-8227-44e5-abc9-ed192a1ecaef"},"execution_count":49,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 1","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-49-a4cedde81ed0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mA\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 1"]}]},{"cell_type":"markdown","source":["#### <font color = 'pickle'>**Prediction on Multiple Training Examples via Matrix Multiplication**"],"metadata":{"id":"cI4L4lCPbCar"}},{"cell_type":"code","source":["bias = torch.tensor([0.])\n","theta = torch.tensor([0.2, 9.3])\n","theta = theta.view(-1,1)\n","X = torch.tensor(\n","   [[1.8, 9.2],\n","    [0.2, 3.3],\n","    [5.2, 3.4],\n","    [3.4, 4.5],\n","    [6.1, 7.1]]\n",")\n","print(X.shape, theta.shape, bias.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_1-orC5SbYgs","executionInfo":{"status":"ok","timestamp":1705972116307,"user_tz":360,"elapsed":301,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"fc5fb015-00d8-4de0-f0aa-6d899e71b510"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 2]) torch.Size([2, 1]) torch.Size([1])\n"]}]},{"cell_type":"code","source":["predictions = X.matmul(theta) + bias\n","predictions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KLTjgJt9cfSu","executionInfo":{"status":"ok","timestamp":1705972120890,"user_tz":360,"elapsed":598,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"12ab4b83-3b81-434a-fd39-9738de1a3464"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[85.9200],\n","        [30.7300],\n","        [32.6600],\n","        [42.5300],\n","        [67.2500]])"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","source":["# <font color = 'pickle'>**Self Study**"],"metadata":{"id":"EUh62jYWUZvj"}},{"cell_type":"markdown","metadata":{"id":"khxj8DIV5Eic"},"source":["## <font color = 'pickle'>**Broadcasting - Operations on tensors of different  size**\n","\n","Broadcasting describes how a tensor has to be treated during arithematic operation. If we have tensors of different sizes, we can broadcast the smaller array across the larger one so that they can have comaptible shapes.\n","\n","\n","\n"]},{"cell_type":"markdown","source":["### <font color = 'pickle'>**Broadcasting Examples**</font>"],"metadata":{"id":"u9J4-PQRPRtk"}},{"cell_type":"markdown","source":["#### <font color = 'pickle'>**Broadcasting with a scalar**</font>"],"metadata":{"id":"1ZxySDAIPtMW"}},{"cell_type":"code","source":["t= torch.tensor([1,-2, 4])"],"metadata":{"id":"HIuRF2yiPcAn","executionInfo":{"status":"ok","timestamp":1705972197922,"user_tz":360,"elapsed":2,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["t > 0"],"metadata":{"id":"AjfLaYt5PiHz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705972203542,"user_tz":360,"elapsed":5,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"4872dde4-9729-429c-aa98-4728d9a3af30"},"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ True, False,  True])"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["t2 = torch.tensor([[12, 16, 14], [13, 17, 13], [14, 18, 12]])\n","t2 * 2"],"metadata":{"id":"nUqLAPSoPjmC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705972208496,"user_tz":360,"elapsed":709,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"a9785d9e-bd0e-43c9-ae91-4967f76bad64"},"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[24, 32, 28],\n","        [26, 34, 26],\n","        [28, 36, 24]])"]},"metadata":{},"execution_count":54}]},{"cell_type":"markdown","source":["#### <font color = 'pickle'>**Broadcasting a vector to matrix**</font>"],"metadata":{"id":"TCNMwObZQEDR"}},{"cell_type":"code","source":["t2 = torch.tensor([[12, 16, 14], [13, 17, 13], [14, 18, 12]])\n","t1 = torch.tensor([1, 2, 3])\n","print(t2.shape)\n","print(t1.shape)\n","print(t1 + t2)"],"metadata":{"id":"XowS3mFpQUdc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705972231268,"user_tz":360,"elapsed":307,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"19b34372-ff00-439d-acd6-0fb31cd145f5"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 3])\n","torch.Size([3])\n","tensor([[13, 18, 17],\n","        [14, 19, 16],\n","        [15, 20, 15]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"H9Vet8dAPu5O"},"source":["### <font color = 'pickle'>**1) Understanding how broadcasting works**\n","\n","* The following image describes how a tensor of 2 dimensional tensor will be added to a 1 dimensional tensor\n","<img src=\"https://drive.google.com/uc?export=view&id=1QG2GO1owGpyXbcugJFVFGb4o_buV4s3j\" width=\"500\"/>"]},{"cell_type":"code","metadata":{"id":"bR4BtZeAPu5Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705972283834,"user_tz":360,"elapsed":612,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"678f510c-b78f-4c89-f2e0-5209ec7db072"},"source":["# create tensor\n","t2 = torch.tensor([[12, 16, 14], [13, 17, 13], [14, 18, 12]])\n","t1 = torch.tensor([1, 2, 3])\n","print(t2.shape)\n","print(t1.shape)"],"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 3])\n","torch.Size([3])\n"]}]},{"cell_type":"code","source":["print(t1.storage())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QFyWgq3Ae1QV","executionInfo":{"status":"ok","timestamp":1705972288592,"user_tz":360,"elapsed":319,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"1d643d9d-161d-4a09-b7c4-d5e7af7bdb09"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":[" 1\n"," 2\n"," 3\n","[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 3]\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-57-623047f164ba>:1: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  print(t1.storage())\n"]}]},{"cell_type":"code","metadata":{"id":"aC7ihRDUPu5S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705972320169,"user_tz":360,"elapsed":318,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"6990732b-9d32-43af-b6f5-1a3f246c9bcb"},"source":["t1_mod = t1.expand_as(t2)\n","t1_mod.shape"],"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 3])"]},"metadata":{},"execution_count":58}]},{"cell_type":"markdown","source":["Although it appears as though we are copying the rows, we are actually not duplicating them."],"metadata":{"id":"PchW_tUmMdIw"}},{"cell_type":"code","source":["print(t1_mod.storage())"],"metadata":{"id":"WvOz2svpMoo2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705972324296,"user_tz":360,"elapsed":658,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"5ef0a706-7d14-4db3-855a-ba648ff88312"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":[" 1\n"," 2\n"," 3\n","[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 3]\n"]}]},{"cell_type":"code","source":["t1_mod"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AlaaXuAm4Aim","executionInfo":{"status":"ok","timestamp":1705972342320,"user_tz":360,"elapsed":642,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"3aedaa7a-ff0e-42de-fa79-a87f55860bb5"},"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3],\n","        [1, 2, 3],\n","        [1, 2, 3]])"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"se8ZvnApPu5U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705972354990,"user_tz":360,"elapsed":694,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"52ba1101-fe9c-4e13-fbc3-c27591f03ff1"},"source":["print(t1_mod + t2)"],"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[13, 18, 17],\n","        [14, 19, 16],\n","        [15, 20, 15]])\n"]}]},{"cell_type":"code","metadata":{"id":"D-H5uc89Pu5V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705972384474,"user_tz":360,"elapsed":738,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"5e4d0706-e5ae-44a3-aa87-25751b16635d"},"source":["# we can check that it gives us the same result if we simply add t1 and t2\n","# so broadcasting is an efficient way of performing operations on tensors of unequal sizes\n","print(t1 + t2)"],"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[13, 18, 17],\n","        [14, 19, 16],\n","        [15, 20, 15]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"zyhE_dEUQwLk"},"source":["### <font color = 'pickle'>**2) Rules for Broadcasting**</font>\n","Broadcasting can only happen if the two tensors are broadcastable. Conditions for broadcasting:\n","\n","- Each tensor has at least one dimension.\n","\n","- When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n","\n","Examples:\n","\n","1. `t1(5, 8, 10) t2(5, 8, 10)`\n","Same size -> Broadcasting possible.\n","2. `t1((0,)) t2(5, 8, 10)`\n","t1 doesn't have atleast one dimension -> Broadcasting not possible.\n","3. `t1(5, 8, 10, 1) t2(8, 1, 1)` Broadcasting possible. Reasons:\n","  - 1st trailing position : both have size 1\n","  - 2nd trailing position : t2 has size 1\n","  - 3rd trailing position : both have size 8\n","  - 4th training position: t2 size doesn't exist but t2 has atleast 1 dimension."]},{"cell_type":"code","source":["1, 8,1,1"],"metadata":{"id":"X2_M8s9N4qq8"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x1BuDjiW4MNx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705972564372,"user_tz":360,"elapsed":612,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"41b541f3-2776-4081-83f2-120cd0f7e56e"},"source":["# Broadcasting\n","t1 = torch.empty(5, 8, 10, 1)\n","t2 = torch.empty(  8, 1, 1,)\n","(t1 + t2).size()"],"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([5, 8, 10, 1])"]},"metadata":{},"execution_count":63}]},{"cell_type":"markdown","metadata":{"id":"kjZwjy9d8wbD"},"source":["The dimensions after broadcasting will be:\n","\n","- If the number of dimensions are\n"," not equal, prepend 1 to the dimensions of the tensor with fewer dimensions to make them equal length.\n","\n","- Then, for each dimension size, the resulting dimension size is the max of the sizes along that dimension."]},{"cell_type":"code","metadata":{"id":"AqqPPTU78v83","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705972581235,"user_tz":360,"elapsed":807,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"55840556-bd27-4fe9-e58e-8c540df3c59a"},"source":["# Another example for broadcasting\n","t1 = torch.empty(1)\n","t2 = torch.empty(3, 1, 7)\n","(t1 + t2).size()"],"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 1, 7])"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","metadata":{"id":"02HFhB1S9QfX","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1692584101140,"user_tz":300,"elapsed":372,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"a45ad648-406d-43e4-9537-316cccb96ff8"},"source":["# Example where broadcasting is not possible\n","t1 = torch.empty(5, 8, 10, 1)\n","t2 = torch.empty  ( 3, 1, 1)\n","(t1 + t2).size()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-335-869bb92903e6>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (3) at non-singleton dimension 1"]}]},{"cell_type":"markdown","metadata":{"id":"wru9LKgg9f5G"},"source":["Here, at third trailing position sizes are not equal and none of them is 1, thus broadcasting is not possible."]},{"cell_type":"markdown","metadata":{"id":"UeOLV3qj7DTS"},"source":["## <font color = 'pickle'>**Reduction**\n","\n","We can calculate the sum of all elemnets of a vector or a matrix of any shape. This can be done using the ***sum*** function.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RleGFr-96dnx","executionInfo":{"status":"ok","timestamp":1692584105327,"user_tz":300,"elapsed":577,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"17a067e0-8c21-4e6a-ca2c-33f6359464f3"},"source":["# Creating a vector\n","x = torch.arange(5)\n","print(x)\n","\n","# This will do summation of all the elements of the vector : 0 + 1 + 2 + 3 + 4 = 10\n","print(x.sum())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0, 1, 2, 3, 4])\n","tensor(10)\n"]}]},{"cell_type":"code","metadata":{"id":"YC7ffhAU9bnO","executionInfo":{"status":"ok","timestamp":1692584106938,"user_tz":300,"elapsed":2,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3389cd22-ab85-4ed4-d5ff-3ca636affb5e"},"source":["# Creating a matrix\n","X = torch.arange(0, 10).reshape(2, 5)\n","X = X.to(torch.float32)\n","print(X)\n","\n","# This will do summation of all the elements of the matrix\n","print(X.sum())\n","# This will takle the mean of all teh elements\n","print(X.mean())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 1., 2., 3., 4.],\n","        [5., 6., 7., 8., 9.]])\n","tensor(45.)\n","tensor(4.5000)\n"]}]},{"cell_type":"markdown","metadata":{"id":"IRQX5pa6-r6V"},"source":["We can also calculate the mean or average of all elements in a vector or a matrix by dividing the sum of elements by no. of elements."]},{"cell_type":"markdown","metadata":{"id":"k6Ti1nNrBP4f"},"source":["By default, invoking the sum/mean finction on a tensor will give us a scaler (reduces the tensor along all its axes)\n","\n","We can also calculate sum, along the rows or columns by specifying the value of parameter \"axis\".\n","\n","axis = 0 will calculate sum along the rows while axis = 1 will calculate sum along the columns.\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ifrBSeGm_P6Z","executionInfo":{"status":"ok","timestamp":1692584109993,"user_tz":300,"elapsed":371,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"fbff884f-2fc4-460b-da10-383f200b39c4"},"source":["# Creating a matrix A\n","A = torch.arange(0, 15, dtype = float).reshape(5, 3)\n","A"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.,  1.,  2.],\n","        [ 3.,  4.,  5.],\n","        [ 6.,  7.,  8.],\n","        [ 9., 10., 11.],\n","        [12., 13., 14.]], dtype=torch.float64)"]},"metadata":{},"execution_count":338}]},{"cell_type":"code","metadata":{"id":"MSk9Q9uyQHL7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584112647,"user_tz":300,"elapsed":405,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"914cef2c-6424-44c9-e83f-117f5220e725"},"source":["# Sum of elements along axis = 0\n","# row sum for each column\n","# Since we are taking sum along axis = 0, the input tensor reduces along axis 0\n","# The shape of A was ([5,3])\n","# After invoking sum along axis = 0, the shape reduces to ([3])\n","print(f'Shape before rediction{A.shape}')\n","A.sum(axis = 0), A.sum(axis=0).shape"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape before redictiontorch.Size([5, 3])\n"]},{"output_type":"execute_result","data":{"text/plain":["(tensor([30., 35., 40.], dtype=torch.float64), torch.Size([3]))"]},"metadata":{},"execution_count":339}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YRsJKUOcQG7a","executionInfo":{"status":"ok","timestamp":1692584114392,"user_tz":300,"elapsed":512,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"be13847a-14d8-4ac6-d62f-47048714e4a1"},"source":["# Sum of elements along axis =  1\n","# column sum for each row\n","# Since we are taking sum along axis = 1, the input tensor reduces along axis 1\n","# The shape of A was ([5,3])\n","# After invoking sum along axis = 1, the shape redices to ([5])\n","A.sum(axis = 1), A.sum(axis = 1).shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([ 3., 12., 21., 30., 39.], dtype=torch.float64), torch.Size([5]))"]},"metadata":{},"execution_count":340}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"421spORkPFph","executionInfo":{"status":"error","timestamp":1692584115960,"user_tz":300,"elapsed":367,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"af270d1e-0af8-40c4-8be7-b8390ee889ad"},"source":["# Rules of broadcasting:  A and A.sum(axis=1) are not broadcastable\n","print(A/A.sum(axis=1))\n","\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-341-d93b8912f840>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Rules of broadcasting:  A and A.sum(axis=1) are not broadcastable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 1"]}]},{"cell_type":"markdown","metadata":{"id":"Pt8xjJ8w8sba"},"source":["## <font color = 'pickle'>**Non-Reduction Sum**"]},{"cell_type":"markdown","metadata":{"id":"YEp87iCYDX3h"},"source":["As seem in above examples, invoking sum() or mean() will reduce number of dimensions. We can keep number of axis unchanged by passing argument keepdims = True."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KGZhGYCLIM0S","executionInfo":{"status":"ok","timestamp":1692584120240,"user_tz":300,"elapsed":466,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"7f7fb6f9-d964-4c84-d97f-d2ca2561a26a"},"source":["# When we pass argument keepdim=True, the shape will now be ([5,1]. The output has 2-dimensions\n","# if we do not pass the argument keepdim=True, the shape will be ([5]). The output has one-dimension\n","sum_A_0 = A.sum(axis=1, keepdim=True)\n","print(sum_A_0.shape)\n","print(sum_A_0)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 1])\n","tensor([[ 3.],\n","        [12.],\n","        [21.],\n","        [30.],\n","        [39.]], dtype=torch.float64)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zQQoa2RlJn-J","executionInfo":{"status":"ok","timestamp":1692584121609,"user_tz":300,"elapsed":4,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"2f9d02d5-25f2-4898-a085-ca6e5926a604"},"source":["# Let us now try operation : A/(sum(A, axis=0))\n","print(A/A.sum(axis=1, keepdim=True))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.0000, 0.3333, 0.6667],\n","        [0.2500, 0.3333, 0.4167],\n","        [0.2857, 0.3333, 0.3810],\n","        [0.3000, 0.3333, 0.3667],\n","        [0.3077, 0.3333, 0.3590]], dtype=torch.float64)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2LexuntQ6KMY","executionInfo":{"status":"ok","timestamp":1692584122948,"user_tz":300,"elapsed":2,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"a10200ed-4d6a-41ab-ba5e-ee3f9a0967c0"},"source":["A"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.,  1.,  2.],\n","        [ 3.,  4.,  5.],\n","        [ 6.,  7.,  8.],\n","        [ 9., 10., 11.],\n","        [12., 13., 14.]], dtype=torch.float64)"]},"metadata":{},"execution_count":344}]},{"cell_type":"code","source":["A = torch.randint(0, 10 , (2,3,4))"],"metadata":{"id":"01wpGjMFbj3e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["  A"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FYWuI5K4b0DB","executionInfo":{"status":"ok","timestamp":1692584125736,"user_tz":300,"elapsed":3,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"d535f069-3805-4ed1-be4c-191eac66353f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[4, 1, 9, 9],\n","         [9, 0, 1, 2],\n","         [3, 0, 5, 5]],\n","\n","        [[2, 9, 1, 8],\n","         [8, 3, 6, 9],\n","         [1, 7, 3, 5]]])"]},"metadata":{},"execution_count":346}]},{"cell_type":"code","source":["A.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VCKO6sBgb3Kd","executionInfo":{"status":"ok","timestamp":1692584127410,"user_tz":300,"elapsed":363,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"ffb127b9-a8b8-4edb-ef0f-4b3dc7f353f7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 3, 4])"]},"metadata":{},"execution_count":347}]},{"cell_type":"code","source":["B = A.sum(axis = 1, keepdim=True)"],"metadata":{"id":"7w5hzOz3b5t8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["B.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cw9CKv3ub_4r","executionInfo":{"status":"ok","timestamp":1692584130495,"user_tz":300,"elapsed":354,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"05d4443b-76ac-4f6d-e339-59e354a5e627"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 1, 4])"]},"metadata":{},"execution_count":349}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pyENwjkxDSkP","executionInfo":{"status":"ok","timestamp":1692584132946,"user_tz":300,"elapsed":479,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"b0b03799-e9dc-4d25-990f-1491bfdf406f"},"source":["# Cumulative sum of elements along rows\n","A.cumsum(axis = 0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 4,  1,  9,  9],\n","         [ 9,  0,  1,  2],\n","         [ 3,  0,  5,  5]],\n","\n","        [[ 6, 10, 10, 17],\n","         [17,  3,  7, 11],\n","         [ 4,  7,  8, 10]]])"]},"metadata":{},"execution_count":350}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8OYWH9PMAt4g","executionInfo":{"status":"ok","timestamp":1692584135048,"user_tz":300,"elapsed":361,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"40610ddb-5a19-4bce-8c51-d4cd6cafd9d3"},"source":["# Cumulative sum of elements along columns\n","A.cumsum(axis = 1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 4,  1,  9,  9],\n","         [13,  1, 10, 11],\n","         [16,  1, 15, 16]],\n","\n","        [[ 2,  9,  1,  8],\n","         [10, 12,  7, 17],\n","         [11, 19, 10, 22]]])"]},"metadata":{},"execution_count":351}]},{"cell_type":"markdown","metadata":{"id":"v_4VmMUwl826"},"source":["## <font color = 'pickle'>**Accessing elements of a Tensor**\n","\n","We can access individual elements of a Tensor using **index values**. Indexing always **starts from 0**.\n","\n","For example if the tensor is: `[10, 12, 31, 34]`\n","\n","Index of 10 is 0, index of 12 is 1 and so on."]},{"cell_type":"code","metadata":{"id":"G6nbzmQxkkIp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584137627,"user_tz":300,"elapsed":322,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"06a49ccb-3602-4a27-cbb2-81e9a262da2a"},"source":["t1 = torch.tensor([[1, 2, 5], [7, 8, 9]])\n","\n","# Printing all elements\n","print(t1)\n","\n","# Get the first row\n","print(t1[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 2, 5],\n","        [7, 8, 9]])\n","tensor([1, 2, 5])\n"]}]},{"cell_type":"code","source":["# Get the first element of the second row\n","print(t1[1][0])"],"metadata":{"id":"03U-6P0JFbse","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584139341,"user_tz":300,"elapsed":335,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"0a09bba7-f893-40c4-9e57-d422eeb633d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(7)\n"]}]},{"cell_type":"code","source":["# Get the first element of the second row\n","t1[1, 0]"],"metadata":{"id":"-DeI6GRw31bK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584141054,"user_tz":300,"elapsed":526,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"e3ed43ca-d169-47fc-e54d-9e24099d94e5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(7)"]},"metadata":{},"execution_count":354}]},{"cell_type":"markdown","metadata":{"id":"MDgRsn4wuoVA"},"source":["## <font color = 'pickle'>**Accessing a Sub-Tensor**"]},{"cell_type":"code","metadata":{"id":"CwK4E8muunYb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584142409,"user_tz":300,"elapsed":6,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"32c1c1ad-56f2-4815-ca25-11943eb45fe3"},"source":["t1 = torch.tensor([1, 5, 9, 13, 21, 45, 67, 34])\n","\n","# Specify [from: to : step)\n","# by default step is 1\n","# here from is inclusive but to is not\n","\n","# Get a subarray [9, 13, 21, 45]\n","# index 2 (i.e 9) is inclusive but index 6 (i.e. 67) is not and step size is 1\n","print(t1[2:6])\n","\n","# Get a subarray [9, 21]\n","print(t1[2:6:2])  # step size is 2"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 9, 13, 21, 45])\n","tensor([ 9, 21])\n"]}]},{"cell_type":"code","metadata":{"id":"knosW3KhFZA0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584144134,"user_tz":300,"elapsed":2,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"e8d040dd-8d78-4aa9-c231-1c67705ac2a6"},"source":["# subarrays created using slicing and indexing do not create a copy,\n","# modifying the subarray modifies the original tensor as well\n","\n","t1 = torch.tensor([1, 5, 9, 13, 21, 45, 67, 34])\n","t2 = t1[2:6]\n","print(\"array and subarray before modifying subarray\")\n","print(t1)\n","print(t2)\n","\n","# modify subarray\n","\n","t2[0] = 100\n","print(\"\\narray and subarray after modifying subarray\")\n","print(t1)\n","print(t2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["array and subarray before modifying subarray\n","tensor([ 1,  5,  9, 13, 21, 45, 67, 34])\n","tensor([ 9, 13, 21, 45])\n","\n","array and subarray after modifying subarray\n","tensor([  1,   5, 100,  13,  21,  45,  67,  34])\n","tensor([100,  13,  21,  45])\n"]}]},{"cell_type":"markdown","metadata":{"id":"o6iUsJ8uGhWG"},"source":["<font color = 'indianred'> **As we can see above, modifying subarray, modifes the original array as well**"]},{"cell_type":"code","metadata":{"id":"APgdHOel8Jmi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584147622,"user_tz":300,"elapsed":1274,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"99ac2530-8210-4dfe-c5fd-a62e239fd983"},"source":["# use clone() method if yu specifically want to create a copy\n","\n","t1 = torch.tensor([1, 5, 9, 13, 21, 45, 67, 34])\n","t2 = t1[2:6].clone()\n","print(\"array and subarray before modifying subarray\")\n","print(t1)\n","print(t2)\n","\n","# modify subarray\n","\n","t2[0] = 100\n","print(\"\\narray and subarray after modifying subarray\")\n","print(t1)\n","print(t2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["array and subarray before modifying subarray\n","tensor([ 1,  5,  9, 13, 21, 45, 67, 34])\n","tensor([ 9, 13, 21, 45])\n","\n","array and subarray after modifying subarray\n","tensor([ 1,  5,  9, 13, 21, 45, 67, 34])\n","tensor([100,  13,  21,  45])\n"]}]},{"cell_type":"markdown","source":["<font color = 'indianred'> **SInce we created  a copy, modifying subarray, does not modify the original array as well**"],"metadata":{"id":"1pE-pEt88b5R"}},{"cell_type":"code","source":["t1 = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\n","t1"],"metadata":{"id":"qV1DADJLGKcJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584149094,"user_tz":300,"elapsed":3,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"0725d88d-c2a6-40cc-a6b9-422452a2d08e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1,  2,  3,  4],\n","        [ 5,  6,  7,  8],\n","        [ 9, 10, 11, 12],\n","        [13, 14, 15, 16]])"]},"metadata":{},"execution_count":358}]},{"cell_type":"code","source":["# get the sub array [[6,7], [10,11]]\n","t1[1:3, 1:3]"],"metadata":{"id":"0R7kNTmtGmIj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584151963,"user_tz":300,"elapsed":1823,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"928a9c56-a98c-46a0-9afd-ce34ccd4427e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 6,  7],\n","        [10, 11]])"]},"metadata":{},"execution_count":359}]},{"cell_type":"markdown","metadata":{"id":"nhLHAqPLix2O"},"source":["## <font color = 'pickle'>**Operations on tensors of same size**\n","We can call element-wise operations on any two tensors of the same shape."]},{"cell_type":"code","metadata":{"id":"fXtbVUp8IAUZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584151963,"user_tz":300,"elapsed":7,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"6c99d0c7-3750-45f3-ec48-dadc6f7fff92"},"source":["x = torch.tensor([1.0, 2, 4, 8])\n","y = torch.tensor([2, 2, 2, 2])\n","x + y, x - y, x * y, x / y, x**y  # The ** operator is exponentiation"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([ 3.,  4.,  6., 10.]),\n"," tensor([-1.,  0.,  2.,  6.]),\n"," tensor([ 2.,  4.,  8., 16.]),\n"," tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n"," tensor([ 1.,  4., 16., 64.]))"]},"metadata":{},"execution_count":360}]},{"cell_type":"markdown","metadata":{"id":"NsU8FU-9s3kl"},"source":["## <font color = 'pickle'>**Changing the shape of tensors**"]},{"cell_type":"markdown","metadata":{"id":"4Dw-lJ1Lu3fT"},"source":["### <font color = 'pickle'>**1) Reshape**\n","\n","If we want to change the shape of our tensor, without affecting the elements present, we can use the ***reshape*** function."]},{"cell_type":"code","metadata":{"id":"SlxcKezfxMVK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584153821,"user_tz":300,"elapsed":2,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"9dae4767-5dd6-480e-a3e1-3dbd88428bcd"},"source":["# Initializing a tensor with 10 elements from 0 to 9\n","t = torch.arange(10)\n","print(t)\n","\n","# Changing the shape of tensor t from 1x10 to 2x5\n","tr = t.reshape(2, 5)\n","print(tr)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","tensor([[0, 1, 2, 3, 4],\n","        [5, 6, 7, 8, 9]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"00BapNUKxlMQ"},"source":["If we have to specify just 1 dimension in reshape function and want the function to calculate the second dimension itself, we can write `-1` in place of second dimension.\n","\n","For 2 rows, we will write `reshape(2,-1)`\n","\n","For 5 columns, we will write `reshape(-1,5)`."]},{"cell_type":"code","metadata":{"id":"9CYpAZaAxkxc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584157164,"user_tz":300,"elapsed":851,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"05702284-f865-4a99-b642-89db38c57494"},"source":["# Changing the shape of tensor t from 1 row to 2 rows\n","tr1 = t.reshape(2, -1)\n","print(tr1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 1, 2, 3, 4],\n","        [5, 6, 7, 8, 9]])\n"]}]},{"cell_type":"code","metadata":{"id":"D6qaekDtPp9u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584157164,"user_tz":300,"elapsed":4,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"18c3a271-03bd-4928-d831-7db08c2e8090"},"source":["# Changing the shape of tensor t from 10 columns to 5 columns\n","tr2 = t.reshape(-1, 5)\n","print(tr2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 1, 2, 3, 4],\n","        [5, 6, 7, 8, 9]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"AWCEvFyfu7Xl"},"source":["### <font color = 'pickle'>**2) View**\n","\n","We can allow a tensor to be a view of an existing tensor. It performs the same operation as reshape. The only difference is that View will not create a copy and will allow us to perform fast and memory efficient computations whereas reshape may or may not share the same memory. There's a good discussion of the differences [here](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch).\n","\n","Line from above link \" *Another difference is that reshape() can operate on both contiguous and non-contiguous tensor while view() can only operate on contiguous tensor* \"\n","\n","[Definition of contiguous](https://stackoverflow.com/questions/26998223/what-is-the-difference-between-contiguous-and-non-contiguous-arrays/26999092#26999092)\n"]},{"cell_type":"code","metadata":{"id":"WVrQF7xw25-f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584157727,"user_tz":300,"elapsed":6,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"e2461790-f01c-425a-9cb0-4107fd39d1ca"},"source":["# Initializing a tensor with 10 elements from 0 to 9\n","t = torch.arange(10)\n","print(t,'shape:', t.shape, sep='\\n', end = '\\n\\n')\n","# Changing the shape of tensor t from 1x10 to 2x5\n","t = t.view(2, 5)\n","print(t,'shape:', t.shape, sep='\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","shape:\n","torch.Size([10])\n","\n","tensor([[0, 1, 2, 3, 4],\n","        [5, 6, 7, 8, 9]])\n","shape:\n","torch.Size([2, 5])\n"]}]},{"cell_type":"markdown","metadata":{"id":"WOZJuWP84T7N"},"source":["Views can reflect changes from the base tensor."]},{"cell_type":"code","metadata":{"id":"Yfl2ob_w3X88","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584162495,"user_tz":300,"elapsed":100,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"2dd32d3c-ea4d-4b35-9594-15ed1884233c"},"source":["t = torch.arange(10)\n","\n","# Create a view of tensor t\n","tr = t.view(2, 5)\n","\n","# Before change in base tensor\n","print(f\"before changing the base tensor\\n{tr}\")\n","\n","# Modifying element of base tensor\n","t[0] = 67\n","\n","# After change in base tensor\n","print(f\"\\nafter changing the base tensor\\n {tr}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["before changing the base tensor\n","tensor([[0, 1, 2, 3, 4],\n","        [5, 6, 7, 8, 9]])\n","\n","after changing the base tensor\n"," tensor([[67,  1,  2,  3,  4],\n","        [ 5,  6,  7,  8,  9]])\n"]}]},{"cell_type":"code","metadata":{"id":"FQUVywTktsOf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584163206,"user_tz":300,"elapsed":23,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"deb04a26-93a0-4585-e842-f7a1d4185b7b"},"source":["# we can use -1 with view as well.\n","t = torch.rand((4, 5))\n","t1 = t.view(2, -1)\n","print(t1.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 10])\n"]}]},{"cell_type":"code","metadata":{"id":"FvbN4qgNQFEs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584163206,"user_tz":300,"elapsed":22,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"e2e1dac1-bfbb-46d7-dcc6-44e3daa64e2b"},"source":["# we can also flatten the tensor (convert the tensor to one dimensional tensor) by using view(-1)\n","# this gives the same result as method flatten()\n","t = torch.rand((4, 5, 3))\n","t2 = t.view(-1)\n","t3 = t.flatten()\n","print(t2.shape)\n","print(t3.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([60])\n","torch.Size([60])\n"]}]},{"cell_type":"markdown","metadata":{"id":"UCacRHTqv-SN"},"source":["### <font color = 'pickle'> **3) Adding and removing dimensions of size 1**\n","- Insert a dimension of size 1 at a specific location (location specified by dim) using `torch.unsqueeze(dim)`\n","- Remove a dimension of size 1 at a specific location (location specified by dim) using `torch.squeeze(dim)`\n","- Remove all dimensions of size 1 using `torch.squeeze()`\n","- Insert dimenion of size 1 using `None `keyword\n","- Remove dimenion of size 1 using `0 `keyword"]},{"cell_type":"code","metadata":{"id":"bjzYf6InwH6_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584163206,"user_tz":300,"elapsed":18,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"dac80a82-05b1-4c1f-cf9d-102e0f453c38"},"source":["# Initialize an tensor\n","t1 = torch.ones(2, 2)\n","print(t1)\n","t1.shape"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1.],\n","        [1., 1.]])\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 2])"]},"metadata":{},"execution_count":368}]},{"cell_type":"code","metadata":{"id":"X0NYoZm0wqRF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584163206,"user_tz":300,"elapsed":15,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"116f67e9-8195-4c4e-e802-9554a203ccca"},"source":["# add dimension of size 1 in the beginning using unsqueeze method and argument dim = 0\n","t1 = t1.unsqueeze(dim=0)\n","print(t1)\n","t1.shape"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[1., 1.],\n","         [1., 1.]]])\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 2, 2])"]},"metadata":{},"execution_count":369}]},{"cell_type":"code","metadata":{"id":"w2F1MRj3xBYX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584163206,"user_tz":300,"elapsed":12,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"ffee20ce-213b-4932-c5c2-84af2a334423"},"source":["# add dimesnion of size 1 at the end usin unsqueeze method and dim = 3\n","t1 = t1.unsqueeze(dim=3)\n","print(t1)\n","print(t1.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[[1.],\n","          [1.]],\n","\n","         [[1.],\n","          [1.]]]])\n","torch.Size([1, 2, 2, 1])\n"]}]},{"cell_type":"code","metadata":{"id":"o_9kJKcjxffh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584163207,"user_tz":300,"elapsed":11,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"f5bb38fd-c01b-4ccd-d54b-37d7f0aec653"},"source":["# We can add new dimesnion at any place\n","t1 = torch.arange(20).view(2, 2, 5)\n","print(t1.shape)\n","t1 = t1.unsqueeze(dim=1)\n","print(t1.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 2, 5])\n","torch.Size([2, 1, 2, 5])\n"]}]},{"cell_type":"code","metadata":{"id":"clT8xFNkx_GL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584163207,"user_tz":300,"elapsed":9,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"3dc84f92-d6f6-4be4-9a67-d55d16582711"},"source":["# we can also use None keyword to add dimension of size 1 at multiple locations\n","t1 = t1[:, :, :, None, :, None]\n","print(t1.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 1, 2, 1, 5, 1])\n"]}]},{"cell_type":"code","metadata":{"id":"fVJMCxpPYgxP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584163207,"user_tz":300,"elapsed":6,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"30609dcb-1565-42b5-9cd7-b439ed9c3961"},"source":["# Remove a dimension of size 1 at a specific location using torch.squeeze(dim)\n","t1 = t1.squeeze(dim=1)\n","print(t1.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 2, 1, 5, 1])\n"]}]},{"cell_type":"code","metadata":{"id":"NedAKkzJYnhU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584165804,"user_tz":300,"elapsed":103,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"500cf2f4-48ed-4e3a-b27e-0f7de68c4bc3"},"source":["# Remove a dimension of size 1 at a specific location using 0 keyword\n","t1 = t1[:, :, 0]\n","print(t1.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 2, 5, 1])\n"]}]},{"cell_type":"code","metadata":{"id":"r7fHrZ-TY2qN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584166632,"user_tz":300,"elapsed":2,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"e1f7a4cb-001a-4629-f8e2-9ad7b533eb1c"},"source":["# Removing all dimensions of size 1 using torch.squeeze()\n","t1 = t1.squeeze()\n","print(t1.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 2, 5])\n"]}]},{"cell_type":"markdown","metadata":{"id":"4SewxjuW7O6Y"},"source":["### <font color = 'pickle'>**4) Adopting shape of other tensors**\n","We can use view_as(input) to adopt shape of other tensors"]},{"cell_type":"code","metadata":{"id":"rkxKGSH57eBt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584170135,"user_tz":300,"elapsed":108,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"3ab0a879-be25-404a-839f-98e9c335724a"},"source":["a = torch.arange(10).view(2, 5)\n","# create a tensor b filled with ones (10 elements) and has same shape as b\n","b = torch.ones(10).view_as(a)\n","print(a.shape)\n","print(b.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 5])\n","torch.Size([2, 5])\n"]}]},{"cell_type":"markdown","metadata":{"id":"tMtrejzioBMJ"},"source":["### <font color = 'pickle'>**5) Permute**\n","\n","Permute function rearranges the original tensor according to the desired ordering and returns a new multidimensional rotated tensor.\n","\n","Let us consider an example:\n","\n","If the size of a tensor is (2, 3, 4),\n","\n","- First size is 2\n","- Second size is 3\n","- Third size is 4\n","\n","Now, in case of permute we will just change the ordering of the sizes. Thus if we write permute(0, 2, 1) the new tensor will have:\n","\n","- First size is 2 (1st size of previous)\n","- Second size is 4 (3rd size of previous)\n","- Third size is 3 (2nd size of previous)\n","\n","Pytorch's function permute() only permutes or in other words shuffles the order of the axes of a tensor whereas view() reshapes the tensor by reducing/expanding the size of each dimension.\n"]},{"cell_type":"code","metadata":{"id":"1r4bB2APt7oR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584170382,"user_tz":300,"elapsed":9,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"9fcdbc52-d36e-4404-d538-91a6ce8358c1"},"source":["# Initilaize a tensor and print it's size and elements\n","torch.manual_seed(0)\n","t1 = torch.randint(0, 10, size=(2, 4))\n","print(t1.size())\n","print(t1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 4])\n","tensor([[4, 9, 3, 0],\n","        [3, 9, 7, 3]])\n"]}]},{"cell_type":"code","source":["t1.storage()"],"metadata":{"id":"33olxUBS7dcW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584170383,"user_tz":300,"elapsed":8,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"44d1ac97-f8f6-4f99-e6fc-f75c56aa781b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":[" 4\n"," 9\n"," 3\n"," 0\n"," 3\n"," 9\n"," 7\n"," 3\n","[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 8]"]},"metadata":{},"execution_count":378}]},{"cell_type":"code","source":["t1.is_contiguous()"],"metadata":{"id":"2FbSfoYX7qw0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584170383,"user_tz":300,"elapsed":5,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"c6784e0f-8de9-4369-d27f-510de3047c49"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":379}]},{"cell_type":"code","source":["t1.stride()"],"metadata":{"id":"hW-TQkXr7yKL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584170817,"user_tz":300,"elapsed":5,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"b218d129-a5e9-4af8-b69e-200543afee63"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4, 1)"]},"metadata":{},"execution_count":380}]},{"cell_type":"code","metadata":{"id":"OC0qXNZNVz3_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584170817,"user_tz":300,"elapsed":4,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"f739f95d-3195-4a63-b948-f602fb20f4d1"},"source":["# Permute the tensor and print it's size and elements\n","t1_p = t1.permute(1, 0)\n","print(t1_p.size())\n","print()\n","print(t1_p)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 2])\n","\n","tensor([[4, 3],\n","        [9, 9],\n","        [3, 7],\n","        [0, 3]])\n"]}]},{"cell_type":"code","source":["t1_p.storage()"],"metadata":{"id":"JmcbvT6t8E3h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584172857,"user_tz":300,"elapsed":479,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"f6d1bddf-e17f-45da-9655-1bb37b1a2b4e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":[" 4\n"," 9\n"," 3\n"," 0\n"," 3\n"," 9\n"," 7\n"," 3\n","[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 8]"]},"metadata":{},"execution_count":382}]},{"cell_type":"code","source":["t1_p.stride()"],"metadata":{"id":"aoDcYejx8VkU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584174682,"user_tz":300,"elapsed":519,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"1d32c856-eb8a-4ae0-9172-183a947bb7ac"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 4)"]},"metadata":{},"execution_count":383}]},{"cell_type":"code","source":["t1_p.is_contiguous()"],"metadata":{"id":"QWm1K3dq8YFE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584175444,"user_tz":300,"elapsed":3,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"b6878b5b-2069-45b2-cdac-2009e49d4e57"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":384}]},{"cell_type":"code","source":["t1_p.view(2, 4)"],"metadata":{"id":"D4xvyGKg8t0w","colab":{"base_uri":"https://localhost:8080/","height":194},"executionInfo":{"status":"error","timestamp":1692584177914,"user_tz":300,"elapsed":10,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"f5a3c8a0-5683-429d-d839-d82c7b73b925"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-385-9e0e74491421>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt1_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."]}]},{"cell_type":"code","source":["t1_p.reshape(2, 4)"],"metadata":{"id":"EZFobTRY86RF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584177914,"user_tz":300,"elapsed":8,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"3031cab2-dd16-4711-884a-1ce15a00b971"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[4, 3, 9, 9],\n","        [3, 7, 0, 3]])"]},"metadata":{},"execution_count":386}]},{"cell_type":"code","metadata":{"id":"YBaHNUYro71U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584179150,"user_tz":300,"elapsed":146,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"06506b6f-b34d-448a-c2b2-5f584d48ff17"},"source":["# Initilaize a tensor and print it's size and elements\n","torch.manual_seed(0)\n","t2 = torch.rand(2, 3, 4)\n","print(t2.size())\n","print(f\"\\n{t2}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 3, 4])\n","\n","tensor([[[0.4963, 0.7682, 0.0885, 0.1320],\n","         [0.3074, 0.6341, 0.4901, 0.8964],\n","         [0.4556, 0.6323, 0.3489, 0.4017]],\n","\n","        [[0.0223, 0.1689, 0.2939, 0.5185],\n","         [0.6977, 0.8000, 0.1610, 0.2823],\n","         [0.6816, 0.9152, 0.3971, 0.8742]]])\n"]}]},{"cell_type":"code","metadata":{"id":"wCufxBysSUta","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584181583,"user_tz":300,"elapsed":887,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"223b80e5-d1b4-41d2-93b4-892a6c158722"},"source":["# Permute the tensor and print it's size and elements - use permute (0, 2, 1)\n","t2_p = t2.permute(0, 2, 1)\n","print(t2_p.size())\n","print(f\"\\n{t2_p}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 4, 3])\n","\n","tensor([[[0.4963, 0.3074, 0.4556],\n","         [0.7682, 0.6341, 0.6323],\n","         [0.0885, 0.4901, 0.3489],\n","         [0.1320, 0.8964, 0.4017]],\n","\n","        [[0.0223, 0.6977, 0.6816],\n","         [0.1689, 0.8000, 0.9152],\n","         [0.2939, 0.1610, 0.3971],\n","         [0.5185, 0.2823, 0.8742]]])\n"]}]},{"cell_type":"code","metadata":{"id":"KDbnALFR2ut_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584181583,"user_tz":300,"elapsed":6,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"7f3cee94-02d9-4acd-a0ca-1c37d6954291"},"source":["# difference between permute and view\n","x = torch.arange(3 * 2).view(2, 3)\n","print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 1, 2],\n","        [3, 4, 5]])\n"]}]},{"cell_type":"code","source":["# create a view (3, 2)\n","print(x.view(3, 2))"],"metadata":{"id":"pr6fydJ1uuFo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584181583,"user_tz":300,"elapsed":4,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"7b6bd594-a293-418d-a502-e8c73914f1e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 1],\n","        [2, 3],\n","        [4, 5]])\n"]}]},{"cell_type":"code","source":["# permute axis(1, 0)\n","print(x.permute(1, 0))"],"metadata":{"id":"pK7vUWrQuyoy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584182725,"user_tz":300,"elapsed":5,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"2f109d0a-a2b5-4694-c34c-45ecf37d1f08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 3],\n","        [1, 4],\n","        [2, 5]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"f-HexBDq4PIi"},"source":["Question and answer taken from following reference: <br>\n","https://discuss.pytorch.org/t/different-between-permute-transpose-view-which-should-i-use/32916\n","\n","- (1) If I have a feature size of BxCxHxW, I want to reshape it to BxCxHW. Which one is a good option?\n","- (2) If I have a feature size of BxCxHxW, I want to change it to BxCxWxH . Which one is a good option?\n","- (3) If I have a feature size of BxCxH, I want to change it to BxCxHx1 . Which one is a good option?\n","\n","Solution:\n","- permute changes the order of dimensions aka axes, so 2 would be a use case. Transpose is a special case of permute, use it with 2d tensors.\n","- view can combine and split axes, so 1 and 3 can use view,\n","- note that view can fail for noncontiguous layouts (e.g. crop a picture using indexing), in these cases reshape will do the right thing,\n","- for adding dimensions of size 1 (case 3), there also are unsqueeze and indexing with None.\n"]},{"cell_type":"code","source":[],"metadata":{"id":"Nq3DaJK0F76y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wMkz0iL56Va7"},"source":["## <font color = 'pickle'>**Concatenating Tensors**\n","\n","We can use `torch.cat((tensors_to_concatenate), dim)` to concatenate tensors.\n","\n","The tensors must have the same shape (except in the concatenating dimension)."]},{"cell_type":"code","metadata":{"id":"TwWw4dEphsw_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584190272,"user_tz":300,"elapsed":2561,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"c2e302ac-af21-4d6b-cb2b-d9134aeee282"},"source":["# we can use torch\n","x1 = torch.randint(low=0, high=10, size=(2, 5))\n","x2 = torch.ones(4, 5)\n","x3 = torch.zeros(2, 3)\n","\n","# The tensors must have the same shape (except in the concatenating dimension)\n","# x1 and x2 have the same shape except for dim = 0, hence we can conactenate these along dim = 0\n","# x1 and x3 have the same shape except for dim = 1, hence we can conactenate these along dim = 1\n","# we cannot concatenate x2 and x3 along any dimension\n","\n","x1_x2 = torch.cat((x1, x2), dim=0)\n","x1_x3 = torch.cat((x1, x3), dim=1)\n","print(f\"shape of x1_x2 is {x1_x2.shape}\")\n","print(f\"shape of x2_x3 is {x1_x3.shape}\")\n","print(f\"\\nx1_x2\\n:{x1_x2}\")\n","print(f\"\\nx1_x3\\n:{x1_x3}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["shape of x1_x2 is torch.Size([6, 5])\n","shape of x2_x3 is torch.Size([2, 8])\n","\n","x1_x2\n",":tensor([[4., 1., 9., 9., 9.],\n","        [0., 1., 2., 3., 0.],\n","        [1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.]])\n","\n","x1_x3\n",":tensor([[4., 1., 9., 9., 9., 0., 0., 0.],\n","        [0., 1., 2., 3., 0., 0., 0., 0.]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"xX3tyHVrpN4s"},"source":["## <font color = 'pickle'>**Some commonly used Tensors**"]},{"cell_type":"markdown","metadata":{"id":"wpm1RohNpzKv"},"source":["###<font color = 'pickle'>**1) Tensor containing all zeros/ all ones/ or any value**"]},{"cell_type":"code","metadata":{"id":"O8FzuQrEoB2T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584190837,"user_tz":300,"elapsed":106,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"6c744c7f-cd88-4089-9720-c037f926498b"},"source":["# Tensor containing all zeros, size = 10\n","z1 = torch.zeros(5)\n","\n","# Tensor containing all zeros, size = 2 X 2 X 3\n","z2 = torch.zeros(2, 2, 3)\n","\n","print(z1)\n","print(z2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0., 0., 0., 0., 0.])\n","tensor([[[0., 0., 0.],\n","         [0., 0., 0.]],\n","\n","        [[0., 0., 0.],\n","         [0., 0., 0.]]])\n"]}]},{"cell_type":"code","metadata":{"id":"JMbGq7ceqba0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584192673,"user_tz":300,"elapsed":1838,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"5b8d9f3c-98a9-48ad-c528-bbfc8656ac8d"},"source":["# Tensor containing all ones, size = 7\n","z1 = torch.ones(7)\n","\n","# Tensor containing all ones, size = 1 X 2 X 3\n","z2 = torch.ones(1, 2, 3)\n","\n","print(z1)\n","print(z2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 1., 1., 1., 1., 1., 1.])\n","tensor([[[1., 1., 1.],\n","         [1., 1., 1.]]])\n"]}]},{"cell_type":"code","metadata":{"id":"EVREaBSAZK7w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584192674,"user_tz":300,"elapsed":11,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"605129ae-39f9-45ca-d049-58c2a47d2ec5"},"source":["# We can also use torch.full(size, fill_value) to create a tensor filled with any value\n","# Tensor containing all fives, size = 1 X 2 X 3\n","\n","z3 = torch.full(size=(2, 2, 3), fill_value=5)\n","print(z3)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[5, 5, 5],\n","         [5, 5, 5]],\n","\n","        [[5, 5, 5],\n","         [5, 5, 5]]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"HxOWMc03qszf"},"source":["### <font color = 'pickle'>**2) Tensor with elements in a particular range**\n","Suppose we need a tensor with values `1, 2, 3, 4.....n. `\n","\n","We can simply specify the range and tensor will automatically get filled with these values."]},{"cell_type":"code","metadata":{"id":"Ydycx6TMqowD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584192674,"user_tz":300,"elapsed":7,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"06cfffe1-ccad-48a2-ba13-b93a81d4cd5e"},"source":["# Creating a tensor with integers from 1 to 5 with space 1: [1, 2, 3, 4, 5]\n","# syntax arange(start, end, step) - create tensor with values in the interval [start, end).\n","# start is inclusive , end is not i.e. start <= values < end\n","tr1 = torch.arange(1, 6)\n","print(tr1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1, 2, 3, 4, 5])\n"]}]},{"cell_type":"code","metadata":{"id":"I9n03rD_sFdS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584192674,"user_tz":300,"elapsed":5,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"070687fa-d273-41bb-a687-62e262b8db95"},"source":["# Creating a tensor with integers from 0 to 10 with space 2 using \"step\" parameter: [0, 2, 4, 6, 8, 10]\n","tr2 = torch.arange(0, 11, step=2)\n","print(tr2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 0,  2,  4,  6,  8, 10])\n"]}]},{"cell_type":"markdown","metadata":{"id":"T4TOw7gUZ-He"},"source":["We can also use `torch.linspace()` to generate evenly spaced values between two numbers"]},{"cell_type":"code","metadata":{"id":"-9iboyquHrmW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584196812,"user_tz":300,"elapsed":11,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"729bf108-a5c5-4169-b29d-826d485a38cc"},"source":["# Generate 10 evenly spaced values between 0 and 1 (both inclusive)\n","t1 = torch.linspace(0, 1, 10)\n","print(t1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.0000, 0.1111, 0.2222, 0.3333, 0.4444, 0.5556, 0.6667, 0.7778, 0.8889,\n","        1.0000])\n"]}]},{"cell_type":"markdown","metadata":{"id":"sVqmTY7IzBmT"},"source":["###<font color = 'pickle'>**3) Tensor with elements from probability distribution**\n","\n","We can use the randn function to get elements from standard normal probabilty distribution i.e. normal dustribution with mean = 0 and variance = 1. If we want to select elements from normal ditsribution with different mean and variance then we should use torch.normal"]},{"cell_type":"code","metadata":{"id":"N9dnwRY1zAoO"},"source":["# for reproducabilty so that we get same results everytime we run this cell\n","torch.manual_seed(42)\n","\n","# Sample 500,000 values from standard normal distribution (mean = 0 , variance = 1)\n","t1 = torch.randn(500000)\n","\n","# Sample 500,000 values from normal distribution (mean = 5 , std = 2)\n","t2 = torch.normal(mean=5, std=2, size=(500000,))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nty188mkR6Yx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584198756,"user_tz":300,"elapsed":1951,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"a16ee43f-ea4a-468d-d631-6b02aa99bfa7"},"source":["print(\"Mean and std of tensor using torch.randn\")\n","print(torch.mean(t1))\n","print(torch.std(t1))\n","\n","print(\"\\nMean and std of tensor using torch.normal\")\n","print(torch.mean(t2))\n","print(torch.std(t2))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean and std of tensor using torch.randn\n","tensor(-0.0013)\n","tensor(1.0014)\n","\n","Mean and std of tensor using torch.normal\n","tensor(5.0002)\n","tensor(1.9971)\n"]}]},{"cell_type":"code","metadata":{"id":"iyiCCPVZVg8H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584198757,"user_tz":300,"elapsed":1947,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"7eff9111-7301-4d2e-e05b-71de0687d809"},"source":["# for reproducabilty so that we get same results everytime we run this cell\n","torch.manual_seed(0)\n","\n","# we sampled 10 values from standard noemal distribution. (5, 2) is the shape.\n","t1 = torch.randn(5, 2)\n","t1"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1.5410, -0.2934],\n","        [-2.1788,  0.5684],\n","        [-1.0845, -1.3986],\n","        [ 0.4033,  0.8380],\n","        [-0.7193, -0.4033]])"]},"metadata":{},"execution_count":401}]},{"cell_type":"markdown","metadata":{"id":"SxiimV9UY7zu"},"source":["We can also sample from other distributions like torch.rand, torch.randint etc."]},{"cell_type":"markdown","metadata":{"id":"OIeaFS6SV5lS"},"source":["### <font color = 'pickle'>**4) Empty Tensor**\n","We can create uninitialized  tensors using torch.empty.\n"]},{"cell_type":"code","metadata":{"id":"n7Z40rBVY3oA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584199268,"user_tz":300,"elapsed":3,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"d15360fb-317d-423c-b2cd-ff1d69d8e595"},"source":["# create empty tensor of shape (2, 4)\n","empty_tensor = torch.empty(2, 4)\n","empty_tensor"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.1210e-43, 0.0000e+00, 1.5695e-43, 0.0000e+00],\n","        [2.5290e-30, 3.2664e-41, 0.0000e+00, 0.0000e+00]])"]},"metadata":{},"execution_count":402}]},{"cell_type":"markdown","metadata":{"id":"UiNZ0HioJdXF"},"source":["### <font color = 'pickle'>**5) Commonly used tensors based on shape of other tensors**"]},{"cell_type":"markdown","metadata":{"id":"iJEmlIsUIuYu"},"source":["We can also use `torch.zeros_like(input)`, `torch.ones_like(input)`, `torch.full_like(input)` and `torch.empty_like(input)` to create tensors based on the shape of other tensors\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"f4J69YwmJz75","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584200518,"user_tz":300,"elapsed":7,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"14adf9d5-f557-4c5d-85eb-eec3b7eee154"},"source":["input_tensor = torch.arange(6).view(2, 3)\n","input_tensor.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 3])"]},"metadata":{},"execution_count":403}]},{"cell_type":"code","metadata":{"id":"avYjEMStKF5g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584200519,"user_tz":300,"elapsed":7,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"6c3b46cf-b9ee-4136-c88a-a8adcaf0c5a3"},"source":["print(torch.ones_like(input_tensor))\n","print(torch.zeros_like(input_tensor))\n","print(torch.full_like(input_tensor, 5))\n","print(torch.empty_like(input_tensor))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 1, 1],\n","        [1, 1, 1]])\n","tensor([[0, 0, 0],\n","        [0, 0, 0]])\n","tensor([[5, 5, 5],\n","        [5, 5, 5]])\n","tensor([[132871877815712, 132871877815712,               1],\n","        [100106002631040,               0,               0]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"tsMg-Jujsm2W"},"source":["###<font color = 'pickle'> **6) Identity Matrix**\n","\n","Identity matrix is a matrix which has 1's along the diagnal and zeros everywhere else."]},{"cell_type":"code","metadata":{"id":"mM4w1jmDsOwR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584201137,"user_tz":300,"elapsed":6,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"fcbbd9d8-2d12-42de-960f-bac9e1beb8a0"},"source":["# Identity matrix of size 3\n","id_matrix = torch.eye(3)\n","print(id_matrix)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.]])\n"]}]},{"cell_type":"code","metadata":{"id":"6PqCSmvwtBp4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584201137,"user_tz":300,"elapsed":5,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"65a4cd62-55d7-4285-c18c-8515f764f31f"},"source":["# Identity matrix of size 5\n","id_matrix = torch.eye(5)\n","print(id_matrix)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0.],\n","        [0., 0., 1., 0., 0.],\n","        [0., 0., 0., 1., 0.],\n","        [0., 0., 0., 0., 1.]])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"CiBC1vbQDdMX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"24OjlU8RnSie"},"source":["## <font color = 'pickle'>**Masks using binary tensors**"]},{"cell_type":"code","metadata":{"id":"CxyRZVxGPgt_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584204708,"user_tz":300,"elapsed":6,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"bbb595ec-5cd2-4241-d56a-015fe90fd3af"},"source":["# create a tensor which has probailities of events\n","prob = torch.tensor([0.7, 0.4, 0.6, 0.2, 0.8, 0.1])\n","\n","# Binary tensors\n","print(prob > 0.5)\n","print(prob <= 0.5)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ True, False,  True, False,  True, False])\n","tensor([False,  True, False,  True, False,  True])\n"]}]},{"cell_type":"code","metadata":{"id":"ZwpKs1DRQd7J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692584204708,"user_tz":300,"elapsed":4,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"fa8170e4-0cce-4225-9f84-0acc9b6c248e"},"source":["# create output tensor where output = 1 if prob >0.5 and 0 otherwise\n","# craete an empty output tensor of same shape as prob\n","output = torch.empty_like(prob)\n","\n","# update output tensor using the binary mask\n","output[prob > 0.5] = 1\n","output[prob <= 0.5] = 0\n","print(output)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 0., 1., 0., 1., 0.])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"FxTCjB994VN6"},"execution_count":null,"outputs":[]}]}