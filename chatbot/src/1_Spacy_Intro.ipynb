{"cells":[{"cell_type":"markdown","metadata":{"id":"Czf2260KKAvK"},"source":["#  <font color = 'dodgerblue'> **Objective**\n","The first step in NLP projects is to clean the text. For example we might want to remove punctuations, white spaces etc. Futher we want to break our strings into tokens. This step is required as we want to lean the vector (number) representaion of the tokens that we can use in our models. Spacy is a very useful library which can help us in text cleaning and tokeinzation. In this notebook, you will understand the basics of the spacy library.\n","\n","After completing this notebook, you will be able to\n","- Clean text using spacy\n","- Create tokens using spacy\n","- Extract Part of Speech Tags\n","- Extract Named Entities"]},{"cell_type":"markdown","metadata":{"id":"e3XVtBZC3Z5W"},"source":["#  <font color = 'dodgerblue'>**Install latest version of spaCy**\n","\n","spaCy is a popular library for Natural Language Processing (NLP) in Python. It provides advanced NLP tools and pre-trained models for performing common NLP tasks such as tokenization, lemmatization, POS tagging, entity recognition, and dependency parsing.\n","\n","spaCy's NLP models are designed to be efficient and production-ready, which makes it a popular choice for NLP tasks in real-world applications"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ohxr8GRscIpK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705878744157,"user_tz":360,"elapsed":13077,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"baab834a-df07-4379-87bf-9726001b496d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","en-core-web-sm 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.7.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["# install spacy\n","!pip install -U spacy -qq"]},{"cell_type":"markdown","source":["The code `!pip install -U spacy -qq` is a shell command that uses the pip package manager to install the latest version of the spacy library.\n","\n","- The `!` at the beginning of the line indicates that this is a shell command, rather than a Python command.\n","- The `pip install` command is used to install a package or library,\n","- `-U` is an option that tells pip to upgrade to the latest version of the package if it is already installed.\n","\n","The `-qq` option at the end of the command is used to control the verbosity of the output.\n","- The `-q` option is for quiet output, and the double `-qq` option is for even quieter output. With this option, pip will produce minimal output, only printing error messages, if any"],"metadata":{"id":"HLGAeZ7h_3Lq"}},{"cell_type":"markdown","metadata":{"id":"vgZHQ4OwB3S4"},"source":["#  <font color = 'dodgerblue'>**Import Libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mzyN8ym07DCt"},"outputs":[],"source":["import spacy\n","import pandas as pd"]},{"cell_type":"markdown","source":["UserWarning: Can't initialize NVML (Nvidia Management Library).\n","\n","NVML provides a uniform way to monitor and manage various NVIDIA GPU-related activities, such as monitoring GPU utilization, temperature, power, and other performance-related metrics. If the library is unable to initialize NVML, it means that it cannot access the NVIDIA GPU and perform the necessary operations.\n","\n","We are not using GPUs , hence we do not need to worry about the warning."],"metadata":{"id":"18wEdQRp6_oi"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1705878754292,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"lDgJ4PeEU-EE","outputId":"74e5daa0-13a4-4978-9ffb-077f15028d46"},"outputs":[{"output_type":"stream","name":"stdout","text":["3.7.2\n"]}],"source":["# check spaCy Verion\n","print(spacy.__version__)"]},{"cell_type":"markdown","metadata":{"id":"i6o-N-d5B-d_"},"source":["#  <font color = 'dodgerblue'>**Sample Strings**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RtF6PxC6i7Kw"},"outputs":[],"source":["# Sample String - Create a sample String\n","\n","text1 = \"\"\"China's capital is Beijing. \\n\\nBeijing is where we'll go. \\n\\nLet's travel to Hong Kong from Beijing. \\\n","          \\n\\nA friend is pursuing his M.S from Beijing. \\n\\nBeijing is a cool place!!! :-P <3 #Awesome \\\n","          \\n\\nA Rolex watch costs in the range of $3000.0 - $8000.0 in USA and China. \\n\\n@tompeter I'm \\\n","          \\n\\ngoing to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3 \\\n","          for more info see: http://www.example_beijing.com! Ten is different from 10\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hL3VOCmBZ8fe"},"outputs":[],"source":["text2  = \"\"\"China's capital is Beijing. \\n\\nA Rolex watch costs in the range of $3000.0 - $8000.0 in USA\"\"\""]},{"cell_type":"markdown","metadata":{"id":"z3cSytbc3z7p"},"source":["#  <font color = 'dodgerblue'>**White Space Tokenizers**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1705878754292,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"zI-hr_Lti7K6","outputId":"14886228-bb97-4f14-9738-ffff652c6fe0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"China's\",\n"," 'capital',\n"," 'is',\n"," 'Beijing.',\n"," 'A',\n"," 'Rolex',\n"," 'watch',\n"," 'costs',\n"," 'in',\n"," 'the',\n"," 'range',\n"," 'of',\n"," '$3000.0',\n"," '-',\n"," '$8000.0',\n"," 'in',\n"," 'USA']"]},"metadata":{},"execution_count":6}],"source":["# Whitespace Tokenizer splits text across whitespaces\n","text2.split()"]},{"cell_type":"markdown","metadata":{"id":"q8tGWjno5f04"},"source":["#  <font color = 'dodgerblue'>**spaCy Basics**\n","\n","**spaCy** (https://spacy.io/) is an open-source Python library that parses and \"understands\" large volumes of text. Separate models are available that cater to specific languages (English, French, German, etc.).\n","\n","Models in spaCy for English Language as of release 3.0.0\n","- **en_core_web_sm:** 11MB\n","- **en_core_web_md:** 48MB\n","- **en_core_web_lg:** 746MB\n","<br><br>\n","![picture](https://spacy.io/images/pipeline.svg)\n","\n","Picture Source :https://spacy.io/usage/processing-pipelines\n","\n","The first step in spaCy is to create an `nlp` object. The `nlp` object is a instance of a model and consists of various operations like tokenizaton, tagger, parser, ner etc (see figure above). When a text is passed through the object, it goes throught these operations. When creating an object ,we can disable the operations that we do not need.\n"]},{"cell_type":"markdown","metadata":{"id":"wwzpMf35CQGz"},"source":[" ## <font color = 'dodgerblue'>**Download Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V7WDFOLUCO0T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705878994244,"user_tz":360,"elapsed":35832,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"4a09701f-4254-4739-810f-6acc8dfee909"},"outputs":[{"output_type":"stream","name":"stdout","text":["2024-01-21 23:16:00.002146: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-01-21 23:16:00.002208: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-01-21 23:16:00.003583: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-01-21 23:16:01.235744: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Collecting en-core-web-lg==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.2)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n","Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.3.4)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.9.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.10.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (23.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.23.5)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2023.11.17)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n","Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.3)\n","Installing collected packages: en-core-web-lg\n","Successfully installed en-core-web-lg-3.7.1\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_lg')\n"]}],"source":["!python -m spacy download en_core_web_lg"]},{"cell_type":"markdown","metadata":{"id":"_rbWF4oTCZW1"},"source":[" ## <font color = 'dodgerblue'>**Load Model**\n","\n","- `spacy.load` is a function that is used to load a pre-trained spaCy model.\n","\n","- The argument passed to the load function, `'en_core_web_sm'`, specifies which model to load. In this case, it's the small English model `en_core_web_sm`.\n","\n","- The result of the `load` function is assigned to the variable `nlp`. This variable now contains an instance of the spaCy NLP pipeline that is loaded with the pre-trained English model.\n","\n","Now, you can use the `nlp` variable to perform NLP tasks such as tokenization, part-of-speech tagging, entity recognition, and more, on text data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePemPus6CblL"},"outputs":[],"source":["# load model\n","nlp = spacy.load('en_core_web_sm')"]},{"cell_type":"markdown","source":["## <font color = 'dodgerblue'>**Check Pipelines**\n","\n","The spaCy NLP pipeline is a series of processing steps that are applied to the input text data. The pipeline components are responsible for tasks such as tokenization, part-of-speech tagging, named entity recognition, etc. Each component in the pipeline is named, and nlp.pipe_names returns a list of these names."],"metadata":{"id":"qnEnkOdDawGw"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":419,"status":"ok","timestamp":1705880723828,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"bUW3_NUDnO5U","outputId":"321ecf2e-1232-4ad1-a33e-e5a43d03b005"},"outputs":[{"output_type":"stream","name":"stdout","text":["['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"]}],"source":["# check pipelines\n","print(nlp.pipe_names)"]},{"cell_type":"markdown","source":["## <font color = 'dodgerblue'>**Disable Components not required**\n","\n","- `nlp.select_pipes` is a method that is used to select specific components of the NLP pipeline to be disabled (turned off) for processing.\n","\n","- The argument passed to the `select_pipes` method, `disable= ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']`, is a list of component names that should be disabled. In this case, the components named `tok2vec`, `tagger`, `parser`, `attribute_ruler`, `lemmatizer`, and `ner` are all being disabled.\n","\n","- The result of the `select_pipes` method is assigned to the variable `disabled`. This variable now contains a list of the component names that were selected for disabling.\n","\n","By disabling specific components of the NLP pipeline, you can control which NLP tasks are performed on your text data and improve processing speed. For example, if you only need to perform tokenization, you can disable all other components to reduce processing time."],"metadata":{"id":"be_X4H-hbakG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pEXllpsOnY49"},"outputs":[],"source":["# disable all the components as we are going to use only tokenizer in this notebook\n","disabled = nlp.select_pipes(disable= ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":67,"status":"ok","timestamp":1705880726732,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"hlOCQxxtnrw4","outputId":"7a2ce685-b222-4a53-9feb-120f5e88eff9"},"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n"]}],"source":["# check the pipeline components\n","print(nlp.pipe_names)"]},{"cell_type":"markdown","metadata":{"id":"L17v9uKVwo91"},"source":["# <font color = 'dodgerblue'>**Tokenization in spaCy**\n","A Text is tokenized in spaCy when creating the Language processing pipeline nlp() object."]},{"cell_type":"markdown","metadata":{"id":"NSBinQlyxpAf"},"source":["![picture](https://spacy.io/images/tokenization.svg)"]},{"cell_type":"markdown","metadata":{"id":"pRRhFhW_2Trr"},"source":["Picture Source: https://spacy.io/usage/linguistic-features#how-tokenizer-works"]},{"cell_type":"markdown","metadata":{"id":"RyWTJcf6Ltg3"},"source":["The algorithm below is taken from Tokenization part from spaCy's Documentation.  \n","https://spacy.io/usage/linguistic-features#tokenization\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zeLLMMNxg2bi"},"source":["1. Iterate over space-separated substrings.\n","2. Check whether we have an explicitly defined special case for this substring. If we do, use it.\n","3. Look for a token match. If there is a match, stop processing and keep this token.\n","4. Check whether we have an explicitly defined special case for this substring. If we do, use it.\n","5. Otherwise, try to consume one prefix. If we consumed a prefix, go back to #3, so that the token match and special cases always get priority.\n","6. If we didn’t consume a prefix, try to consume a suffix and then go back to #3.\n","7. If we can’t consume a prefix or a suffix, look for a URL match.\n","8. If there’s no URL match, then look for a special case.\n","9. Look for “infixes” – stuff like hyphens etc. and split the substring into tokens on all infixes.\n","10. Once we can’t consume any more of the string, handle it as a single token.\n","11. Make a final pass over the text to check for special cases that include spaces or that were missed due to the incremental processing of affixes.\n"]},{"cell_type":"markdown","metadata":{"id":"zcq_x_mMiNPx"},"source":[" ## <font color = 'dodgerblue'>**Create Doc Object**\n","\n","When we call nlp on a string, spaCy first tokenizes the text and creates a `Doc` object.\n","\n","A `Doc` object in spaCy is a processed representation of a text document. It is created when text is processed by the spaCy NLP pipeline, and contains the following attributes.\n","\n","- `text`: The original text of the document.\n","\n","- `ents`: A sequence of Span objects that represent named entities in the document.\n","\n","- `sents`: A sequence of Span objects that represent individual sentences in the document.\n","\n","- `vocab`: A reference to the vocabulary object used by the NLP pipeline.\n","\n","In addition to these attributes, `Doc` objects also have many other properties and methods that allow you to perform various NLP tasks, such as part-of-speech tagging, named entity recognition, and more.\n","\n","The `Doc` object is the starting point for many NLP tasks, and it is often used as the input to various spaCy components, such as the dependency parser, entity recognizer, and more."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W_V6pOrri7LV"},"outputs":[],"source":["# creating a Doc object\n","doc = nlp(text2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1705880729903,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"ZfhUAFNxi7Ld","outputId":"d1d2175b-521b-496c-9d71-79d97da7457e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["China's capital is Beijing. \n","\n","A Rolex watch costs in the range of $3000.0 - $8000.0 in USA"]},"metadata":{},"execution_count":92}],"source":["doc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1705880729903,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"is99sNori7Ll","outputId":"44645da2-dfca-49a3-dced-f58c2520b450"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["spacy.tokens.doc.Doc"]},"metadata":{},"execution_count":93}],"source":["# check the type of doc\n","type(doc)"]},{"cell_type":"markdown","source":["Code explanation:\n","- The NLP pipeline object is passed a string of text as an argument: `nlp(text2)`.\n","\n","- The string of text is processed by each component in the NLP pipeline in the order specified by `nlp.pipe_names`.\n","\n","- The processed text is stored in a variable `doc`. The variable `doc` is a spaCy `Doc` object that contains the processed text and various attributes, such as tokenization, part-of-speech tags, named entities, etc.\n","\n","The result of the NLP processing is stored in the `doc` variable, which can then be used for further processing or analysis. For example, you can iterate over the tokens in the `doc` object and access the attributes of each token, such as its part-of-speech tag, lemma, or entity label."],"metadata":{"id":"4ASWRteQfi-M"}},{"cell_type":"code","source":["print(doc.text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z0Qjbm4gzmUW","executionInfo":{"status":"ok","timestamp":1705880730377,"user_tz":360,"elapsed":64,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"bf7cf07f-d1a5-4182-e784-d8e0dc9c9cda"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["China's capital is Beijing. \n","\n","A Rolex watch costs in the range of $3000.0 - $8000.0 in USA\n"]}]},{"cell_type":"code","source":["doc.ents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"304YOirRzxsO","executionInfo":{"status":"ok","timestamp":1705880730377,"user_tz":360,"elapsed":5,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"9748efe7-163b-4572-e5fe-687b607828fc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["()"]},"metadata":{},"execution_count":95}]},{"cell_type":"code","source":["doc.sents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M6O-4jRw7oEv","executionInfo":{"status":"ok","timestamp":1705880730719,"user_tz":360,"elapsed":7,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"580083d8-0a5b-4076-9e9e-4f3d0ae60d0c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<generator at 0x7bb9a352ccc0>"]},"metadata":{},"execution_count":96}]},{"cell_type":"code","source":["list(doc.sents)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":236},"id":"3T3G5sznZQ4H","executionInfo":{"status":"error","timestamp":1705880730719,"user_tz":360,"elapsed":5,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"80502c2d-654d-4439-828e-b665c4a8e0e4"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-97-533c53c1bfc1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36msents\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."]}]},{"cell_type":"markdown","metadata":{"id":"farHJxJ_Dhel"},"source":["  ## <font color = 'dodgerblue'>**Accessing text of the tokens**\n","token is an object. We can acccess the text of the token using text attribute."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WDDbkA7wi7L9","scrolled":true},"outputs":[],"source":["[token.text for token in doc]"]},{"cell_type":"markdown","metadata":{"id":"vljZl9sfDZfp"},"source":["  ## <font color = 'dodgerblue'>**Compare spacy tokenizer with white space tokenizer**\n","We can create tokenizer using Python split() function. In the last lecture we created tokenizer by splitting on non-alpha numeric characters. That gave us tokens separated by non-alphanumeric caharacters i.e. our tokens only have alpha numeric characters (words, numbers and underscores). We will now craete a white space tokenizer. i.e. it will split the string based on white spaces and create tokens.\n","\n","We wil compare this tokenizer with spacy's tokenizer."]},{"cell_type":"markdown","metadata":{"id":"x2wqquhMFXTD"},"source":["  ## <font color = 'dodgerblue'>**Example 1 (more and better tokens)**\n","\n","The Whitespace Tokenizer splits the words from whitespaces.\n","\n","The spaCy tokenizer splits the text into meaningful segments dependent on the language model that is used.\n","\n","It is apparent that spaCy is a better tokenizer, as it's tokens contain more than just words separated from whitespaces."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1705880732458,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"zwLhE_pBi7MF","outputId":"402a20c4-d3e3-4360-a7ac-d1accfa39d7b"},"outputs":[{"output_type":"stream","name":"stdout","text":["[\"China's\", 'capital', 'is', 'Beijing.', 'Beijing', 'is', 'where', \"we'll\", 'go.', \"Let's\", 'travel', 'to', 'Hong', 'Kong', 'from', 'Beijing.', 'A', 'friend', 'is', 'pursuing', 'his', 'M.S', 'from', 'Beijing.', 'Beijing', 'is', 'a', 'cool', 'place!!!', ':-P', '<3', '#Awesome', 'A', 'Rolex', 'watch', 'costs', 'in', 'the', 'range', 'of', '$3000.0', '-', '$8000.0', 'in', 'USA', 'and', 'China.', '@tompeter', \"I'm\", 'going', 'to', 'buy', 'a', 'Rolexxxxxxxx', 'watch!!!', ':-D', '#happiness', '#rolex', '<3', 'for', 'more', 'info', 'see:', 'http://www.example_beijing.com!', 'Ten', 'is', 'different', 'from', '10']\n","['China', \"'s\", 'capital', 'is', 'Beijing', '.', '\\n\\n', 'Beijing', 'is', 'where', 'we', \"'ll\", 'go', '.', '\\n\\n', 'Let', \"'s\", 'travel', 'to', 'Hong', 'Kong', 'from', 'Beijing', '.', '          \\n\\n', 'A', 'friend', 'is', 'pursuing', 'his', 'M.S', 'from', 'Beijing', '.', '\\n\\n', 'Beijing', 'is', 'a', 'cool', 'place', '!', '!', '!', ':-P', '<3', '#', 'Awesome', '          \\n\\n', 'A', 'Rolex', 'watch', 'costs', 'in', 'the', 'range', 'of', '$', '3000.0', '-', '$', '8000.0', 'in', 'USA', 'and', 'China', '.', '\\n\\n', '@tompeter', 'I', \"'m\", '          \\n\\n', 'going', 'to', 'buy', 'a', 'Rolexxxxxxxx', 'watch', '!', '!', '!', ':-D', '#', 'happiness', '#', 'rolex', '<3', '          ', 'for', 'more', 'info', 'see', ':', 'http://www.example_beijing.com', '!', 'Ten', 'is', 'different', 'from', '10']\n"]}],"source":["doc1 = nlp(text1)\n","# Whitespace Tokenizer\n","print(text1.split())\n","\n","# spaCy Tokenizer\n","print([token.text for token in doc1])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1705880732458,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"i1Wi4Ewfi7MM","outputId":"bb0e4c36-64d6-4b6b-8c41-fbca8e7cd06f"},"outputs":[{"output_type":"stream","name":"stdout","text":["69\n","99\n"]}],"source":["# No. of Tokens in Whitespace Tokenizer\n","print(len(text1.split()))\n","# No. of Tokens in spaCy's Tokenizer\n","print(len([token.text for token in doc1]))"]},{"cell_type":"markdown","metadata":{"id":"t9-WzMA-FoG4"},"source":["  ## <font color = 'dodgerblue'>**Example 2**\n","You can see that spacy recognizes % symbol and create a separate token for it."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1705880733466,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"KvLyXUp7i7MU","outputId":"a4021f07-7756-4eff-f899-826b16cbe3fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["['There', 'is', '20%', 'probaility', 'of', 'winning', 'a', 'lottery.']\n","['There', 'is', '20', '%', 'probaility', 'of', 'winning', 'a', 'lottery', '.']\n"]}],"source":["text3 = \"There is 20% probaility of winning a lottery.\"\n","doc3 = nlp(text3)\n","# Whitespace Tokenizer\n","print(text3.split())\n","\n","# spaCy Tokenizer\n","print([token.text for token in doc3])"]},{"cell_type":"markdown","metadata":{"id":"8wNEEIhPF2wb"},"source":["  ## <font color = 'dodgerblue'>**Example 3**\n","Spacy's tokenizer recognizes that m is the unit of distance (based on sentence and creates a separate token for it. It will not split random combination of numbers and alphabets.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1705880733466,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"jsNGne55i7Mb","outputId":"7c1ee1bd-8407-463a-845d-fb2d24d2e51c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Whitespace Tokenizer\n","['I', 'walk', '10m', 'everyday.']\n","\n","spaCy Tokenizer\n","['I', 'walk', '10', 'm', 'everyday', '.']\n","\n","spaCy Tokenizer\n","[' ', 'What', 'is', '10o8iu']\n"]}],"source":["# Another example measuring difference between Whitespace and spaCy tokenizers\n","text4=\"I walk 10m everyday.\"\n","doc4 = nlp(text4)\n","# Whitespace Tokenizer\n","print('Whitespace Tokenizer', text4.split(), sep = '\\n')\n","\n","# spaCy Tokenizer\n","print('\\nspaCy Tokenizer',[token.text for token in doc4],sep = '\\n')\n","\n","text5 = \" What is 10o8iu\"\n","doc5 = nlp(text5)\n","# spaCy Tokenizer\n","print('\\nspaCy Tokenizer', [token.text for token in doc5], sep = '\\n')"]},{"cell_type":"markdown","metadata":{"id":"Q3shEsBmHDsf"},"source":["  ## <font color = 'dodgerblue'>**Example 4**\n","It takes into acount special cases like C++, U.S.A"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705880734739,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"xzwC7j9Si7Mi","outputId":"2f0e24e7-b2a7-4ada-eb7c-af450a327790"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Some', 'good', 'programming', 'languages', 'to', 'know', 'HTML,', 'CSS,', 'JavaScript,', 'C++,', 'and', 'Node.js.']\n","['Some', 'good', 'programming', 'languages', 'to', 'know', 'HTML', ',', 'CSS', ',', 'JavaScript', ',', 'C++', ',', 'and', 'Node.js', '.']\n"]}],"source":["text5=\"Some good programming languages to know HTML, CSS, JavaScript, C++, and Node.js.\"\n","doc5 = nlp(text5)\n","# Whitespace Tokenizer\n","print(text5.split())\n","\n","# spaCy Tokenizer\n","print([token.text for token in doc5])\n"]},{"cell_type":"markdown","metadata":{"id":"8PtmgOHyHZZa"},"source":["  ## <font color = 'dodgerblue'>**Text Processing/Cleaning**\n","Spacy's tokens have attributes which can be very useful in text cleaning. https://spacy.io/api/token."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7FJg3TnvY8Lw"},"outputs":[],"source":["# let us check other attributes of token class\n","att_doc1 ={'token': [token for token in doc1],\n","          'token.idx': [token.idx for token in doc1],\n","          'token.text': [token.text for token in doc1],\n","          'token.is_alpha': [token.is_alpha for token in doc1],\n","          'token.is_punct': [token.is_punct for token in doc1],\n","          'token.is_space': [token.is_space for token in doc1],\n","          'token.is_stop': [token.is_stop for token in doc1],\n","          'token.like_num': [token.like_num for token in doc1],\n","          'token.is_digit': [token.is_digit for token in doc1],\n","          'token.like_url': [token.like_url for token in doc1],\n","           'token.like_email': [token.like_url for token in doc1],\n","\n","          }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1705880736223,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"yJwfSHax9Oc-","outputId":"bd701156-f416-42e9-ec24-2c31379eb7d7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["        token  token.idx token.text  token.is_alpha  token.is_punct  \\\n","0       China          0      China            True           False   \n","1          's          5         's           False           False   \n","2     capital          8    capital            True           False   \n","3          is         16         is            True           False   \n","4     Beijing         19    Beijing            True           False   \n","..        ...        ...        ...             ...             ...   \n","94        Ten        434        Ten            True           False   \n","95         is        438         is            True           False   \n","96  different        441  different            True           False   \n","97       from        451       from            True           False   \n","98         10        456         10           False           False   \n","\n","    token.is_space  token.is_stop  token.like_num  token.is_digit  \\\n","0            False          False           False           False   \n","1            False           True           False           False   \n","2            False          False           False           False   \n","3            False           True           False           False   \n","4            False          False           False           False   \n","..             ...            ...             ...             ...   \n","94           False           True            True           False   \n","95           False           True           False           False   \n","96           False          False           False           False   \n","97           False           True           False           False   \n","98           False          False            True            True   \n","\n","    token.like_url  token.like_email  \n","0            False             False  \n","1            False             False  \n","2            False             False  \n","3            False             False  \n","4            False             False  \n","..             ...               ...  \n","94           False             False  \n","95           False             False  \n","96           False             False  \n","97           False             False  \n","98           False             False  \n","\n","[99 rows x 11 columns]"],"text/html":["\n","  <div id=\"df-5a3f97b0-7c1d-4a7d-9aa1-0d05b3c1b2ed\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>token</th>\n","      <th>token.idx</th>\n","      <th>token.text</th>\n","      <th>token.is_alpha</th>\n","      <th>token.is_punct</th>\n","      <th>token.is_space</th>\n","      <th>token.is_stop</th>\n","      <th>token.like_num</th>\n","      <th>token.is_digit</th>\n","      <th>token.like_url</th>\n","      <th>token.like_email</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>China</td>\n","      <td>0</td>\n","      <td>China</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>'s</td>\n","      <td>5</td>\n","      <td>'s</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>capital</td>\n","      <td>8</td>\n","      <td>capital</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>is</td>\n","      <td>16</td>\n","      <td>is</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Beijing</td>\n","      <td>19</td>\n","      <td>Beijing</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>94</th>\n","      <td>Ten</td>\n","      <td>434</td>\n","      <td>Ten</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>is</td>\n","      <td>438</td>\n","      <td>is</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>different</td>\n","      <td>441</td>\n","      <td>different</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>from</td>\n","      <td>451</td>\n","      <td>from</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>10</td>\n","      <td>456</td>\n","      <td>10</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>99 rows × 11 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5a3f97b0-7c1d-4a7d-9aa1-0d05b3c1b2ed')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-5a3f97b0-7c1d-4a7d-9aa1-0d05b3c1b2ed button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-5a3f97b0-7c1d-4a7d-9aa1-0d05b3c1b2ed');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-03a42c72-1c87-4e29-9b5e-d129fa214c6d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-03a42c72-1c87-4e29-9b5e-d129fa214c6d')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-03a42c72-1c87-4e29-9b5e-d129fa214c6d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":104}],"source":["pd.DataFrame(att_doc1)\n"]},{"cell_type":"markdown","metadata":{"id":"-aWp36ssIEp_"},"source":["  ## <font color = 'dodgerblue'>**Extract only numbers and alphabets**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705880737541,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"v2-4NG0KXZJY","outputId":"db98129a-d59f-41be-db37-481e87f7e036"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['China',\n"," 'capital',\n"," 'is',\n"," 'Beijing',\n"," 'Beijing',\n"," 'is',\n"," 'where',\n"," 'we',\n"," 'go',\n"," 'Let',\n"," 'travel',\n"," 'to',\n"," 'Hong',\n"," 'Kong',\n"," 'from',\n"," 'Beijing',\n"," 'A',\n"," 'friend',\n"," 'is',\n"," 'pursuing',\n"," 'his',\n"," 'from',\n"," 'Beijing',\n"," 'Beijing',\n"," 'is',\n"," 'a',\n"," 'cool',\n"," 'place',\n"," 'Awesome',\n"," 'A',\n"," 'Rolex',\n"," 'watch',\n"," 'costs',\n"," 'in',\n"," 'the',\n"," 'range',\n"," 'of',\n"," '3000.0',\n"," '8000.0',\n"," 'in',\n"," 'USA',\n"," 'and',\n"," 'China',\n"," 'I',\n"," 'going',\n"," 'to',\n"," 'buy',\n"," 'a',\n"," 'Rolexxxxxxxx',\n"," 'watch',\n"," 'happiness',\n"," 'rolex',\n"," 'for',\n"," 'more',\n"," 'info',\n"," 'see',\n"," 'Ten',\n"," 'is',\n"," 'different',\n"," 'from',\n"," '10']"]},"metadata":{},"execution_count":105}],"source":["# extract only alphabets and numbers\n","[token.text for token in doc1 if  (token.is_alpha or token.like_num)]"]},{"cell_type":"markdown","metadata":{"id":"opiBlWneISd3"},"source":["  ## <font color = 'dodgerblue'>**Remove punctuations**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":610,"status":"ok","timestamp":1705880740208,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"cTTDTLQDJkJM","outputId":"5cf2be14-0827-483e-eacf-d5d60f617567"},"outputs":[{"output_type":"stream","name":"stdout","text":["China's capital is Beijing. \n","\n","A Rolex watch costs in the range of $3000.0 - $8000.0 in USA\n"]}],"source":["print(text2)"]},{"cell_type":"code","source":["# crate doc object\n","doc2 = nlp(text2)"],"metadata":{"id":"S9Wlh-dIi3dV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1705880741372,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"XXxZO6IYaEKb","outputId":"5426d8f2-0784-417f-a99a-f83e034ce18c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"China 's capital is Beijing \\n\\n A Rolex watch costs in the range of $ 3000.0 $ 8000.0 in USA\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":108}],"source":["# remove punctuation\n","\" \".join([token.text for token in doc2 if  not token.is_punct])"]},{"cell_type":"markdown","metadata":{"id":"I-rO3UDQIYSB"},"source":["  ## <font color = 'dodgerblue'>**Extract/Remove URLs**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1705880741872,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"MBCfP-UEZLdx","outputId":"56248eab-50f3-4ad9-c831-f088f663c8a9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['https://colab.research.google.com/', 'utdallas.edu']"]},"metadata":{},"execution_count":109}],"source":["# extract urls\n","text7 = 'my urls are https://colab.research.google.com/ and utdallas.edu '\n","doc7 = nlp(text7)\n","[token.text for token in doc7 if  token.like_url]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1705880743478,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"L3Dbeq6YIlsw","outputId":"f63564a9-043d-46b0-c5e8-ca2ad0b6fc54"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'my urls are and'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":110}],"source":["# remove urls\n","\" \".join([token.text for token in doc7 if not token.like_url])"]},{"cell_type":"markdown","metadata":{"id":"3neuQHgiIcqI"},"source":["  ## <font color = 'dodgerblue'>**Extract/Remove emails**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1705880744724,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"kTzBuT95ZWLR","outputId":"d355ba0f-ceff-4843-90a5-5d726de9458f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['xyz@utdallas.edu', 'xyz@gmail.com']"]},"metadata":{},"execution_count":111}],"source":["# extract emails\n","text8 = 'my email is xyz@utdallas.edu or xyz@gmail.com'\n","doc8 = nlp(text8)\n","[token.text for token in doc8 if  token.like_email]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1705880745649,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"wiKcjI8RJMiF","outputId":"12af8f50-7e02-41b6-c1c9-9fbfd4e56fbf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'my email is or'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":112}],"source":["\" \".join([token.text for token in doc8 if not token.like_email])"]},{"cell_type":"markdown","metadata":{"id":"BWvBlKxGL0DO"},"source":["  ## <font color = 'dodgerblue'>**Stopwords**\n","\n","# Stop words\n","- Stop words are basically a set of most commonly used words in a language, for exampe, 'the', 'a', 'in', 'an' etc.\n","- The stop words do not provide any contextual meaning to the text and are therefore sometimes removed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7kLFH7IIfG_a"},"outputs":[],"source":["#  The following paragraph has been taken from https://en.wikipedia.org/wiki/Regular_expression\n","text9 = \"\"\"A regular expression (shortened as regex or regexp;[1] also referred to as rational expression[2][3]) is a sequence of characters that define a search pattern. Usually such patterns are used by string-searching algorithms for \"find\" or \"find and replace\" operations on strings, or for input validation. It is a technique developed in theoretical computer science and formal language theory.\n","The concept arose in the 1950s when the American mathematician Stephen Cole Kleene formalized the description of a regular language. The concept came into common use with Unix text-processing utilities. Different syntaxes for writing regular expressions have existed since the 1980s, one being the POSIX standard and another, widely used, being the Perl syntax.\n","Regular expressions are used in search engines, search and replace dialogs of word processors and text editors, in text processing utilities such as sed and AWK and in lexical analysis. Many programming languages provide regex capabilities either built-in or via libraries.\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ekf20_b7fTyl"},"outputs":[],"source":["doc9 = nlp(text9)"]},{"cell_type":"markdown","metadata":{"id":"3tb44jB1fXDC"},"source":["  ## <font color = 'dodgerblue'>**Understanding Stopwords**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pxECRXBPfhPV"},"outputs":[],"source":["# create tokens using spacy\n","tokens = [token.text for token in doc9 if not token.is_punct]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"co9ue8_Efo5I"},"outputs":[],"source":["from collections import Counter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3HXBSlOTfiAF"},"outputs":[],"source":["# create a counter object based on tokens obtained\n","# A Counter is a class containing dict objects that is used to count hashable objects\n","# Counter contains elements as keys in a dictionary and their counts as the values for the respective keys.\n","\n","counter = Counter(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1705880752989,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"Bol2sqh6fwXz","outputId":"bd661497-b1c7-4a11-a528-871a998d3a1e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Counter({'and': 7, 'in': 6, 'the': 6, 'or': 4, 'a': 4, 'regular': 3, 'as': 3, 'of': 3, 'search': 3, 'used': 3, 'for': 3, 'text': 3, 'regex': 2, 'is': 2, 'such': 2, 'are': 2, 'find': 2, 'replace': 2, 'language': 2, '\\n': 2, 'The': 2, 'concept': 2, 'processing': 2, 'utilities': 2, 'expressions': 2, 'being': 2, 'A': 1, 'expression': 1, 'shortened': 1, 'regexp;[1': 1, 'also': 1, 'referred': 1, 'to': 1, 'rational': 1, 'expression[2][3': 1, 'sequence': 1, 'characters': 1, 'that': 1, 'define': 1, 'pattern': 1, 'Usually': 1, 'patterns': 1, 'by': 1, 'string': 1, 'searching': 1, 'algorithms': 1, 'operations': 1, 'on': 1, 'strings': 1, 'input': 1, 'validation': 1, 'It': 1, 'technique': 1, 'developed': 1, 'theoretical': 1, 'computer': 1, 'science': 1, 'formal': 1, 'theory': 1, 'arose': 1, '1950s': 1, 'when': 1, 'American': 1, 'mathematician': 1, 'Stephen': 1, 'Cole': 1, 'Kleene': 1, 'formalized': 1, 'description': 1, 'came': 1, 'into': 1, 'common': 1, 'use': 1, 'with': 1, 'Unix': 1, 'Different': 1, 'syntaxes': 1, 'writing': 1, 'have': 1, 'existed': 1, 'since': 1, '1980s': 1, 'one': 1, 'POSIX': 1, 'standard': 1, 'another': 1, 'widely': 1, 'Perl': 1, 'syntax': 1, 'Regular': 1, 'engines': 1, 'dialogs': 1, 'word': 1, 'processors': 1, 'editors': 1, 'sed': 1, 'AWK': 1, 'lexical': 1, 'analysis': 1, 'Many': 1, 'programming': 1, 'languages': 1, 'provide': 1, 'capabilities': 1, 'either': 1, 'built': 1, 'via': 1, 'libraries': 1})\n"]}],"source":["# print counter\n","print(counter)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":487,"status":"ok","timestamp":1705880754257,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"JZzfJeXqfw2O","outputId":"386b8a81-0eb5-4038-8cce-1ca695b9f249"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('and', 7),\n"," ('in', 6),\n"," ('the', 6),\n"," ('or', 4),\n"," ('a', 4),\n"," ('regular', 3),\n"," ('as', 3),\n"," ('of', 3),\n"," ('search', 3),\n"," ('used', 3)]"]},"metadata":{},"execution_count":119}],"source":["# Counter class contains class methods that provide useful info using the count of elements.\n","counter.most_common(10)"]},{"cell_type":"markdown","metadata":{"id":"L8czdjt1gB0W"},"source":["  ## <font color = 'dodgerblue'>**Stop Words with Spacy**\n","- Each model in Spacy has default list of stopwords.\n","- You can check that using model.Defaults.stop_words.\n","- You can also check whether a particular word is a stopword.\n","- Further, you can modify the default list of stopwords.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1705880755616,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"U_HB0d-wgFu9","outputId":"7cff96fc-ba91-44ed-d47d-8e0359b64eb1"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'towards', 'becoming', 'whence', 'whose', 'together', '‘s', 'though', 'nor', 're', 'namely', 'elsewhere', 'so', 'yourselves', 'never', 'therein', 'its', 'hundred', 'as', 'was', 'thereby', 'becomes', 'formerly', 'but', 'call', 'she', 'by', 'ever', 'really', 'last', 'thus', 'regarding', 'empty', 'beforehand', 'often', 'on', 'anyway', \"'d\", 'seeming', 'himself', 'upon', 'several', 'via', 'serious', 'her', 'an', 'therefore', 'otherwise', 'eight', 'only', 'among', 'no', 'nobody', 'except', 'those', 'herein', 'from', 'side', 'can', 'within', 'until', 'yet', 'thence', 'this', 'under', '’d', 'n’t', '’ll', 'throughout', 'keep', 'me', 'cannot', 'give', 'since', 'top', 'ten', 'wherein', 'back', 'below', 'already', 'noone', '’re', 'how', 'during', 'due', 'ourselves', 'latter', 'nowhere', 'toward', 'wherever', 'then', 'everywhere', 'rather', 'up', 'almost', 'a', 'anything', 'part', \"'re\", 'sometime', 'along', 'other', 'fifteen', 'have', 'amongst', 'indeed', 'least', 'meanwhile', 'all', '‘ve', 'could', 'ours', 'where', 'various', 'whereafter', 'whenever', 'which', \"'m\", 'whereas', 'used', 'they', 'at', 'just', 'mine', 'with', 'move', 'whom', 'above', 'even', 'between', 'amount', 'name', 'before', 'what', 'show', 'your', 'say', 'get', 'our', 'has', 'him', '‘m', 'less', 'become', 'we', 'herself', '’s', \"'ve\", 'us', 'any', 'although', 'over', 'first', 'hence', 'many', 'however', 'six', 'seems', 'always', 'down', 'same', 'beyond', 'n‘t', 'yours', 'take', 'whereupon', 'whole', 'much', 'unless', 'sometimes', 'former', 'may', 'everyone', \"'s\", 'be', 'front', 'others', 'once', 'few', 'twelve', 'who', 'anywhere', 'yourself', 'enough', 'either', 'ca', 'moreover', 'see', 'none', 'put', 'will', 'would', 'hereby', 'must', 'you', 'also', 'these', 'most', 'every', 'thereupon', 'themselves', 'nine', 'onto', 'in', 'did', 'please', 'off', 'done', 'perhaps', 'afterwards', 'eleven', 'because', 'around', 'four', 'someone', 'behind', 'i', 'using', 'more', 'here', 'next', 'doing', 'beside', 'not', 'hereafter', 'it', 'and', 'whither', 'through', 'latterly', 'of', 'nevertheless', 'whether', 'made', 'anyhow', 'whatever', '’m', 'neither', 'one', 'out', 'the', 'further', 'per', 'am', 'mostly', '’ve', 'go', 'five', 'being', 'whoever', 'that', 'well', 'there', 'somewhere', 'thru', 'hereupon', 'some', 'bottom', 'now', 'against', 'about', 'do', 'than', 'are', '‘ll', 'still', 'twenty', 'else', 'anyone', 'his', 'had', 'were', 'seemed', \"'ll\", 'third', 'itself', 'each', 'myself', 'to', 'should', 'after', 'while', 'alone', 'does', 'full', 'make', 'own', 'seem', 'another', 'everything', \"n't\", 'for', 'somehow', 'sixty', '‘d', 'why', 'three', 'whereby', 'two', 'when', 'nothing', 'too', 'fifty', 'thereafter', 'very', 'been', 'again', 'them', 'or', 'my', 'became', 'he', 'into', 'their', 'such', '‘re', 'quite', 'forty', 'something', 'both', 'might', 'besides', 'without', 'is', 'across', 'hers', 'if'}\n"]}],"source":["# default stopwords from the loaded model in spaCy\n","# the stopwords will change with the librray we import\n","print(nlp.Defaults.stop_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1705880757253,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"Z8aRa8yqgLVU","outputId":"16a252f8-f299-4efa-a508-4f4a178856e4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["326"]},"metadata":{},"execution_count":121}],"source":["len(nlp.Defaults.stop_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1705880758802,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"Sjd70KhJhmc4","outputId":"bc52b18f-d189-4877-caa1-a45cef71262f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":122}],"source":["# To check whether word regular is in default stop words\n","'regular' in nlp.Defaults.stop_words"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":479,"status":"ok","timestamp":1705880760401,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"7eXMQdHphnIY","outputId":"a8760dd5-f079-4029-92a3-bef22e5b63f2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":123}],"source":["# modify spacy's default stop words;\n","# add regular as stopwords\n","nlp.Defaults.stop_words.add('regular')\n","'regular' in nlp.Defaults.stop_words"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78,"status":"ok","timestamp":1705880761371,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"Lv6xl-k9hqyp","outputId":"d0c8df33-2282-484e-aba1-2f7fa511e4eb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":124}],"source":["# now let us modify the default words again\n","# remove regular from default stop words\n","nlp.Defaults.stop_words.remove('regular')\n","'regular' in nlp.Defaults.stop_words"]},{"cell_type":"markdown","metadata":{"id":"lXXolZuTiOKE"},"source":["  ## <font color = 'dodgerblue'>**Remove stop words from text**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GVvehQKchvQK"},"outputs":[],"source":["tokens = [ token.text for token in doc9  if not (token.is_stop or token.is_punct)]\n","text9_clean = \" \".join(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705880763882,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"GD_5FP5OieEH","outputId":"26848ecf-1664-44aa-e201-1cc9064902b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["regular expression shortened regex regexp;[1 referred rational expression[2][3 sequence characters define search pattern Usually patterns string searching algorithms find find replace operations strings input validation technique developed theoretical computer science formal language theory \n"," concept arose 1950s American mathematician Stephen Cole Kleene formalized description regular language concept came common use Unix text processing utilities Different syntaxes writing regular expressions existed 1980s POSIX standard widely Perl syntax \n"," Regular expressions search engines search replace dialogs word processors text editors text processing utilities sed AWK lexical analysis programming languages provide regex capabilities built libraries\n"]}],"source":["print(text9_clean)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mhVaOmz0i6XI"},"outputs":[],"source":["counter = Counter(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705880765484,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"TkhgekZljAAi","outputId":"2361c5af-1438-44d8-b737-5e533451643e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('regular', 3),\n"," ('search', 3),\n"," ('text', 3),\n"," ('regex', 2),\n"," ('find', 2),\n"," ('replace', 2),\n"," ('language', 2),\n"," ('\\n', 2),\n"," ('concept', 2),\n"," ('processing', 2)]"]},"metadata":{},"execution_count":128}],"source":["counter.most_common(10)"]},{"cell_type":"markdown","metadata":{"id":"wBje2fG0egJW"},"source":["  ## <font color = 'dodgerblue'> **Lammetization**"]},{"cell_type":"markdown","metadata":{"id":"OHl8EWXue9EH"},"source":["<img src =\"https://drive.google.com/uc?export=view&id=1zk5L9vyg6LlTW8nCZh-YxBOyU-IinShN\" width = 500>\n","\n","image source: https://spacy.io/models"]},{"cell_type":"markdown","metadata":{"id":"HsxNLGqyemn8"},"source":["- For Lammetization we need POS and for POS we need `['tagger', 'attribute_ruler' , tok2vec]`\n","- Hence for lammetization we can disable  `['ner', 'parser']`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":420,"status":"ok","timestamp":1705880768255,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"QBOLsAXvfrHM","outputId":"533b1f11-35cd-49ba-9947-11e2b6867179"},"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n"]}],"source":["print(nlp.pipe_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8NNey3IQflhz"},"outputs":[],"source":["# restore pipelines comonents\n","disabled.restore()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":262,"status":"ok","timestamp":1705880774306,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"VTU_-dsYftEo","outputId":"b58e9b01-a8ea-4290-fcd2-ab2c4def8442"},"outputs":[{"output_type":"stream","name":"stdout","text":["['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"]}],"source":["print(nlp.pipe_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xhbp76CUfvJv"},"outputs":[],"source":["disabled = nlp.select_pipes(disable= ['ner', 'parser'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705880778659,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"TZk_7xfEgI1C","outputId":"d5c73b84-0100-4af7-c554-a65a1a9e705d"},"outputs":[{"output_type":"stream","name":"stdout","text":["['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer']\n"]}],"source":["print(nlp.pipe_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z0eNtqssgcAS"},"outputs":[],"source":["# We will look at lemmatizing on a small part of the text\n","text10 =\" A regular expression also referred to as rational expression is a sequence of characters that define a search pattern\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UrfZ-1-TguB6"},"outputs":[],"source":["doc10=nlp(text10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"obRnprZeg9px"},"outputs":[],"source":["# Lemmatizing the text\n","lemmas= [token.lemma_ for token in doc10]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":223,"status":"ok","timestamp":1705880804110,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"C0c64-VihCuo","outputId":"2ffac680-0239-4410-f55e-6effb0b3d6cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["lemmatized :   a regular expression also refer to as rational expression be a sequence of character that define a search pattern\n","original   :  A regular expression also referred to as rational expression is a sequence of characters that define a search pattern\n"]}],"source":["print(f'lemmatized : {\" \".join(lemmas)}')\n","print(f'original   : {text10}')"]},{"cell_type":"markdown","metadata":{"id":"LPv6PbcB6n-H"},"source":["# <font color = 'dodgerblue'>**Sentence tokenization using spacy**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PiReNVEPofkT"},"outputs":[],"source":["disabled.restore()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gZ6vK4u5ojLb"},"outputs":[],"source":["doc2 = nlp(text2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705880810824,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"9v2yerQx6lHW","outputId":"49a8853b-733d-4a0a-9b98-979331599ec6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"China's capital is Beijing. \\n\\n\",\n"," 'A Rolex watch costs in the range of $3000.0 - $8000.0 in USA']"]},"metadata":{},"execution_count":140}],"source":["# We use doc.sents to tokenize sentences\n","sentences = [sent.text for sent in doc2.sents]\n","sentences"]},{"cell_type":"markdown","metadata":{"id":"incarAQAzYZH"},"source":["# <font color = 'dodgerblue'>**Name Entity Recognition (NER)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JpJACyiXzbYE"},"outputs":[],"source":["disabled = nlp.select_pipes(disable= ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":248,"status":"ok","timestamp":1705880816021,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"BSeRI_eFzpIk","outputId":"06d45f66-522f-4506-b680-ffeb5ef0ae9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["['ner']\n"]}],"source":["print(nlp.pipe_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1705880817473,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"n-O50Pijzstk","outputId":"e557f1a0-5d5e-45c6-be2b-67d3e72f907d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Entity              : Tag\n","\n","China                : GPE\n","Beijing              : GPE\n","Beijing              : GPE\n","Hong Kong            : GPE\n","Beijing              : GPE\n","M.S                  : ORG\n","Beijing              : GPE\n","Beijing              : GPE\n","Rolex                : PERSON\n","$3000.0 - $          : MONEY\n","8000.0               : MONEY\n","USA                  : GPE\n","China                : GPE\n","Rolexxxxxxxx         : NORP\n","http://www.example_beijing.com : PERSON\n","Ten                  : CARDINAL\n","10                   : CARDINAL\n"]}],"source":["doc1 = nlp(text1)\n","print(f'{\"Entity\":<20}: Tag\\n')\n","for entity in doc1.ents:\n","  print(f'{entity.text:<20} : {entity.label_}')"]},{"cell_type":"code","source":["spacy.explain('GPE')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"49_YSB950cBh","executionInfo":{"status":"ok","timestamp":1705880820667,"user_tz":360,"elapsed":4,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"aaf5c27a-3f2a-4100-8988-545ffb967167"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Countries, cities, states'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":144}]},{"cell_type":"code","source":["spacy.explain('NORP')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"JYzGeb2D0jSh","executionInfo":{"status":"ok","timestamp":1705880822142,"user_tz":360,"elapsed":3,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"606abe8c-f3c4-4d2a-b249-263dfcd1fe13"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Nationalities or religious or political groups'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":145}]},{"cell_type":"code","source":["spacy.explain('CARDINAL')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"cKh8_NGi0rco","executionInfo":{"status":"ok","timestamp":1705880824744,"user_tz":360,"elapsed":224,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"eda512d1-5731-419e-dd63-6e2d8779ce0e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Numerals that do not fall under another type'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":146}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANqtbmjU05AL"},"outputs":[],"source":["# You can use displacy to visualize the named entities\n","from spacy import displacy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"elapsed":226,"status":"ok","timestamp":1705880829823,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"-OEWQiPB1Dgl","outputId":"8c0f1178-e4c2-429c-b310-6a8d3c00a997"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    China\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n","'s capital is \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Beijing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n",". <br><br>\n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Beijing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n"," is where we'll go. <br><br>Let's travel to \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Hong Kong\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n"," from \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Beijing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n",".           <br><br>A friend is pursuing his \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    M.S\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," from \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Beijing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n",". <br><br>\n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Beijing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n"," is a cool place!!! :-P &lt;3 #Awesome           <br><br>A \n","<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Rolex\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n","</mark>\n"," watch costs in the range of \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    $3000.0 - $\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n","</mark>\n","\n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    8000.0\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n","</mark>\n"," in \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    USA\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n"," and \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    China\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n",". <br><br>@tompeter I'm           <br><br>going to buy a \n","<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Rolexxxxxxxx\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n","</mark>\n"," watch!!! :-D #happiness #rolex &lt;3           for more info see: \n","<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    http://www.example_beijing.com\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n","</mark>\n","! \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Ten\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n","</mark>\n"," is different from \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    10\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n","</mark>\n","</div></span>"]},"metadata":{}}],"source":["displacy.render(doc1,style='ent',jupyter=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hdMPbtzw05pQ"},"outputs":[],"source":["# text taken from https://oilprice.com/Energy/Oil-Prices/Oil-Rally-Continues-On-Bright-US-Economic-Data.html on June23 2021.\n","# Defining String\n","text11 = \"\"\"\n","Oil prices rose early on Wednesday, driven by brighter economic prospects for the United States and continued recovery in oil demand in America and elsewhere in the world.\n","As of 9:04 a.m. EDT on Wednesday, ahead of the weekly inventory report by the U.S. Energy Information Administration (EIA), WTI Crude was up 1.04 percent at $73.61,\n","and Brent Crude traded at $75.54, up by 0.99 percent on the day.Prices found support late on Tuesday after the American Petroleum Institute (API)\n","reported a draw in crude oil inventories of 7.199 million barrels for the week ending June 18. If the EIA confirms a draw today, it would be the fifth consecutive week of crude inventory draws in the United States, where demand for fuels continues to grow.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02vppqSI1N4L"},"outputs":[],"source":["# Creating doc object\n","doc11 = nlp(text11)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705880840217,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"JczOtRPA1Tj-","outputId":"773ad13b-c03c-4efd-a6b0-25bc05448629"},"outputs":[{"output_type":"stream","name":"stdout","text":["Entity                                        : Tag\n","\n","Wednesday                                     : DATE\n","the United States                             : GPE\n","America                                       : GPE\n","9:04 a.m. EDT                                 : TIME\n","Wednesday                                     : DATE\n","weekly                                        : DATE\n","the U.S. Energy Information Administration    : ORG\n","EIA                                           : ORG\n","1.04 percent                                  : PERCENT\n","73.61                                         : MONEY\n","Brent Crude                                   : ORG\n","75.54                                         : MONEY\n","0.99 percent                                  : PERCENT\n","the day                                       : DATE\n","Tuesday                                       : DATE\n","the American Petroleum Institute              : ORG\n","API                                           : ORG\n","7.199 million barrels                         : QUANTITY\n","the week ending June 18                       : DATE\n","EIA                                           : ORG\n","today                                         : DATE\n","the fifth consecutive week                    : DATE\n","the United States                             : GPE\n"]}],"source":["# doc.ents give us the named entities\n","# We can use entity.text and entity.label_ to get the entities and their tags\n","print(f'{\"Entity\":<45} : Tag\\n')\n","for entity in doc11.ents:\n","  print(f'{entity.text:<45} : {entity.label_}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"executionInfo":{"elapsed":252,"status":"ok","timestamp":1705880843654,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"vDH9OBGX1Z_y","outputId":"340d619f-0fda-4980-e675-25bb36f703c2"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"><br>Oil prices rose early on \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Wednesday\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n",", driven by brighter economic prospects for \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    the United States\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n"," and continued recovery in oil demand in \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    America\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n"," and elsewhere in the world.<br>As of \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    9:04 a.m. EDT\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n","</mark>\n"," on \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Wednesday\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n",", ahead of the \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    weekly\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n"," inventory report by \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    the U.S. Energy Information Administration\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," (\n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    EIA\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n","), WTI Crude was up \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    1.04 percent\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n","</mark>\n"," at $\n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    73.61\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n","</mark>\n",",<br>and \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Brent Crude\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," traded at $\n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    75.54\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n","</mark>\n",", up by \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    0.99 percent\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n","</mark>\n"," on \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    the day\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n",".Prices found support late on \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Tuesday\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n"," after \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    the American Petroleum Institute\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," (\n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    API\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n",")<br>reported a draw in crude oil inventories of \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    7.199 million barrels\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n","</mark>\n"," for \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    the week ending June 18\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n",". If the \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    EIA\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," confirms a draw \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    today\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n",", it would be \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    the fifth consecutive week\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n"," of crude inventory draws in \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    the United States\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n",", where demand for fuels continues to grow.<br></div></span>"]},"metadata":{}}],"source":["# You can use displacy to visualize the named entities\n","displacy.render(doc11,style='ent',jupyter=True)"]},{"cell_type":"markdown","metadata":{"id":"vt2gZ_Im3KZY"},"source":["# <font color = 'dodgerblue'>**Part of Speech Tagging**\n","\n","<img src =\"https://drive.google.com/uc?export=view&id=1zk5L9vyg6LlTW8nCZh-YxBOyU-IinShN\" width = 500>\n","\n","image source: https://spacy.io/models\n","\n","- For POS we need `['tagger', 'attribute_ruler' , tok2vec]`\n","-The POS tags come from rules that map token.tag to token.pos in (see mapping here https://spacy.io/api/annotation) the attribute_ruler component.\n","- If the dependency parse is available, there are more specific rules it can apply related to AUX and VERB.\n","- The mapping is hard to do perfectly because the token.tag (PTB tags) that come from the tagger don't make an aux/verb distinction at all.\n","- Hence for POS we can disable `['lemmatizer', 'ner']`\n","\n","source: https://stackoverflow.com/questions/69313960/does-spacys-version3-1-pos-tagger-depends-on-parser\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":229,"status":"ok","timestamp":1705880848005,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"ZM83lPHy3qPI","outputId":"0d115081-5761-4de8-c9f3-74584546fd26"},"outputs":[{"output_type":"stream","name":"stdout","text":["['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"]}],"source":["disabled.restore()\n","print(nlp.pipe_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1705880849575,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"CWAQyD7v3ttn","outputId":"6b3397e6-6982-47ef-95ec-60e583db2657"},"outputs":[{"output_type":"stream","name":"stdout","text":["['tok2vec', 'tagger', 'parser', 'attribute_ruler']\n"]}],"source":["disabled = nlp.select_pipes(disable= ['ner','lemmatizer'])\n","print(nlp.pipe_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":233,"status":"ok","timestamp":1705880852057,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"K1sqN0-Rjkvo","outputId":"022599cf-bea3-4184-fd09-50739d403efe"},"outputs":[{"output_type":"stream","name":"stdout","text":["A               -> DET        -> DT        \n","regular         -> ADJ        -> JJ        \n","expression      -> NOUN       -> NN        \n","(               -> PUNCT      -> -LRB-     \n","shortened       -> VERB       -> VBN       \n","as              -> ADP        -> IN        \n","regex           -> NOUN       -> NN        \n","or              -> CCONJ      -> CC        \n","regexp;[1       -> PROPN      -> NNP       \n","]               -> PUNCT      -> -RRB-     \n","also            -> ADV        -> RB        \n","referred        -> VERB       -> VBD       \n","to              -> ADP        -> IN        \n","as              -> ADP        -> IN        \n","rational        -> ADJ        -> JJ        \n","expression[2][3 -> PROPN      -> NNP       \n","]               -> PUNCT      -> -RRB-     \n",")               -> PUNCT      -> -RRB-     \n","is              -> AUX        -> VBZ       \n","a               -> DET        -> DT        \n","sequence        -> NOUN       -> NN        \n","of              -> ADP        -> IN        \n","characters      -> NOUN       -> NNS       \n","that            -> PRON       -> WDT       \n","define          -> VERB       -> VBP       \n","a               -> DET        -> DT        \n","search          -> NOUN       -> NN        \n","pattern         -> NOUN       -> NN        \n",".               -> PUNCT      -> .         \n","Usually         -> ADV        -> RB        \n","such            -> ADJ        -> JJ        \n","patterns        -> NOUN       -> NNS       \n","are             -> AUX        -> VBP       \n","used            -> VERB       -> VBN       \n","by              -> ADP        -> IN        \n","string          -> NOUN       -> NN        \n","-               -> PUNCT      -> HYPH      \n","searching       -> VERB       -> VBG       \n","algorithms      -> NOUN       -> NNS       \n","for             -> ADP        -> IN        \n","\"               -> PUNCT      -> ``        \n","find            -> VERB       -> VB        \n","\"               -> PUNCT      -> ''        \n","or              -> CCONJ      -> CC        \n","\"               -> PUNCT      -> ``        \n","find            -> VERB       -> VB        \n","and             -> CCONJ      -> CC        \n","replace         -> VERB       -> VB        \n","\"               -> PUNCT      -> ''        \n","operations      -> NOUN       -> NNS       \n","on              -> ADP        -> IN        \n","strings         -> NOUN       -> NNS       \n",",               -> PUNCT      -> ,         \n","or              -> CCONJ      -> CC        \n","for             -> ADP        -> IN        \n","input           -> NOUN       -> NN        \n","validation      -> NOUN       -> NN        \n",".               -> PUNCT      -> .         \n","It              -> PRON       -> PRP       \n","is              -> AUX        -> VBZ       \n","a               -> DET        -> DT        \n","technique       -> NOUN       -> NN        \n","developed       -> VERB       -> VBN       \n","in              -> ADP        -> IN        \n","theoretical     -> ADJ        -> JJ        \n","computer        -> NOUN       -> NN        \n","science         -> NOUN       -> NN        \n","and             -> CCONJ      -> CC        \n","formal          -> ADJ        -> JJ        \n","language        -> NOUN       -> NN        \n","theory          -> NOUN       -> NN        \n",".               -> PUNCT      -> .         \n","\n","               -> SPACE      -> _SP       \n","The             -> DET        -> DT        \n","concept         -> NOUN       -> NN        \n","arose           -> VERB       -> VBD       \n","in              -> ADP        -> IN        \n","the             -> DET        -> DT        \n","1950s           -> NOUN       -> NNS       \n","when            -> SCONJ      -> WRB       \n","the             -> DET        -> DT        \n","American        -> ADJ        -> JJ        \n","mathematician   -> NOUN       -> NN        \n","Stephen         -> PROPN      -> NNP       \n","Cole            -> PROPN      -> NNP       \n","Kleene          -> PROPN      -> NNP       \n","formalized      -> VERB       -> VBD       \n","the             -> DET        -> DT        \n","description     -> NOUN       -> NN        \n","of              -> ADP        -> IN        \n","a               -> DET        -> DT        \n","regular         -> ADJ        -> JJ        \n","language        -> NOUN       -> NN        \n",".               -> PUNCT      -> .         \n","The             -> DET        -> DT        \n","concept         -> NOUN       -> NN        \n","came            -> VERB       -> VBD       \n","into            -> ADP        -> IN        \n","common          -> ADJ        -> JJ        \n","use             -> NOUN       -> NN        \n","with            -> ADP        -> IN        \n","Unix            -> PROPN      -> NNP       \n","text            -> NOUN       -> NN        \n","-               -> PUNCT      -> HYPH      \n","processing      -> VERB       -> VBG       \n","utilities       -> NOUN       -> NNS       \n",".               -> PUNCT      -> .         \n","Different       -> ADJ        -> JJ        \n","syntaxes        -> NOUN       -> NNS       \n","for             -> ADP        -> IN        \n","writing         -> VERB       -> VBG       \n","regular         -> ADJ        -> JJ        \n","expressions     -> NOUN       -> NNS       \n","have            -> AUX        -> VBP       \n","existed         -> VERB       -> VBN       \n","since           -> SCONJ      -> IN        \n","the             -> DET        -> DT        \n","1980s           -> NUM        -> CD        \n",",               -> PUNCT      -> ,         \n","one             -> NUM        -> CD        \n","being           -> AUX        -> VBG       \n","the             -> DET        -> DT        \n","POSIX           -> PROPN      -> NNP       \n","standard        -> NOUN       -> NN        \n","and             -> CCONJ      -> CC        \n","another         -> PRON       -> DT        \n",",               -> PUNCT      -> ,         \n","widely          -> ADV        -> RB        \n","used            -> ADJ        -> JJ        \n",",               -> PUNCT      -> ,         \n","being           -> AUX        -> VBG       \n","the             -> DET        -> DT        \n","Perl            -> PROPN      -> NNP       \n","syntax          -> NOUN       -> NN        \n",".               -> PUNCT      -> .         \n","\n","               -> SPACE      -> _SP       \n","Regular         -> ADJ        -> JJ        \n","expressions     -> NOUN       -> NNS       \n","are             -> AUX        -> VBP       \n","used            -> VERB       -> VBN       \n","in              -> ADP        -> IN        \n","search          -> NOUN       -> NN        \n","engines         -> NOUN       -> NNS       \n",",               -> PUNCT      -> ,         \n","search          -> NOUN       -> NN        \n","and             -> CCONJ      -> CC        \n","replace         -> VERB       -> VB        \n","dialogs         -> NOUN       -> NNS       \n","of              -> ADP        -> IN        \n","word            -> NOUN       -> NN        \n","processors      -> NOUN       -> NNS       \n","and             -> CCONJ      -> CC        \n","text            -> NOUN       -> NN        \n","editors         -> NOUN       -> NNS       \n",",               -> PUNCT      -> ,         \n","in              -> ADP        -> IN        \n","text            -> NOUN       -> NN        \n","processing      -> NOUN       -> NN        \n","utilities       -> NOUN       -> NNS       \n","such            -> ADJ        -> JJ        \n","as              -> ADP        -> IN        \n","sed             -> VERB       -> VBN       \n","and             -> CCONJ      -> CC        \n","AWK             -> PROPN      -> NNP       \n","and             -> CCONJ      -> CC        \n","in              -> ADP        -> IN        \n","lexical         -> ADJ        -> JJ        \n","analysis        -> NOUN       -> NN        \n",".               -> PUNCT      -> .         \n","Many            -> ADJ        -> JJ        \n","programming     -> NOUN       -> NN        \n","languages       -> NOUN       -> NNS       \n","provide         -> VERB       -> VBP       \n","regex           -> NOUN       -> NN        \n","capabilities    -> NOUN       -> NNS       \n","either          -> CCONJ      -> CC        \n","built           -> VERB       -> VBN       \n","-               -> PUNCT      -> HYPH      \n","in              -> ADP        -> RP        \n","or              -> CCONJ      -> CC        \n","via             -> ADP        -> IN        \n","libraries       -> NOUN       -> NNS       \n",".               -> PUNCT      -> .         \n"]}],"source":["# Get Part of Speech (POS) tags\n","# print token text, pos and tag\n","doc9 = nlp(text9)\n","for token in doc9:\n","    print(f'{token.text:<15} -> {token.pos_:<10} -> {token.tag_:<10}')"]},{"cell_type":"markdown","metadata":{"id":"q9mNHiGJt-Dh"},"source":["The list of pos_ attributes along with its meaning:\n","\n","* ADJ: adjective, e.g. old, green, first, etc.\n","* ADP: adposition, e.g. in, to, during, etc.\n","* ADV: adverb, e.g. very, tomorrow, down, where, there, etc.\n","* AUX: auxiliary, e.g. is, has (done), will (do), should (do), etc.\n","* CONJ: conjunction, e.g. and, or, but, etc.\n","* CCONJ: coordinating conjunction, e.g. and, or, but, etc.\n","* DET: determiner, e.g. a, an, the, etc.\n","* INTJ: interjection, e.g. psst, ouch, bravo, hello, etc.\n","* NOUN: noun, e.g. girl, cat, tree, air, etc.\n","* NUM: numeral, e.g. 1, 2017, one, seventy-seven, IV, MMXIV, etc.\n","* PART: particle, e.g. ’s, not, etc.\n","* PRON: pronoun, e.g I, you, he, she, myself, themselves, somebody, etc.\n","* PROPN: proper noun, e.g. Mary, John, Chucago, NATO, etc.\n","* PUNCT: punctuation, e.g. ., (, ), ?, etc.\n","* SCONJ: subordinating conjunction, e.g. if, while, that, etc.\n","* SYM: symbol, e.g. $, %, §, ©, +, −, ×, ÷, =, :), 😝, etc.\n","* VERB: verb, e.g. run, runs, running, ate, eating, etc.\n","* X: other, e.g. sfpksdpsxmsa(some random text).\n","* SPACE: space."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gz1Mr0MEjkwL"},"outputs":[],"source":["# We can get any of the above Part of Speech\n","# For a list of the Parts of Speech click the link here\n","# https://spacy.io/usage/linguistic-features\n","# Let us get Verbs , Nouns and Proper Nouns\n","\n","Verbs = [token.text for token in doc9 if(token.pos_=='VERB')]\n","Nouns = [token.text for token in doc9 if(token.pos_=='NOUN')]\n","Proper_Nouns = [token.text for token in doc9 if(token.pos_=='PROPN')]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":236,"status":"ok","timestamp":1705880863385,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"gM6Foq6-jkwS","outputId":"f3a9eeee-8f32-40a8-e3fe-3e96b336eba2"},"outputs":[{"output_type":"stream","name":"stdout","text":["shortened\n","referred\n","define\n","used\n","searching\n","find\n","find\n","replace\n","developed\n","arose\n"]}],"source":["# Print Verbs\n","for verb in Verbs[:10]:\n","  print(verb)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":284,"status":"ok","timestamp":1705880866522,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"BDWs4ypDxwKq","outputId":"eb65739e-da09-49c2-f76f-99b38dc71042"},"outputs":[{"output_type":"stream","name":"stdout","text":["expression\n","regex\n","sequence\n","characters\n","search\n","pattern\n","patterns\n","string\n","algorithms\n","operations\n"]}],"source":["# Print Nouns\n","for noun in Nouns[:10]:\n","  print(noun)"]},{"cell_type":"markdown","metadata":{"id":"ummJZrvx0lKU"},"source":["# <font color = 'dodgerblue'>**Stemming**\n","- Not available in SPacy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D3Ch1Yais_jr"},"outputs":[],"source":["!pip install -U nltk -qq"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-28T23:38:09.490944Z","iopub.status.busy":"2022-08-28T23:38:09.490415Z","iopub.status.idle":"2022-08-28T23:38:09.951854Z","shell.execute_reply":"2022-08-28T23:38:09.951303Z","shell.execute_reply.started":"2022-08-28T23:38:09.490902Z"},"id":"BjfG4wSds_jr"},"outputs":[],"source":["# Import PorterStemmer from nltk.stem\n","from nltk.stem import PorterStemmer"]},{"cell_type":"markdown","metadata":{"id":"6fYOHH7hVngD"},"source":["## <font color = 'dodgerblue'>**Example**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-08-28T23:39:24.130677Z","iopub.status.busy":"2022-08-28T23:39:24.130365Z","iopub.status.idle":"2022-08-28T23:39:24.134007Z","shell.execute_reply":"2022-08-28T23:39:24.133565Z","shell.execute_reply.started":"2022-08-28T23:39:24.130663Z"},"executionInfo":{"elapsed":242,"status":"ok","timestamp":1705880896726,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"juQm06FateFv","outputId":"aba18e5a-dfb6-4d9f-a99b-32566f9f987a"},"outputs":[{"output_type":"stream","name":"stdout","text":["connection  :  connect\n","connected  :  connect\n","connnecter  :  connnect\n","connnecting  :  connnect\n","connect  :  connect\n"]}],"source":["# Create an object of class PorterStemmer\n","stemmer = PorterStemmer()\n","\n","words = ['connection', 'connected', 'connnecter', 'connnecting', 'connect']\n","\n","for w in words:\n","  print(w, \" : \", stemmer.stem(w))"]},{"cell_type":"markdown","metadata":{"id":"MOj1X0pks_jr"},"source":["# <font color = 'dodgerblue'>**Remove HTML Tags**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-29T02:26:02.857594Z","iopub.status.busy":"2022-08-29T02:26:02.857358Z","iopub.status.idle":"2022-08-29T02:26:02.860337Z","shell.execute_reply":"2022-08-29T02:26:02.859895Z","shell.execute_reply.started":"2022-08-29T02:26:02.857580Z"},"id":"FtOkdqESs_jr","tags":[]},"outputs":[],"source":["text3=\"\"\"I just can't understand the negative comments about this film. Yes it is a typical\n","boy-meets-girl romance but it is done with such flair and polish that the time just flies by.\n","Henstridge (talk about winning the gene-pool lottery!) is as magnetic and alluring as ever\n","(who says the golden age of cinema is dead?) and Vartan holds his own.<br /><br />There is\n","simmering chemistry between the two leads; the film is most alive when they share a scene -\n","lots! It is done so well that you find yourself willing them to get together...<br /><br />Ignore\n","the negative comments - if you are feeling a bit blue, watch this flick, you will feel so much\n","better. If you are already happy, then you will be euphoric.<br /><br />(PS: I am 33, Male,\n","from the UK and a hopeless romantic still searching for his Princess...)\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-29T02:29:47.357070Z","iopub.status.busy":"2022-08-29T02:29:47.356872Z","iopub.status.idle":"2022-08-29T02:29:47.359439Z","shell.execute_reply":"2022-08-29T02:29:47.359124Z","shell.execute_reply.started":"2022-08-29T02:29:47.357057Z"},"id":"-PSmWoQ-s_js","tags":[]},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-29T02:26:44.406623Z","iopub.status.busy":"2022-08-29T02:26:44.406265Z","iopub.status.idle":"2022-08-29T02:26:44.409549Z","shell.execute_reply":"2022-08-29T02:26:44.409060Z","shell.execute_reply.started":"2022-08-29T02:26:44.406608Z"},"id":"4lZnq5Mus_js","tags":[]},"outputs":[],"source":["soup = BeautifulSoup(text3, \"html.parser\")"]},{"cell_type":"markdown","source":["- The code above is using the BeautifulSoup library in Python to parse an HTML document represented by the variable `text3`. The `BeautifulSoup` function creates a new BeautifulSoup object, which is stored in the variable `soup`.\n","\n","- The second argument, `\"html.parser\"`, specifies the parser to be used. In this case, the `\"html.parser\"` argument means that the HTML document will be parsed using Python's built-in HTML parser. This parser will process the HTML and convert it into a tree-like structure that can be easily navigated and searched using BeautifulSoup methods.\n","\n","- The resulting `soup` object can be used to extract data from the HTML document, such as extracting tags, searching for tags based on specific attributes, and modifying the HTML document."],"metadata":{"id":"U3W8S9LW2m7-"}},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-29T02:27:06.185972Z","iopub.status.busy":"2022-08-29T02:27:06.185740Z","iopub.status.idle":"2022-08-29T02:27:06.188519Z","shell.execute_reply":"2022-08-29T02:27:06.188157Z","shell.execute_reply.started":"2022-08-29T02:27:06.185958Z"},"id":"_eIS5dLws_js"},"outputs":[],"source":["# Extract all text from HTML document and store it in cleaned_text3\n","cleaned_text3 = soup.get_text()"]},{"cell_type":"markdown","source":["- The code uses the `get_text` method of the soup object to extract all text from the HTML document and store it in the variable `cleaned_text3`. The `get_text` method retrieves all text from the HTML document, including text within tags and any whitespace, and returns it as a string. This string can then be further processed, such as being split into sentences or tokens, or used for text analysis.\n","\n","- By using the `get_text` method, the code is effectively \"cleaning\" the HTML document of all its tags and just retaining its plain text content, which can be useful for certain text processing tasks where the HTML tags are not needed."],"metadata":{"id":"5-CK1nLi3uBN"}},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-29T02:27:14.927358Z","iopub.status.busy":"2022-08-29T02:27:14.926810Z","iopub.status.idle":"2022-08-29T02:27:14.932890Z","shell.execute_reply":"2022-08-29T02:27:14.932563Z","shell.execute_reply.started":"2022-08-29T02:27:14.927343Z"},"id":"8AQKgKKEs_js","outputId":"7675d74d-a82b-4fc9-9b0a-08b9ead61e10","tags":[],"colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"status":"ok","timestamp":1705880910168,"user_tz":360,"elapsed":3,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"I just can't understand the negative comments about this film. Yes it is a typical\\nboy-meets-girl romance but it is done with such flair and polish that the time just flies by.\\nHenstridge (talk about winning the gene-pool lottery!) is as magnetic and alluring as ever\\n(who says the golden age of cinema is dead?) and Vartan holds his own.There is\\nsimmering chemistry between the two leads; the film is most alive when they share a scene -\\nlots! It is done so well that you find yourself willing them to get together...Ignore\\nthe negative comments - if you are feeling a bit blue, watch this flick, you will feel so much\\nbetter. If you are already happy, then you will be euphoric.(PS: I am 33, Male,\\nfrom the UK and a hopeless romantic still searching for his Princess...)\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":166}],"source":["cleaned_text3"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-29T02:29:49.988862Z","iopub.status.busy":"2022-08-29T02:29:49.988652Z","iopub.status.idle":"2022-08-29T02:29:49.991577Z","shell.execute_reply":"2022-08-29T02:29:49.991242Z","shell.execute_reply.started":"2022-08-29T02:29:49.988849Z"},"id":"7JQoHQiVs_js","tags":[]},"outputs":[],"source":["def basic_clean(text: str) -> str:\n","    \"\"\"\n","    This function performs basic text cleaning on an input string by removing HTML tags (if present)\n","    and replacing newline and return characters with a space.\n","\n","    Parameters:\n","    text (str): The input string to be cleaned\n","\n","    Returns:\n","    str: The cleaned string\n","    \"\"\"\n","    # Use BeautifulSoup to remove HTML tags (if present)\n","    soup = BeautifulSoup(text, \"html.parser\")\n","    text = soup.get_text()\n","\n","    # Replace newline and return characters with a space\n","    return re.sub(r'[\\n\\r]',' ', text)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-29T02:29:50.482046Z","iopub.status.busy":"2022-08-29T02:29:50.481790Z","iopub.status.idle":"2022-08-29T02:29:50.484993Z","shell.execute_reply":"2022-08-29T02:29:50.484508Z","shell.execute_reply.started":"2022-08-29T02:29:50.482031Z"},"id":"kYaWgMP-s_js","tags":[]},"outputs":[],"source":["cleaned_text = basic_clean(text=text3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-29T02:29:56.702834Z","iopub.status.busy":"2022-08-29T02:29:56.702641Z","iopub.status.idle":"2022-08-29T02:29:56.706111Z","shell.execute_reply":"2022-08-29T02:29:56.705709Z","shell.execute_reply.started":"2022-08-29T02:29:56.702821Z"},"id":"TGyBFp_2s_js","outputId":"bff01bc9-31c5-44b6-c672-819dd81de198","colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"status":"ok","timestamp":1705880918092,"user_tz":360,"elapsed":225,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"I just can't understand the negative comments about this film. Yes it is a typical boy-meets-girl romance but it is done with such flair and polish that the time just flies by. Henstridge (talk about winning the gene-pool lottery!) is as magnetic and alluring as ever (who says the golden age of cinema is dead?) and Vartan holds his own.There is simmering chemistry between the two leads; the film is most alive when they share a scene - lots! It is done so well that you find yourself willing them to get together...Ignore the negative comments - if you are feeling a bit blue, watch this flick, you will feel so much better. If you are already happy, then you will be euphoric.(PS: I am 33, Male, from the UK and a hopeless romantic still searching for his Princess...)\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":169}],"source":["cleaned_text"]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}