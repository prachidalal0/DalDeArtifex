{"cells":[{"cell_type":"markdown","id":"b84121b7-f061-4e3b-8e4d-95e94859115b","metadata":{"id":"b84121b7-f061-4e3b-8e4d-95e94859115b"},"source":["# <font color = 'indianred'>**Understanding Inputs/Outputs for BERT** </font>\n","\n","**Objective:**\n","\n","We will\n","\n","**Key Changes and Innovations from prebious Notebook:**\n","\n","1. **Advanced Tokenization Techniques:**\n","   - We delve deeper into advanced tokenization, using BERT's pre-trained tokenizer. We need to use this tokenizer so that the inputs are compatible with the pre-trained model.\n","2. **Introduction of Pre-Trained BERT Model:**\n","   - Unlike the custom model in the first notebook, we now utilize a pre-trained BERT model. This approach allows us to benefit from a model already trained on a vast corpus of text, bringing in rich contextual embeddings. By using a pre-trained model, we can get better accuracy compared to training a model from scratch.\n","\n","\n","\n","<img src =\"https://drive.google.com/uc?export=view&id=1IQgmPzHxbVw3a7EfwWfIGtiPAZY7mAMD\" width =800>\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","id":"fj41POPjrtzA","metadata":{"id":"fj41POPjrtzA"},"source":["# <font color = 'indianred'> **1. Setting up the Environment** </font>\n","\n"]},{"cell_type":"code","execution_count":null,"id":"0d54de23-80e9-4e1b-816c-6f6916c92836","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-12-23T23:12:55.528562Z","iopub.status.busy":"2022-12-23T23:12:55.528020Z","iopub.status.idle":"2022-12-23T23:12:55.539182Z","shell.execute_reply":"2022-12-23T23:12:55.538417Z","shell.execute_reply.started":"2022-12-23T23:12:55.528532Z"},"executionInfo":{"elapsed":46531,"status":"ok","timestamp":1711940422662,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"0d54de23-80e9-4e1b-816c-6f6916c92836","outputId":"26bc9bc2-7b4f-4362-e642-dd50c829e570","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["from pathlib import Path\n","if 'google.colab' in str(get_ipython()):\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    !pip install datasets transformers -U -qq\n"]},{"cell_type":"markdown","id":"1Kcz9H4QsW6_","metadata":{"id":"1Kcz9H4QsW6_"},"source":["<font color = 'indianred'> *Load Libraries* </font>"]},{"cell_type":"code","execution_count":null,"id":"yC6lJrxYvxeF","metadata":{"id":"yC6lJrxYvxeF"},"outputs":[],"source":["# standard data science librraies for data handling and v isualization\n","import torch.nn as nn\n","import torch\n","import matplotlib.pyplot as plt\n","\n","\n","# New libraries introduced in this notebook\n","from datasets import Dataset\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoModel\n","from transformers import AutoConfig\n","from transformers import PreTrainedModel, PretrainedConfig\n","from transformers import DataCollatorWithPadding\n","from transformers.modeling_outputs import SequenceClassifierOutput\n"]},{"cell_type":"markdown","source":["# <font color = 'indianred'>**2. Understanding Tokenization**\n","\n","Tokenization is the process of dividing a sequence of text into smaller parts called tokens.\n","\n","*Why we need Tokenization?*\n","\n","In natural language processing (NLP), this is an essential preprocessing step. NLP models do not accept raw strings directly; instead, these <font color='indianred'><b>models expect the input text to be tokenized and converted into numerical vectors</b>.</font>\n","\n","*Tokenization Approaches*\n","\n","Let's explore the three common tokenization strategies - word, character, and subword - and examine their advantages and disadvantages.\n","\n","1. *Word Tokenization*: It breaks the text into individual words. This is the most intuitive way to split text but can suffer from Out-of-Vocabulary (OOV) Words (Words not found in the training vocabulary ) issues and rigidity in handling variations in words.\n","\n","2. *Character Tokenization*: It breaks the text into individual characters, offering flexibility to represent any string. This can help to deal with OOV, rare words and mis-spellings. However, we lose semantic information and NLP model will need to learn linguistic structures like words from the data.\n","\n","3. *Subword Tokenization*: Subword tokenization represents a middle ground between word and character tokenization, dividing text into units that may include whole words or character n-grams. This approach aims to harness the advantages of both character and word tokenization.\n","   - Rare Words Handling: Breaks down rare, complex, and misspelled words into smaller units, facilitating easier interpretation by the model.\n","   - Frequent Words Preservation: Retains frequently used words as individual entities, keeping input length manageable.\n","\n","\n","*What is a pre-trained Tokenizer?*\n","\n","The pre-trained tokenizer is the tokenizer (for a specific model) trained on a large corpus of text and has learned a set of rules for breaking down words and sentences into tokens. Using these rules, it has created a fixed vocabulary. This vocabulary is a mapping between unique tokens (words, subwords, or characters depending on the tokenizer's method) and unique IDs.\n","\n","The tokenizer uses this fixed vocabulary to tokenize the new data that we pass. The tokenizer follows these steps to create subtokens for the new dataset:\n","Longest Match Rule: The tokenizer looks for the longest matching subword token in its vocabulary. If the whole word is present in the vocabulary, it is not split, and the tokenizer takes the entire word as one token.\n","\n","*Subword Splitting*: If the word is not in the vocabulary or only a part of it is present, the tokenizer breaks it down into subword tokens. It selects the longest matching subword token from the beginning of the word and assigns it as the first token. Then, it looks for the longest matching subword token from the remaining part of the word and assigns it as the next token. This process continues until the entire word is covered by subword tokens.\n","\n","*Why we need a pre-trained Tokenizer?*\n","\n","We aim to fine-tune the pre-trained mode (BERT)l. For this reason, employing the same tokenizer used during BERT's original training is critical to fully leverage the model's capabilities. BERT's training involved a specific tokenization method, the WordPiece Tokenization, which is integral to how the model understands and processes language. By using this tokenizer, we ensure compatibility with the pre-trained embeddings in BERT's embedding layer. This alignment is crucial as it maintains the contextual integrity and consistency of input representation, which BERT relies on for its performance. Deviating from this tokenizer could lead to a mismatch between how the input text is represented and how BERT was trained to interpret text, resulting in decreased accuracy and efficiency of the model. Therefore, to harness BERT's full potential in various NLP tasks, it's vital to use the tokenizer it was trained with.\n"],"metadata":{"id":"lBWpxDbOAE_P"},"id":"lBWpxDbOAE_P"},{"cell_type":"markdown","id":"c7b02690-50f7-49bf-aab4-85dbbb21d182","metadata":{"execution":{"iopub.execute_input":"2022-12-20T11:21:48.647438Z","iopub.status.busy":"2022-12-20T11:21:48.646659Z","iopub.status.idle":"2022-12-20T11:21:48.690232Z","shell.execute_reply":"2022-12-20T11:21:48.689729Z","shell.execute_reply.started":"2022-12-20T11:21:48.647414Z"},"id":"c7b02690-50f7-49bf-aab4-85dbbb21d182","tags":[]},"source":["# <font color = 'indianred'>**3. Load pre-trained Tokenizer**</font>\n","\n","\n","<img src =\"https://drive.google.com/uc?export=view&id=1qH2bkB0or2_KAf84O5y5Y26A1W6ZmWRj\" width =800>\n","\n","image source: https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n","\n","In our next step, we will download a pre-trained tokenizer specifically designed to work with BERT. This tokenizer will handle the conversion of our text into a format that BERT can understand."]},{"cell_type":"code","execution_count":null,"id":"hzUiqZR6hI4o","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":274,"referenced_widgets":["c26e1015923d43b79abb9dcc334cfe54","6f67b9aa8a5f47aaac48052dacef5d56","c3e0f38f52d04b17982f4a5e49db8ec2","4d6c877499f345f08335b2ac129863e8","c208cc7cf2d74405b3d0b30bc528ace1","ec362c35022d48248b7451289e42b0fe","1276eaeb40c7428290746253c8a17bab","9d300f991a624e56887db068ea6480a9","59962aac00ee4f369fb889329938b2f5","7504c15e4d5d47f2a9a680e5a95a6e44","b2293182e3204e0a813bfff147e932be","210cee2e15ac4dfabe6fb03223ebaf31","fa3c31e163a54f28a53cdfc29b946506","91204c102de640c1ac15f59d2f5435ac","1886b7ed8200483eb912452d9bce8450","6e3ceef1cc8149f3b4dd793c56087674","2633c6a298b64f32ad354f27d61ffa2b","bf9a2f636ede451d9f2000af51ce575f","a2d47f7fbaf5437eab0cb48eff7803c9","f7a4528acced4051b524e7ba94ac3047","5f9e9b315ae34d65bdb1a145d2d88710","e132e08183264b0ca96f75d7bf156e95","a41ca147bab04a2eb189017dfa38d011","607b5070b66a42618c613d5ee938a307","2b94d0ff85424b96a3707345b84415df","24b0649097214854ab1fd00b96373469","2ede05b012714503aed29e6e5568555b","3ab56b1922c24d47bcbc8743e220966a","f82f7aca8df74f39857679e2cbf7d65a","5339b5231f0741e6b52b8c51a7d73748","26b4aae2c45740ccbae9fc84c9976348","9b81d8ce5344486cb8b1e57ff975d549","fa99893c8081428bbf6f584732ed584c","8f1beb529ed14aa5a63581704428dd46","9ba8c281a8724e02bb3c88eeb9b6496d","2a2e9a94182e47f78b07c4b13aa5e5c5","5d8785f4173949379d1e80126f10938c","58f2fd3d59ef4496ac98d8d6fb7c6635","7d0e9e81ae2742d19189850f3b5d5bd2","94e7c73e8346445ab2b6cc2d73a99859","562175e32db94700a16edfdf336544f8","4952d884f30a4574a75d6f4496c920c8","e3dbaaef3501426ca942c32f8a0d0524","1a755ebce4594de5b28fbda0d5941b9e"]},"execution":{"iopub.execute_input":"2022-12-23T23:14:07.903271Z","iopub.status.busy":"2022-12-23T23:14:07.902958Z","iopub.status.idle":"2022-12-23T23:14:07.955695Z","shell.execute_reply":"2022-12-23T23:14:07.955154Z","shell.execute_reply.started":"2022-12-23T23:14:07.903251Z"},"executionInfo":{"elapsed":4885,"status":"ok","timestamp":1711940441811,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"hzUiqZR6hI4o","outputId":"7d65122e-b6be-4105-8fd7-63021e94e2fb","tags":[]},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c26e1015923d43b79abb9dcc334cfe54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"210cee2e15ac4dfabe6fb03223ebaf31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a41ca147bab04a2eb189017dfa38d011"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f1beb529ed14aa5a63581704428dd46"}},"metadata":{}}],"source":["checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)"]},{"cell_type":"markdown","id":"vcHnNm9HhOmr","metadata":{"id":"vcHnNm9HhOmr"},"source":["- `checkpoint = \"bert-base-uncased\"` specifies the pre-trained model we want to use. BERT has various versions, and \"bert-base-uncased\" refers to the base version trained on uncased English text.\n","\n","-  `tokenizer = AutoTokenizer.from_pretrained(checkpoint)` downloads and initializes the tokenizer for the specified checkpoint. This method takes care of downloading the required files and setting up the tokenizer with the correct configurations.\n","\n","\n"]},{"cell_type":"markdown","id":"JDp4gu0uYZ1d","metadata":{"id":"JDp4gu0uYZ1d"},"source":["<font color = 'indianred'>*Understanding pre-trained Tokenizer*\n","\n","We will now understand how the tokenizer work by feeding one simple example."]},{"cell_type":"code","execution_count":null,"id":"Pdd63KwJ1wKV","metadata":{"id":"Pdd63KwJ1wKV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711940441811,"user_tz":300,"elapsed":60,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"7a5443b1-e675-4d1a-b088-cd4c2bd08f9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset({\n","    features: ['text', 'label'],\n","    num_rows: 2\n","})\n"]}],"source":["# Define the text and labels\n","data = {\n","    'text': [\n","        \"Tokenization is the process of splitting sequence to tokens\",\n","        \"I like BUAN6342\"\n","    ],\n","    'label': [0, 1]\n","}\n","\n","# Create a Hugging Face dataset\n","dataset = Dataset.from_dict(data)\n","\n","# Display the dataset\n","print(dataset)\n"]},{"cell_type":"code","execution_count":null,"id":"pe6_RHcPcs02","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55,"status":"ok","timestamp":1711940441812,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"pe6_RHcPcs02","outputId":"dc359e46-1127-4c2c-c9b0-a84c77ac3ac8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Pretrained tokenizer vocab size 30522\n"]}],"source":["# get the vocab size\n","print(f'Pretrained tokenizer vocab size {tokenizer.vocab_size}')\n"]},{"cell_type":"markdown","id":"XCz50jkrcukz","metadata":{"id":"XCz50jkrcukz"},"source":["- <font color = 'indianblue'>The vocab size for the tokenizer for bert-base-uncased model is 30522."]},{"cell_type":"code","execution_count":null,"id":"3SfjDfSr10tU","metadata":{"id":"3SfjDfSr10tU"},"outputs":[],"source":["encoded_text = [tokenizer(text, truncation=True, padding=True, return_tensors='pt') for text in dataset['text']]"]},{"cell_type":"markdown","id":"42e8ffc9","metadata":{"id":"42e8ffc9"},"source":["Let us understand the arguments:\n","\n","1. `padding = True`: This argument tells the tokenizer to add padding to the input text. BERT processes inputs in batches, and all sequences in a batch should have the same length. Padding adds special [PAD] tokens to make all sentences in the batch the same length.\n","\n","2. `truncation = True`: This argument instructs the tokenizer to truncate the input text to a maximum length that BERT can handle. BERT has a maximum input length, and if a sentence is longer than that, it will be truncated. If you do not set truncation=True and you have a sequence length greater than teh model can take , the tokenizer will raise an error.\n","\n","3. `return_tensors = 'pt'`: This argument tells the tokenizer to return the output in PyTorch tensor format. PyTorch tensors are data structures used for efficient numerical computations.\n","\n","Now let us look at the output of the tokenizer, and try to understand the output\n"]},{"cell_type":"code","execution_count":null,"id":"7e2a2e62","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51,"status":"ok","timestamp":1711940441813,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"7e2a2e62","outputId":"93cb8ae2-03c6-49ab-a185-4560da73dd69"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'input_ids': tensor([[  101, 19204,  3989,  2003,  1996,  2832,  1997, 14541,  5537,  2000,\n","          19204,  2015,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n"," {'input_ids': tensor([[  101,  1045,  2066, 20934,  2319,  2575, 22022,  2475,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}]"]},"metadata":{},"execution_count":7}],"source":["encoded_text\n"]},{"cell_type":"markdown","id":"pR7HIeu-Ozlf","metadata":{"id":"pR7HIeu-Ozlf"},"source":["- **input_ids**\n","\n","    Input Ids are numerical identifiers assigned to each token from the input text. The tokenizer map each word or sub-word into a unique ID from its predefined vocabulary. It tries to match the whole word first. If unsuccessful, it splits the word into sub-words until each piece can be matched in the vocabulary, each represented by its corresponding ID. If a piece can't be found, a special [UNK] token is used. This process creates the `input_ids` list, a numerical representation of the input text.\n","\n","- **'token_type_ids'**\n","\n","    The token type IDs are used when BERT is fed with pairs of sentences or inputs with distinct segments (e.g., Question-Answer pairs). For single-sentence tasks, all token type IDs are typically set to 0. For tasks that require two separate segments of text, such as Question-Answer tasks, the token type IDs distinguish the segments. The first segment (e.g., the question) is assigned a token type ID of 0, and the second segment (e.g., the answer) is assigned a token type ID of 1.\n","\n","- **attention_mask**\n","\n","    The attention mask is a binary tensor that has the same length as the tokenized input sequence. It is used to instruct the BERT model about which tokens should be used and which ones should be ignored during processing. The attention mask is essential when handling sequences with varying lengths. It works by setting a value of 1 for the tokens that should be used to and 0 for the tokens that should be ignored (typically the padded tokens). This way, the model knows which tokens are actual input and which ones are just padding.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"jZ8xwmkfRHVA","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47,"status":"ok","timestamp":1711940441813,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"jZ8xwmkfRHVA","outputId":"c9a5059d-88da-4504-adc9-23e0fced325a"},"outputs":[{"output_type":"stream","name":"stdout","text":["First sentence tokens: ['[CLS]', 'token', '##ization', 'is', 'the', 'process', 'of', 'splitting', 'sequence', 'to', 'token', '##s', '[SEP]']\n","Second sentence tokens: ['[CLS]', 'i', 'like', 'bu', '##an', '##6', '##34', '##2', '[SEP]']\n"]}],"source":["# Extracting the tensor and converting it to a list for the first sentence\n","tokens_first_sentence = tokenizer.convert_ids_to_tokens(encoded_text[0]['input_ids'][0].tolist())\n","\n","# Extracting the tensor and converting it to a list for the second sentence\n","tokens_second_sentence = tokenizer.convert_ids_to_tokens(encoded_text[1]['input_ids'][0].tolist())\n","\n","# Now you should be able to print or process the tokens\n","print(\"First sentence tokens:\", tokens_first_sentence)\n","print(\"Second sentence tokens:\", tokens_second_sentence)"]},{"cell_type":"markdown","id":"z9OTyPHeRaLJ","metadata":{"id":"z9OTyPHeRaLJ"},"source":["\n","Three things are worth noting in this tokenized sequence:\n","1. **Special Tokens**: We can observe special `[CLS]`, and `[SEP]` tokens added at the beginning and end of the sequence. We can also observe anpther token and `[PAD]` is added at the end of shorter sequence (example).\n","    - **[CLS] Token**: token stands for \"classification\" and is used at the beginning of each input sequence. It is essential for tasks like text classification, where BERT learns to encode the entire sequence's information into the representation of the [CLS] token.\n","\n","    - **[SEP] Token**: This token stands for \"separator\" and is used to separate two different sequences in the input. When processing multiple sequences, BERT takes this separator token to distinguish between the end of one sequence and the start of another.\n","\n","    - **[PAD] Token**: This token stands for \"padding\" and is used to make input sequences of equal length. BERT processes inputs in batches, and all sequences within a batch need to have the same length. If a sequence is shorter than the maximum length in the batch, it is padded with [PAD] tokens to match the length. In our example, the second sentence is smaller and hence the tokenizer add [PAD] tokens to the second sentence.\n","\n","    Now we can also see that why we have four zeros in the attention_ask of the second sentence. We are telling model to not pay attention to these tokens ([PAD] tokens) and ignore these tokens.\n","\n","2. **Lowercasing**: All the tokens have been converted to lowercase. This is a feature of this particular BERT checkpoint (**we have used -uncased version**), which helps standardize the text and ensures that the model treats different cases of the same word equally.\n","\n","3. **Subword Tokens**: Some words like \"tokenizing\" and \"tokens\" have been split into multiple tokens, indicated by the presence of the `##` prefix. This happens because BERT breaks down less common or longer words into smaller subword tokens to handle them effectively. The `##` prefix indicates that these tokens should be merged with the previous token when converting the tokens back to a string.\n","\n","The AutoTokenizer class offers a convenient method called convert_tokens_to_string() that allows us to revert the tokens back to their original textual representation. So, let's utilize this method to convert our tokens into string representtaion.\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"AgvkuoiI_6lH","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1711940441814,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"AgvkuoiI_6lH","outputId":"af3aba68-6e00-4030-87d5-9bb2a6559827"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'[CLS] tokenization is the process of splitting sequence to tokens [SEP]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}],"source":["tokenizer.convert_tokens_to_string(tokens_first_sentence)\n"]},{"cell_type":"code","execution_count":null,"id":"17bf645f","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1711940441814,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"17bf645f","outputId":"1d217bab-54c5-4ebb-d2ab-c98a8beed4b0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'[CLS] i like buan6342 [SEP]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}],"source":["tokenizer.convert_tokens_to_string(tokens_second_sentence)\n"]},{"cell_type":"code","execution_count":null,"id":"4uScYP-Kbfz5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1711940441815,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"4uScYP-Kbfz5","outputId":"b2a4c66a-6d17-42e7-a1bd-3af5489bb799"},"outputs":[{"output_type":"stream","name":"stdout","text":["['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]'] [100, 102, 0, 101, 103]\n"]}],"source":["special_tokens = tokenizer.all_special_tokens\n","special_tokens_ids = tokenizer.all_special_ids\n","print(special_tokens, special_tokens_ids)\n"]},{"cell_type":"markdown","id":"55b19f0b","metadata":{"id":"55b19f0b"},"source":["We have already explained '[SEP]', '[PAD]', '[CLS]' tokens. Now let us look at the other two special tokens.\n","\n","- [UNK] Token: This token stands for \"unknown\" and is used to represent words that are not present in the model's vocabulary. During tokenization, if a word in the input sequence is not found in the pre-trained vocabulary, it is replaced with the [UNK] token.\n","\n","- [MASK] Token: This token is used during pretraining BERT. It is used to mask certain words in the input sequence randomly. During training, BERT tries to predict these masked words based on the context provided by the other words in the sequence. This pretraining process helps BERT capture bidirectional context and understand language more effectively."]},{"cell_type":"markdown","id":"2b4ce3b9-904f-42bd-9c3f-163328f47051","metadata":{"execution":{"iopub.execute_input":"2022-12-20T11:22:33.679936Z","iopub.status.busy":"2022-12-20T11:22:33.679764Z","iopub.status.idle":"2022-12-20T11:22:33.723366Z","shell.execute_reply":"2022-12-20T11:22:33.722847Z","shell.execute_reply.started":"2022-12-20T11:22:33.679918Z"},"id":"2b4ce3b9-904f-42bd-9c3f-163328f47051"},"source":["# <font color = 'indianred'> **4. Create function for Tokenizer**\n","\n","In the previous section, we understood how the tokenization work. We will now create a function for tokenization and then apply the function to training and validation splits to generate tokenized dataset.\n","\n","**Change from previous section**: When exploring tokenization, we utilized options :`padding=True`, and `return_tensors='pt'`. However, in our tokenization function creation, we'll omit these arguments.\n","\n","**Reason for the change**: This approach is deliberate: padding and conversion to tensors are more efficiently managed not at the dataset level, but rather at the batch level during training. Padding each sequence in the dataset to a uniform length can result in unnecessary and excessive padding (based on length of the longest sequence in the whole dataset), especially if there's a significant variation in sequence lengths. Instead, these steps are handled by a data collator (collate function), a concept we've touched upon in previous notebooks. The data collator dynamically adjusts padding for each batch, ensuring it's based on the longest sequence within that specific batch. This method is not only resource-efficient but also optimizes training by reducing the amount of redundant data the model processes in each training step. We have to do padding to create tensors at the batch level. Hence, this step is also done at the batch level."]},{"cell_type":"code","execution_count":null,"id":"8bAD2dsXhvAA","metadata":{"execution":{"iopub.execute_input":"2022-12-23T23:14:12.134855Z","iopub.status.busy":"2022-12-23T23:14:12.134613Z","iopub.status.idle":"2022-12-23T23:14:12.186484Z","shell.execute_reply":"2022-12-23T23:14:12.185866Z","shell.execute_reply.started":"2022-12-23T23:14:12.134828Z"},"id":"8bAD2dsXhvAA","tags":[]},"outputs":[],"source":["def tokenize_fn(batch):\n","    return tokenizer(text = batch[\"text\"], truncation=True)"]},{"cell_type":"markdown","id":"1f09c889-c3fd-4b12-a544-0772594a3899","metadata":{"id":"1f09c889-c3fd-4b12-a544-0772594a3899"},"source":["<font color = 'indianred'> *Use map function to apply tokenization to all splits*"]},{"cell_type":"code","execution_count":null,"id":"gZ2Xf36QQizK","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["907050629ce74693bfd9098050bdafae","adf726ac9dcf446aacc827e46295bbd2","b6bc063f53674619974015315057bc29","9fe1dd4ce5f64eb287f86f1aeaf61f6c","f4ffb6c4780a4ceca1e3eda046cc6ccf","2e58d861d9434783a8d07ca158825faf","623c3f02122549c1ad4f7f71f7840ace","cb97b1a5f93f4a6a9cdee2fa39747cc3","95d2bdab0cf24fdd9dce247502288bb2","004e6e08de6c4a5e82f65c17e20a8d2a","8ccfe46927c1480ab3f1e861623973c7"]},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1711940441815,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"gZ2Xf36QQizK","outputId":"e0a29b81-6ad5-43ff-bfa3-66a7b80bfaa9"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/2 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"907050629ce74693bfd9098050bdafae"}},"metadata":{}}],"source":["tokenized_dataset = dataset.map(tokenize_fn, batched=True,)"]},{"cell_type":"markdown","id":"764159f9","metadata":{"id":"764159f9"},"source":["**Code Explanation**:\n","\n","- The code is taking a dataset (train_val_small), applying a tokenization function (tokenize_fn) to each batch of data, and then storing the tokenized results in a new dataset (tokenized_dataset). The resulting tokenized_dataset will have the same number of elements as the original dataset, but each element will now be in a tokenized form suitable for a transformer model.\n","- The default batch size is 1000.\n","- Using batched=True in the datasets library streamlines data processing by taking advantage of vectorized operations, leading to faster execution. This approach reduces the overhead from individual function calls and benefits from Hugging Face's tokenizers, which are optimized for batch processing. Additionally, batching can enhance memory use and improve I/O efficiency, especially for large datasets read from disk.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"N2Hs7UT9iLVt","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-12-23T23:14:16.920177Z","iopub.status.busy":"2022-12-23T23:14:16.919821Z","iopub.status.idle":"2022-12-23T23:14:16.955337Z","shell.execute_reply":"2022-12-23T23:14:16.954693Z","shell.execute_reply.started":"2022-12-23T23:14:16.920164Z"},"executionInfo":{"elapsed":717,"status":"ok","timestamp":1711940442509,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"N2Hs7UT9iLVt","outputId":"f9619d4d-72f2-48a1-9b73-ca2c6a5cecbb","tags":[]},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n","    num_rows: 2\n","})"]},"metadata":{},"execution_count":14}],"source":["tokenized_dataset\n"]},{"cell_type":"markdown","id":"859f3731","metadata":{"id":"859f3731"},"source":["We can see that tokenization step has added three new columns `('input_ids', 'token_type_ids', 'attention_mask')` to the dataset.\n","We no longer need the column `text`, hence we ill remove it. Further, we will set the dataset format to 'torch' ensuring that the tokenized dataset is converted into PyTorch tensors, making it directly compatible with PyTorch models and training routines."]},{"cell_type":"code","execution_count":null,"id":"cad3cc7e","metadata":{"id":"cad3cc7e"},"outputs":[],"source":["tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"]},{"cell_type":"code","execution_count":null,"id":"57ea9e81","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"57ea9e81","executionInfo":{"status":"ok","timestamp":1711940442510,"user_tz":300,"elapsed":52,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"cdb508ef-eacf-4124-9610-999d547bca34"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'label': tensor(0),\n"," 'input_ids': tensor([  101, 19204,  3989,  2003,  1996,  2832,  1997, 14541,  5537,  2000,\n","         19204,  2015,   102]),\n"," 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}"]},"metadata":{},"execution_count":16}],"source":["tokenized_dataset[0]"]},{"cell_type":"code","execution_count":null,"id":"01a51911","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"01a51911","executionInfo":{"status":"ok","timestamp":1711940442510,"user_tz":300,"elapsed":49,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"23a1d88f-5cb2-4534-bd1d-97d32e109ecc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'label': tensor(1),\n"," 'input_ids': tensor([  101,  1045,  2066, 20934,  2319,  2575, 22022,  2475,   102]),\n"," 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1])}"]},"metadata":{},"execution_count":17}],"source":["tokenized_dataset[1]"]},{"cell_type":"code","execution_count":null,"id":"DT9UYn4VQ9--","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1711940442510,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"DT9UYn4VQ9--","outputId":"ad60f2fc-5fbb-4b65-ff3d-64ae79927427"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n","    num_rows: 2\n","})"]},"metadata":{},"execution_count":18}],"source":["tokenized_dataset"]},{"cell_type":"markdown","id":"c5d6b02f","metadata":{"id":"c5d6b02f"},"source":["Remember, set_format doesn't alter the dataset's storage or remove columns; it only affects how data is retrieved. The dataset retains its comprehensive structure, allowing you to change the format dynamically as needed without losing any data."]},{"cell_type":"code","execution_count":null,"id":"BchXa9V-Q-nq","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1711940442511,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"BchXa9V-Q-nq","outputId":"264ab4c2-038c-4686-dba7-6294895e6423"},"outputs":[{"output_type":"stream","name":"stdout","text":["13\n","9\n"]}],"source":["print(len(tokenized_dataset[\"input_ids\"][0]))\n","print(len(tokenized_dataset[\"input_ids\"][1]))"]},{"cell_type":"markdown","id":"EtbTpHsRRJqj","metadata":{"id":"EtbTpHsRRJqj"},"source":["The varying lengths in the dataset indicate that padding has not been applied yet. Instead of padding the entire dataset, we prefer processing small batches during training. Padding is done selectively for each batch based on the maximum length in the batch. We will discuss this in more detail in a later section of this notebook."]},{"cell_type":"code","execution_count":null,"id":"49a7ba50","metadata":{"id":"49a7ba50"},"outputs":[],"source":["data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2uPOI_Z4_N_b"},"outputs":[],"source":["data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"],"id":"2uPOI_Z4_N_b"},{"cell_type":"markdown","id":"a3fbf63a","metadata":{"id":"a3fbf63a"},"source":["The `DataCollatorWithPadding` function is used to dynamically pad the input data to the maximum length in a batch of inputs. This is essential when batching together sequences of different lengths, ensuring that each sequence in the batch has the same length by padding the shorter ones. Here's how the `DataCollatorWithPadding` function has processed the data:\n","\n","**Padding Input IDs (`input_ids`):**\n","   - The `input_ids` are sequences of integers that represent the tokenized version of the text data.\n","   - The `DataCollatorWithPadding` ensures that all `input_ids` in a batch are of the same length by adding padding tokens (usually represented by the ID `0`) to the sequences that are shorter than the longest sequence in the batch.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wFEDVJ40_N_b"},"outputs":[],"source":["features = [tokenized_dataset[i] for i in range(2)]"],"id":"wFEDVJ40_N_b"},{"cell_type":"code","execution_count":null,"id":"82a541f4","metadata":{"id":"82a541f4"},"outputs":[],"source":["features = [tokenized_dataset[i] for i in range(2)]\n","model_input = data_collator(features)"]},{"cell_type":"code","execution_count":null,"id":"8cc35f5e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8cc35f5e","executionInfo":{"status":"ok","timestamp":1711940442512,"user_tz":300,"elapsed":34,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"5b66f112-5037-4d98-b498-63aac4d4523d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[  101, 19204,  3989,  2003,  1996,  2832,  1997, 14541,  5537,  2000,\n","         19204,  2015,   102],\n","        [  101,  1045,  2066, 20934,  2319,  2575, 22022,  2475,   102,     0,\n","             0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]), 'labels': tensor([0, 1])}"]},"metadata":{},"execution_count":24}],"source":["model_input"]},{"cell_type":"markdown","id":"f2c39d1f","metadata":{"id":"f2c39d1f"},"source":["The output displays two samples of tokenized data from a dataset. Each sample is represented as a dictionary containing the following key-value pairs:\n","\n","1. **`label`**: This is the label or target associated with the text data. It is represented as a tensor.\n","\n","2. **`input_ids`**: This is a tensor containing a sequence of integers. Each integer represents a unique token (word or subword) from the text, as encoded by the tokenizer. This sequence is what the model will take as input. The sequence length and the specific token IDs will vary based on the text content and the tokenizer's vocabulary.\n","\n","3. **`attention_mask`**: This tensor indicates which tokens in the `input_ids` should be paid attention to by the model. **A value of `1` means that the corresponding token is a part of the input and should be considered by the model, while a value of `0` would indicate a padding token that should be ignored. **"]},{"cell_type":"markdown","id":"cc2a39fc-6d87-43ca-8a6e-7129bd8214f1","metadata":{"id":"cc2a39fc-6d87-43ca-8a6e-7129bd8214f1"},"source":["#  <font color = 'indianred'> **5 Understanding Pre-trained BERT model**\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"58babf85","metadata":{"executionInfo":{"elapsed":5455,"status":"ok","timestamp":1711940447948,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"58babf85","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["b5f20d7c16584987869793f56a89442a","8ab661b7ca464e858312dc802e5638dd","aa4b82dfd1b744ffbd03ac81d6726905","9154120e47c54feabfdb390ab0ad274f","28ef5aac3d7742b98c8119b1374fc330","bf18eb96b55b414fb53a149acd670cc4","a3458200346940deabd73d2ef6907843","5f7fac9835db47c38a3a067b13962b1c","ccbe71d5e5e64eec85ab810c49a1c0ee","b497838a956b4b5c86667668c3c5a588","d06be31d2a214b9c997b79ce8b64c443"]},"outputId":"7e8fcf9f-f93b-4ffb-d89c-83b73ba279a5"},"outputs":[{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5f20d7c16584987869793f56a89442a"}},"metadata":{}}],"source":["model = AutoModel.from_pretrained(checkpoint)"]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uv-PfnSXH3pU","executionInfo":{"status":"ok","timestamp":1711940447949,"user_tz":300,"elapsed":39,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"921c2247-5927-4f68-d588-365af7deab6f"},"id":"Uv-PfnSXH3pU","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0-11): 12 x BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","id":"v8bZfRe0wdEX","metadata":{"id":"v8bZfRe0wdEX"},"source":["The code snippet involves using the transformers library from Hugging Face to load a pretrained model tailored for sequence classification tasks.Here's a brief explanation:\n","\n","- **Loading the Pretrained Model:**\n","The AutoModel.from_pretrained() function is used to load a pretrained model with one argument:\n","\n","  - **checkpoint:** This specifies which pretrained model to load. Since the comment mentions that it's the \"same checkpoint as used for the tokenizer,\" it suggests that the model and the tokenizer are both sourced from the same original pretrained model, ensuring compatibility.\n"]},{"cell_type":"markdown","source":["<img src =\"https://drive.google.com/uc?export=view&id=1qKP3ilQHoSPr1SDMfzB4hMOMEee6jx2F\" width =800>\n","\n","<img src =\"https://drive.google.com/uc?export=view&id=1qM3jUSXKKbcEGiUVN-hIFJH6jSUiwpOf\" width =800>"],"metadata":{"id":"ELf5CamDFtNs"},"id":"ELf5CamDFtNs"},{"cell_type":"code","execution_count":null,"id":"9818b8a7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9818b8a7","executionInfo":{"status":"ok","timestamp":1711940447949,"user_tz":300,"elapsed":34,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"3a0ded85-5baa-479d-8673-5d19a2303669"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[  101, 19204,  3989,  2003,  1996,  2832,  1997, 14541,  5537,  2000,\n","         19204,  2015,   102],\n","        [  101,  1045,  2066, 20934,  2319,  2575, 22022,  2475,   102,     0,\n","             0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]), 'labels': tensor([0, 1])}"]},"metadata":{},"execution_count":27}],"source":["model_input"]},{"cell_type":"code","execution_count":null,"id":"Kp85pxcaanZr","metadata":{"id":"Kp85pxcaanZr"},"outputs":[],"source":["# model output\n","model=model.to(device=0)\n","model_input= model_input.to(device=0)\n","model.train()\n","model_output = model(model_input['input_ids'], model_input['attention_mask'])"]},{"cell_type":"code","execution_count":null,"id":"Ea8ElbBcan8v","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1711940452901,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"Ea8ElbBcan8v","outputId":"7874ebe8-21f2-4d1c-de3f-baf2c6ae7012"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['last_hidden_state', 'pooler_output'])"]},"metadata":{},"execution_count":29}],"source":["# keys in model output\n","model_output.keys()"]},{"cell_type":"code","execution_count":null,"id":"d36d314b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d36d314b","executionInfo":{"status":"ok","timestamp":1711940452902,"user_tz":300,"elapsed":15,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"92f39f6d-97c3-496b-d8d8-9b17b35c6ab1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 13, 768])"]},"metadata":{},"execution_count":30}],"source":["# all the tokens of the input sequence\n","model_output.last_hidden_state.shape"]},{"cell_type":"code","execution_count":null,"id":"lgHbL2aibE60","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1711940452902,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"lgHbL2aibE60","outputId":"e07c98f0-b603-436d-edc7-2a0826ced125"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 768])"]},"metadata":{},"execution_count":31}],"source":["# cls token after the\n","model_output.pooler_output.shape\n"]},{"cell_type":"markdown","source":["#  <font color = 'indianred'> **5 Custom classification head for BERT model**"],"metadata":{"id":"ODOeCIteqJpO"},"id":"ODOeCIteqJpO"},{"cell_type":"code","execution_count":null,"id":"7affd319","metadata":{"id":"7affd319"},"outputs":[],"source":["class CustomConfig(PretrainedConfig):\n","  def __init__(self, bert_model,ff_output_dim, n_classes, ff_dropout, cls_only=False, average_all=False, pooler=True, **kwargs):\n","        super().__init__()\n","        self.ff_input_dim = bert_model.config.hidden_size\n","        self.ff_output_dim = ff_output_dim\n","        self.n_classes = n_classes\n","        self.encoder = bert_model\n","        self.ff_dropout = ff_dropout\n","        self.cls_only = cls_only\n","        self.average_all = average_all\n","        self.pooler = pooler"]},{"cell_type":"code","execution_count":null,"id":"ee8a6870","metadata":{"id":"ee8a6870"},"outputs":[],"source":["class BERTClassifier(PreTrainedModel):\n","    config_class = CustomConfig\n","\n","    def __init__(self, config):\n","\n","        super().__init__(config)\n","        # Add assertion to ensure only one of cls_only, average_all, or pooler is True\n","        assert (\n","            sum([config.cls_only, config.average_all, config.pooler]) == 1\n","        ), \"Only one of 'cls_only', 'average_all', or 'pooler' can be True\"\n","\n","        self.classification_head = nn.Sequential(\n","            nn.Linear(config.ff_input_dim, config.ff_output_dim),\n","            nn.ReLU(),\n","            nn.Dropout(config.ff_dropout),\n","            nn.Linear(config.ff_output_dim, config.n_classes),\n","        )\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        outputs = self.config.encoder(\n","            input_ids, attention_mask=attention_mask)\n","\n","        if self.config.cls_only:\n","            output = outputs.last_hidden_state[:, 0, :]\n","        elif self.config.average_all:\n","            last_hidden_state = outputs.last_hidden_state\n","            output = torch.mean(last_hidden_state, dim=1)\n","        elif self.config.pooler:\n","            output = outputs.pooler_output\n","\n","\n","        logits = self.classification_head(output)\n","        loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            loss = loss_fct(logits.view(-1, self.config.n_classes), labels.view(-1))\n","\n","        return SequenceClassifierOutput(\n","            loss=loss,\n","            logits=logits\n","        )"]},{"cell_type":"code","execution_count":null,"id":"2e155e79","metadata":{"id":"2e155e79"},"outputs":[],"source":["my_config = CustomConfig(\n","    bert_model=model,\n","    ff_output_dim=256,\n","    n_classes=2,\n","    ff_dropout=0.1,\n","    pooler=True,\n","    cls_only=False,\n","    average_all=False,\n",")\n","\n","model_pytorch = BERTClassifier(my_config)\n","\n","model_pytorch=model_pytorch.to(device=0)"]},{"cell_type":"code","execution_count":null,"id":"aec4e251","metadata":{"id":"aec4e251"},"outputs":[],"source":["model_pytorch_ouputs = model_pytorch(**model_input)"]},{"cell_type":"code","execution_count":null,"id":"cf6714d0","metadata":{"id":"cf6714d0","outputId":"a75bcbf9-0cac-4078-e328-ba62cae9d42a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711940454224,"user_tz":300,"elapsed":369,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.6826, device='cuda:0', grad_fn=<NllLossBackward0>)"]},"metadata":{},"execution_count":36}],"source":["model_pytorch_ouputs.loss"]},{"cell_type":"code","execution_count":null,"id":"099fd8b1","metadata":{"id":"099fd8b1","outputId":"d5dccd0f-d7f5-4211-f118-d4f845a23e7c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711940454225,"user_tz":300,"elapsed":20,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 2])"]},"metadata":{},"execution_count":37}],"source":["model_pytorch_ouputs.logits.shape"]},{"cell_type":"code","execution_count":null,"id":"70ec9124","metadata":{"id":"70ec9124","outputId":"f9169c7e-47fc-492b-a2be-b949b8704bf6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711940454226,"user_tz":300,"elapsed":17,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.1040, -0.0668],\n","        [ 0.0026, -0.1152]], device='cuda:0', grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":38}],"source":["model_pytorch_ouputs.logits"]},{"cell_type":"markdown","id":"d121d659","metadata":{"id":"d121d659"},"source":["# <font color = 'indianred'> **6. Using AutoModel for SequenceClassification**"]},{"cell_type":"code","execution_count":null,"id":"17d0a30c","metadata":{"id":"17d0a30c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711940454790,"user_tz":300,"elapsed":575,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"48a6add7-d0b5-45dc-d746-ea8a95def9bd"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["auto_model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"]},{"cell_type":"code","execution_count":null,"id":"a253cf97","metadata":{"id":"a253cf97","outputId":"b16da648-3cf4-4543-8201-d04436b11cb4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711940454791,"user_tz":300,"elapsed":13,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":40}],"source":["auto_model"]},{"cell_type":"markdown","id":"b2dcab79","metadata":{"id":"b2dcab79"},"source":["<font color = 'indianred'> *AutoConfig for pre-Trained Model*"]},{"cell_type":"markdown","id":"cf5d3168","metadata":{"id":"cf5d3168"},"source":["<font color = 'indianred'> *Explanation of Model configuration file*</font>\n","\n","- A configuration file, in the context of pretrained models like those in the Hugging Face Transformers library, is a vital component that details the model's architecture, hyperparameters, and other essential settings. It serves as a blueprint, guiding how the model is structured and operates.\n","\n","- Specifically, for models intended for tasks like classification, two critical pieces of information are `id2label` and `label2id`.\n","\n","- `id2label` is a dictionary mapping numerical IDs to their respective class labels, while `label2id` is its inverse, mapping class labels to their IDs. These mappings are fundamental for translating between human-readable class labels (like \"positive\" or \"negative\") and the numerical IDs the model uses internally during training and inference.\n","\n","- By ensuring that the configuration file contains `id2label` and `label2id`, you guarantee a seamless conversion between model outputs and interpretable class labels. Without them, translating the model's predictions into understandable results can be cumbersome. Adding this information enhances the usability and clarity of the model, especially when deploying it for real-world applications.\n"]},{"cell_type":"markdown","id":"20e175e7","metadata":{"id":"20e175e7"},"source":["<font color = 'indianred'>*Download config file of pre-trained Model*</font>\n","\n"]},{"cell_type":"code","execution_count":null,"id":"e8ceb1f2","metadata":{"id":"e8ceb1f2"},"outputs":[],"source":["config = AutoConfig.from_pretrained(checkpoint)\n"]},{"cell_type":"code","execution_count":null,"id":"50fd9e99","metadata":{"id":"50fd9e99","outputId":"6379b739-5a3e-40d2-a1e8-c30dce341c1c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711940455101,"user_tz":300,"elapsed":33,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}"]},"metadata":{},"execution_count":42}],"source":["config"]},{"cell_type":"markdown","id":"9cba74aa","metadata":{"id":"9cba74aa"},"source":["<font color = 'indianred'>*Modify Configuration File*</font>\n","- We need to modify configuration fie to add ids to  label and label to ids mapping\n","- Adding id2label and label2id to the configuration file provides a consistent, interpretable, and user-friendly way to handle model outputs."]},{"cell_type":"code","execution_count":null,"id":"377928d4","metadata":{"id":"377928d4","outputId":"d7a66274-1101-48c1-f3aa-26c13b347a74","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711940455101,"user_tz":300,"elapsed":28,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['neg', 'pos']"]},"metadata":{},"execution_count":43}],"source":["class_names = ['neg', 'pos']\n","class_names\n"]},{"cell_type":"code","execution_count":null,"id":"58b38792","metadata":{"id":"58b38792","outputId":"766d08dc-fb81-452f-b374-0b3c52b7fe46","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711940455101,"user_tz":300,"elapsed":24,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'neg', 1: 'pos'}"]},"metadata":{},"execution_count":44}],"source":["id2label = {}\n","for id_, label_ in enumerate(class_names):\n","    id2label[id_] = label_\n","id2label\n"]},{"cell_type":"markdown","id":"87607afb","metadata":{"id":"87607afb"},"source":["Code Explanation:\n","- First, an empty dictionary, id2label, is initialized.\n","- The enumerate function returns both the index (or ID) and the value (or label) of each item in the class_names list as you loop through it.\n","- Within the loop, each numerical ID (id_) is converted to a string using str(id_) and then used as a key in the id2label dictionary. The corresponding class name (label_) from the class_names list is assigned as the value for that key.\n","- Why was numerical ID converted to string? - When the configuration is saved to disk, it's typically stored in a JSON format. JSON keys must be strings, so using non-string keys would cause serialization errors. By ensuring that the IDs are strings in Python, the configuration can be seamlessly serialized to and deserialized from JSON without any type conversion issues."]},{"cell_type":"code","execution_count":null,"id":"0c564da5","metadata":{"id":"0c564da5","outputId":"9a093d2c-151b-4d9f-ae93-566a1b707c74","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711940455101,"user_tz":300,"elapsed":21,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'neg': 0, 'pos': 1}"]},"metadata":{},"execution_count":45}],"source":["label2id = {}\n","for id_, label_ in enumerate(class_names):\n","    label2id[label_] = id_\n","label2id\n"]},{"cell_type":"code","execution_count":null,"id":"88b4ab08","metadata":{"id":"88b4ab08"},"outputs":[],"source":["config.id2label = id2label\n","config.label2id = label2id\n"]},{"cell_type":"code","execution_count":null,"id":"82acf83f","metadata":{"id":"82acf83f","outputId":"c14f5307-2746-4fd6-d5b7-b202a9eb31d8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711940455102,"user_tz":300,"elapsed":17,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"neg\",\n","    \"1\": \"pos\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"neg\": 0,\n","    \"pos\": 1\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}"]},"metadata":{},"execution_count":47}],"source":["config\n"]},{"cell_type":"code","execution_count":null,"id":"6f79cc8f","metadata":{"id":"6f79cc8f"},"outputs":[],"source":["auto_model.config = config"]},{"cell_type":"code","execution_count":null,"id":"4649f218","metadata":{"id":"4649f218"},"outputs":[],"source":["auto_model= auto_model.to(device=0)\n","auto_model_outputs = auto_model(**model_input)"]},{"cell_type":"code","execution_count":null,"id":"28278ae9","metadata":{"id":"28278ae9","outputId":"b6462d9c-1949-415a-c5f8-25e7cda852dc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711940455383,"user_tz":300,"elapsed":13,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['loss', 'logits'])"]},"metadata":{},"execution_count":50}],"source":["auto_model_outputs.keys()"]},{"cell_type":"code","execution_count":null,"id":"7704b247","metadata":{"id":"7704b247","outputId":"29f7505b-5729-4db0-99d1-39124a33d2e7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711940455384,"user_tz":300,"elapsed":11,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.1743, 0.1758],\n","        [0.0244, 0.4157]], device='cuda:0', grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":51}],"source":["auto_model_outputs.logits"]},{"cell_type":"code","execution_count":null,"id":"da6cf899","metadata":{"id":"da6cf899","outputId":"20758365-c911-4b1d-815c-f6fe98b94da1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711940455384,"user_tz":300,"elapsed":8,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.6052, device='cuda:0', grad_fn=<NllLossBackward0>)"]},"metadata":{},"execution_count":52}],"source":["auto_model_outputs.loss"]},{"cell_type":"markdown","id":"644be32a","metadata":{"id":"644be32a"},"source":["<font color = 'indianred'> *Understanding Model Output*"]},{"cell_type":"markdown","id":"tSgGJD97m8aC","metadata":{"id":"tSgGJD97m8aC"},"source":["The model output consists of logits and a loss value, indicating the model's predictions and the performance on the input data:\n","\n","1. **Logits (`model_output.logits`):**\n","   - The logits are the raw, unnormalized scores output by the model's final layer.\n","   - For each input sequence, the logits represent the model's predictions before applying an activation function (like softmax).\n","\n","2. **Loss (`model_output.loss`):**\n","   - The loss value (e.g., `1.0800`) represents the model's performance on the input data. It quantifies the difference between the model's predictions and the actual labels.\n","   - A lower loss value indicates better model performance, as it means the model's predictions are closer to the true labels.\n","   - The loss is used during training to update the model's weights, with the goal of minimizing this value over time."]},{"cell_type":"markdown","source":["# <font color = 'indianred'> **7. Tokenization for Pair of Documents**"],"metadata":{"id":"H8RBVR7LuQUI"},"id":"H8RBVR7LuQUI"}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c26e1015923d43b79abb9dcc334cfe54":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6f67b9aa8a5f47aaac48052dacef5d56","IPY_MODEL_c3e0f38f52d04b17982f4a5e49db8ec2","IPY_MODEL_4d6c877499f345f08335b2ac129863e8"],"layout":"IPY_MODEL_c208cc7cf2d74405b3d0b30bc528ace1"}},"6f67b9aa8a5f47aaac48052dacef5d56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec362c35022d48248b7451289e42b0fe","placeholder":"​","style":"IPY_MODEL_1276eaeb40c7428290746253c8a17bab","value":"tokenizer_config.json: 100%"}},"c3e0f38f52d04b17982f4a5e49db8ec2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d300f991a624e56887db068ea6480a9","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_59962aac00ee4f369fb889329938b2f5","value":48}},"4d6c877499f345f08335b2ac129863e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7504c15e4d5d47f2a9a680e5a95a6e44","placeholder":"​","style":"IPY_MODEL_b2293182e3204e0a813bfff147e932be","value":" 48.0/48.0 [00:00&lt;00:00, 2.83kB/s]"}},"c208cc7cf2d74405b3d0b30bc528ace1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec362c35022d48248b7451289e42b0fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1276eaeb40c7428290746253c8a17bab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d300f991a624e56887db068ea6480a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59962aac00ee4f369fb889329938b2f5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7504c15e4d5d47f2a9a680e5a95a6e44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2293182e3204e0a813bfff147e932be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"210cee2e15ac4dfabe6fb03223ebaf31":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fa3c31e163a54f28a53cdfc29b946506","IPY_MODEL_91204c102de640c1ac15f59d2f5435ac","IPY_MODEL_1886b7ed8200483eb912452d9bce8450"],"layout":"IPY_MODEL_6e3ceef1cc8149f3b4dd793c56087674"}},"fa3c31e163a54f28a53cdfc29b946506":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2633c6a298b64f32ad354f27d61ffa2b","placeholder":"​","style":"IPY_MODEL_bf9a2f636ede451d9f2000af51ce575f","value":"config.json: 100%"}},"91204c102de640c1ac15f59d2f5435ac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2d47f7fbaf5437eab0cb48eff7803c9","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f7a4528acced4051b524e7ba94ac3047","value":570}},"1886b7ed8200483eb912452d9bce8450":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f9e9b315ae34d65bdb1a145d2d88710","placeholder":"​","style":"IPY_MODEL_e132e08183264b0ca96f75d7bf156e95","value":" 570/570 [00:00&lt;00:00, 38.0kB/s]"}},"6e3ceef1cc8149f3b4dd793c56087674":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2633c6a298b64f32ad354f27d61ffa2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf9a2f636ede451d9f2000af51ce575f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a2d47f7fbaf5437eab0cb48eff7803c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7a4528acced4051b524e7ba94ac3047":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5f9e9b315ae34d65bdb1a145d2d88710":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e132e08183264b0ca96f75d7bf156e95":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a41ca147bab04a2eb189017dfa38d011":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_607b5070b66a42618c613d5ee938a307","IPY_MODEL_2b94d0ff85424b96a3707345b84415df","IPY_MODEL_24b0649097214854ab1fd00b96373469"],"layout":"IPY_MODEL_2ede05b012714503aed29e6e5568555b"}},"607b5070b66a42618c613d5ee938a307":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ab56b1922c24d47bcbc8743e220966a","placeholder":"​","style":"IPY_MODEL_f82f7aca8df74f39857679e2cbf7d65a","value":"vocab.txt: 100%"}},"2b94d0ff85424b96a3707345b84415df":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5339b5231f0741e6b52b8c51a7d73748","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26b4aae2c45740ccbae9fc84c9976348","value":231508}},"24b0649097214854ab1fd00b96373469":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b81d8ce5344486cb8b1e57ff975d549","placeholder":"​","style":"IPY_MODEL_fa99893c8081428bbf6f584732ed584c","value":" 232k/232k [00:00&lt;00:00, 7.46MB/s]"}},"2ede05b012714503aed29e6e5568555b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ab56b1922c24d47bcbc8743e220966a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f82f7aca8df74f39857679e2cbf7d65a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5339b5231f0741e6b52b8c51a7d73748":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26b4aae2c45740ccbae9fc84c9976348":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9b81d8ce5344486cb8b1e57ff975d549":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa99893c8081428bbf6f584732ed584c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f1beb529ed14aa5a63581704428dd46":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9ba8c281a8724e02bb3c88eeb9b6496d","IPY_MODEL_2a2e9a94182e47f78b07c4b13aa5e5c5","IPY_MODEL_5d8785f4173949379d1e80126f10938c"],"layout":"IPY_MODEL_58f2fd3d59ef4496ac98d8d6fb7c6635"}},"9ba8c281a8724e02bb3c88eeb9b6496d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d0e9e81ae2742d19189850f3b5d5bd2","placeholder":"​","style":"IPY_MODEL_94e7c73e8346445ab2b6cc2d73a99859","value":"tokenizer.json: 100%"}},"2a2e9a94182e47f78b07c4b13aa5e5c5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_562175e32db94700a16edfdf336544f8","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4952d884f30a4574a75d6f4496c920c8","value":466062}},"5d8785f4173949379d1e80126f10938c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3dbaaef3501426ca942c32f8a0d0524","placeholder":"​","style":"IPY_MODEL_1a755ebce4594de5b28fbda0d5941b9e","value":" 466k/466k [00:00&lt;00:00, 23.3MB/s]"}},"58f2fd3d59ef4496ac98d8d6fb7c6635":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d0e9e81ae2742d19189850f3b5d5bd2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94e7c73e8346445ab2b6cc2d73a99859":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"562175e32db94700a16edfdf336544f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4952d884f30a4574a75d6f4496c920c8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e3dbaaef3501426ca942c32f8a0d0524":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a755ebce4594de5b28fbda0d5941b9e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"907050629ce74693bfd9098050bdafae":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_adf726ac9dcf446aacc827e46295bbd2","IPY_MODEL_b6bc063f53674619974015315057bc29","IPY_MODEL_9fe1dd4ce5f64eb287f86f1aeaf61f6c"],"layout":"IPY_MODEL_f4ffb6c4780a4ceca1e3eda046cc6ccf"}},"adf726ac9dcf446aacc827e46295bbd2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e58d861d9434783a8d07ca158825faf","placeholder":"​","style":"IPY_MODEL_623c3f02122549c1ad4f7f71f7840ace","value":"Map: 100%"}},"b6bc063f53674619974015315057bc29":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb97b1a5f93f4a6a9cdee2fa39747cc3","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_95d2bdab0cf24fdd9dce247502288bb2","value":2}},"9fe1dd4ce5f64eb287f86f1aeaf61f6c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_004e6e08de6c4a5e82f65c17e20a8d2a","placeholder":"​","style":"IPY_MODEL_8ccfe46927c1480ab3f1e861623973c7","value":" 2/2 [00:00&lt;00:00, 51.87 examples/s]"}},"f4ffb6c4780a4ceca1e3eda046cc6ccf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e58d861d9434783a8d07ca158825faf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"623c3f02122549c1ad4f7f71f7840ace":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb97b1a5f93f4a6a9cdee2fa39747cc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95d2bdab0cf24fdd9dce247502288bb2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"004e6e08de6c4a5e82f65c17e20a8d2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ccfe46927c1480ab3f1e861623973c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5f20d7c16584987869793f56a89442a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8ab661b7ca464e858312dc802e5638dd","IPY_MODEL_aa4b82dfd1b744ffbd03ac81d6726905","IPY_MODEL_9154120e47c54feabfdb390ab0ad274f"],"layout":"IPY_MODEL_28ef5aac3d7742b98c8119b1374fc330"}},"8ab661b7ca464e858312dc802e5638dd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf18eb96b55b414fb53a149acd670cc4","placeholder":"​","style":"IPY_MODEL_a3458200346940deabd73d2ef6907843","value":"model.safetensors: 100%"}},"aa4b82dfd1b744ffbd03ac81d6726905":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f7fac9835db47c38a3a067b13962b1c","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ccbe71d5e5e64eec85ab810c49a1c0ee","value":440449768}},"9154120e47c54feabfdb390ab0ad274f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b497838a956b4b5c86667668c3c5a588","placeholder":"​","style":"IPY_MODEL_d06be31d2a214b9c997b79ce8b64c443","value":" 440M/440M [00:03&lt;00:00, 142MB/s]"}},"28ef5aac3d7742b98c8119b1374fc330":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf18eb96b55b414fb53a149acd670cc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3458200346940deabd73d2ef6907843":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f7fac9835db47c38a3a067b13962b1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccbe71d5e5e64eec85ab810c49a1c0ee":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b497838a956b4b5c86667668c3c5a588":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d06be31d2a214b9c997b79ce8b64c443":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}