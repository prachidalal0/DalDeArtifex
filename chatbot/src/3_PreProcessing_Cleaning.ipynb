{"cells":[{"cell_type":"markdown","metadata":{"id":"JmvijURlnQOq"},"source":["# <font color = 'dodgerblue'>**PreProcessing_Feature_Engineering_IMDB**</font>\n","\n","We have covered the fundamentals of spaCy, especially the process of tokenization in the previous notebooks. In this notebook, we will learn how to alter the default tokenizer in spaCy. Then, we will use a simple example to demonstrate how spaCy can be employed for pre-processing and creating manual features. Lastly, we will develop classes for (1) general preprocessing and (2) manual feature extraction. These classes will be designed to integrate seamlessly with scikit-learn pipelines. Integrating with scikit-learn pipelines is beneficial as it streamlines the process of chaining multiple preprocessing steps and model training, ensuring consistency and efficiency in the workflow from data preparation to model evaluation.This will improve our understanding of scikit-learn classes as well.\n","\n","**Plan**\n","\n","1. Basic cleaning (remove HTML tags).\n","2. Understand how to do preprocessing using spaCy.\n","3. Understand how to change the default behavior of the tokenizer in spaCy.\n","4. Create a custom pre-processing class.\n","5. Understand the extraction of POS (Part Of Speech) related features.\n","6. Understand the extraction of Text Descriptive Features.\n","7. Extract the count of named entities.\n","8. Create a custom class for manual feature extraction.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dsuaOxU-tG5D"},"source":["# <font color = 'dodgerblue'>**Install/Import Libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-30T09:04:46.983241Z","iopub.status.busy":"2023-01-30T09:04:46.982952Z","iopub.status.idle":"2023-01-30T09:04:46.992689Z","shell.execute_reply":"2023-01-30T09:04:46.991818Z","shell.execute_reply.started":"2023-01-30T09:04:46.983163Z"},"id":"SYkzEkPRwVbC","tags":[]},"outputs":[],"source":["# install spacy\n","if 'google.colab' in str(get_ipython()):\n","    !pip install -U spacy -qq"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-30T09:04:47.316892Z","iopub.status.busy":"2023-01-30T09:04:47.316658Z","iopub.status.idle":"2023-01-30T09:04:49.307720Z","shell.execute_reply":"2023-01-30T09:04:49.307364Z","shell.execute_reply.started":"2023-01-30T09:04:47.316866Z"},"id":"kV1KnxhOMK-0","tags":[],"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706501460120,"user_tz":360,"elapsed":2702,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"8c8e0e73-4be5-4293-f8f8-67f2727eb10b"},"outputs":[{"output_type":"stream","name":"stderr","text":["2024-01-28 22:10:58.632442: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-01-28 22:10:59.757804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2024-01-28 22:10:59.759051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2024-01-28 22:10:59.760005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"]}],"source":["# Import the pandas library for working with data frames\n","import pandas as pd\n","import numpy as np\n","\n","# Import the spacy library for natural language processing\n","import spacy\n","\n","# Import the List type from the typing module to use in function annotations\n","from typing import List\n","\n","# for basic cleaning\n","from bs4 import BeautifulSoup\n","import re\n","\n","from pprint import pprint"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-01-30T09:04:49.308462Z","iopub.status.busy":"2023-01-30T09:04:49.308376Z","iopub.status.idle":"2023-01-30T09:04:49.313574Z","shell.execute_reply":"2023-01-30T09:04:49.313316Z","shell.execute_reply.started":"2023-01-30T09:04:49.308450Z"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1706501461723,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"fRNfn9z_wXzL","outputId":"ec670148-851b-4a07-a339-d7c5adc61f93","tags":[]},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'3.7.2'"]},"metadata":{},"execution_count":3}],"source":["# check spacy version\n","spacy.__version__\n"]},{"cell_type":"markdown","metadata":{"id":"D3sRlk1-tG5N"},"source":["# <font color = 'dodgerblue'>**Data**"]},{"cell_type":"code","source":["text = [\"\"\"New version of operation system is iOS 11. It is better than iOS 9.\n","The new version of iPhone X seems cool.\"\"\", \"\"\" <p> The video of iphone x released. I liked iOS 9 but I like iOS 11 more.\n","You may not like my like @Tech_Guru #Iphone #IOS harpreet@utdallas.edu  https://jindal.utdallas.edu/\"\"\",\n","        \"\"\"</p><p>The concept of regular expressions began in the 1950s, when the American mathematician <a href=\"/wiki/Stephen_Cole_Kleene\" title=\"Stephen Cole Kleene\">Stephen Cole Kleene</a> formalized the description of a <i><a href=\"/wiki/Regular_language\" title=\"Regular language\">regular language</a></i>. They came into common use with <a href=\"/wiki/Unix\" title=\"Unix\">Unix</a> text-processing utilities. Different <a href=\"/wiki/Syntax_(programming_languages)\" title=\"Syntax (programming languages)\">syntaxes</a> for writing regular expressions have existed since the 1980s, one being the <a href=\"/wiki/POSIX\" title=\"POSIX\">POSIX</a> standard and another, widely used, being the <a href=\"/wiki/Perl\" title=\"Perl\">Perl</a> syntax.\n","</p><p>Regular expressions are used in <a href=\"/wiki/Search_engine\" title=\"Search engine\">search engines</a>, search and replace dialogs of <a href=\"/wiki/Word_processor\" title=\"Word processor\">word processors</a> and <a href=\"/wiki/Text_editor\" title=\"Text editor\">text editors</a>, in <a href=\"/wiki/Text_processing\" title=\"Text processing\">text processing</a> utilities such as <a href=\"/wiki/Sed\" title=\"Sed\">sed</a> and <a href=\"/wiki/AWK\" title=\"AWK\">AWK</a> and in <a href=\"/wiki/Lexical_analysis\" title=\"Lexical analysis\">lexical analysis</a>. Many <a href=\"/wiki/Programming_language\" title=\"Programming language\">programming languages</a> provide regex capabilities either built-in or via <a href=\"/wiki/Library_(computing)\" title=\"Library (computing)\">libraries</a>, as it has uses in many situations.\n","</p> \"\"\"]\n","df = pd.DataFrame(text, columns=['Reviews'])"],"metadata":{"id":"EmypysdLqKMo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"Io_Y6445tT-K","executionInfo":{"status":"ok","timestamp":1706501465370,"user_tz":360,"elapsed":9,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"cb3c27c0-fdac-4ad0-af2b-820a24ef09ad"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                             Reviews\n","0  New version of operation system is iOS 11. It ...\n","1   <p> The video of iphone x released. I liked i...\n","2  </p><p>The concept of regular expressions bega..."],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Reviews</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>New version of operation system is iOS 11. It ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>&lt;p&gt; The video of iphone x released. I liked i...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>&lt;/p&gt;&lt;p&gt;The concept of regular expressions bega...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["pprint(df['Reviews'].values[0], width = 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v6MTb_dgqTSS","executionInfo":{"status":"ok","timestamp":1706501466963,"user_tz":360,"elapsed":3,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"56e61901-fa72-4f6a-c933-0ea3542a32f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["('New version of operation system is iOS 11. It is better than iOS 9.\\n'\n"," 'The new version of iPhone X seems cool.')\n"]}]},{"cell_type":"code","source":["pprint(df['Reviews'].values[1], width = 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HyucsMQ9q57B","executionInfo":{"status":"ok","timestamp":1706501469182,"user_tz":360,"elapsed":5,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"70c867e5-5c28-49c6-a0b2-aa99e6b35d02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(' <p> The video of iphone x released. I liked iOS 9 but I like iOS 11 more.\\n'\n"," 'You may not like my like @Tech_Guru #Iphone #IOS harpreet@utdallas.edu  '\n"," 'https://jindal.utdallas.edu/')\n"]}]},{"cell_type":"code","source":["pprint(df['Reviews'].values[2], width = 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tRJgH7i4qOa6","executionInfo":{"status":"ok","timestamp":1706501469588,"user_tz":360,"elapsed":13,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"ae28203b-2c3b-487a-922a-241f2a5b89d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["('</p><p>The concept of regular expressions began in the 1950s, when the '\n"," 'American mathematician <a href=\"/wiki/Stephen_Cole_Kleene\" title=\"Stephen '\n"," 'Cole Kleene\">Stephen Cole Kleene</a> formalized the description of a <i><a '\n"," 'href=\"/wiki/Regular_language\" title=\"Regular language\">regular '\n"," 'language</a></i>. They came into common use with <a href=\"/wiki/Unix\" '\n"," 'title=\"Unix\">Unix</a> text-processing utilities. Different <a '\n"," 'href=\"/wiki/Syntax_(programming_languages)\" title=\"Syntax (programming '\n"," 'languages)\">syntaxes</a> for writing regular expressions have existed since '\n"," 'the 1980s, one being the <a href=\"/wiki/POSIX\" title=\"POSIX\">POSIX</a> '\n"," 'standard and another, widely used, being the <a href=\"/wiki/Perl\" '\n"," 'title=\"Perl\">Perl</a> syntax.\\n'\n"," '</p><p>Regular expressions are used in <a href=\"/wiki/Search_engine\" '\n"," 'title=\"Search engine\">search engines</a>, search and replace dialogs of <a '\n"," 'href=\"/wiki/Word_processor\" title=\"Word processor\">word processors</a> and '\n"," '<a href=\"/wiki/Text_editor\" title=\"Text editor\">text editors</a>, in <a '\n"," 'href=\"/wiki/Text_processing\" title=\"Text processing\">text processing</a> '\n"," 'utilities such as <a href=\"/wiki/Sed\" title=\"Sed\">sed</a> and <a '\n"," 'href=\"/wiki/AWK\" title=\"AWK\">AWK</a> and in <a href=\"/wiki/Lexical_analysis\" '\n"," 'title=\"Lexical analysis\">lexical analysis</a>. Many <a '\n"," 'href=\"/wiki/Programming_language\" title=\"Programming language\">programming '\n"," 'languages</a> provide regex capabilities either built-in or via <a '\n"," 'href=\"/wiki/Library_(computing)\" title=\"Library (computing)\">libraries</a>, '\n"," 'as it has uses in many situations.\\n'\n"," '</p> ')\n"]}]},{"cell_type":"code","source":["type(df['Reviews'].values[2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b98LSsSMtt5p","executionInfo":{"status":"ok","timestamp":1706501469957,"user_tz":360,"elapsed":22,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"4ae68592-330d-4b33-85d5-c2b0461bdc06"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["str"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"o1yKR_gAnQPK"},"source":["# <font color = 'dodgerblue'>**Import Spacy Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-01-30T09:05:01.617847Z","iopub.status.busy":"2023-01-30T09:05:01.617776Z","iopub.status.idle":"2023-01-30T09:05:05.855320Z","shell.execute_reply":"2023-01-30T09:05:05.854571Z","shell.execute_reply.started":"2023-01-30T09:05:01.617838Z"},"executionInfo":{"elapsed":7603,"status":"ok","timestamp":1706501478407,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"5oBaejG-xSLh","outputId":"b3063ac0-0ab5-46fd-bdfc-663826327060","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["2024-01-28 22:11:11.992271: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\r\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-01-28 22:11:13.391665: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2024-01-28 22:11:13.392823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2024-01-28 22:11:13.393677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","Collecting en-core-web-sm==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in ./mambaforge/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./mambaforge/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.1)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./mambaforge/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.4)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./mambaforge/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./mambaforge/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.1.8 in ./mambaforge/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.9)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./mambaforge/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.8)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./mambaforge/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./mambaforge/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n","Requirement already satisfied: packaging>=20.0 in ./mambaforge/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.0)\n","Requirement already satisfied: jinja2 in ./mambaforge/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n","Requirement already satisfied: setuptools in ./mambaforge/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.6.3)\n","Requirement already satisfied: numpy>=1.19.0 in ./mambaforge/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.23.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./mambaforge/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./mambaforge/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./mambaforge/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./mambaforge/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.2)\n","Requirement already satisfied: weasel<0.4.0,>=0.1.0 in ./mambaforge/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./mambaforge/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./mambaforge/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.7)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./mambaforge/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.6)\n","Requirement already satisfied: typing-extensions>=4.2.0 in ./mambaforge/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in ./mambaforge/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in ./mambaforge/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.15)\n","Requirement already satisfied: charset-normalizer<4,>=2 in ./mambaforge/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in ./mambaforge/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./mambaforge/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./mambaforge/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.0.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in ./mambaforge/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.3)\n","Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in ./mambaforge/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in ./mambaforge/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.2)\n","Installing collected packages: en-core-web-sm\n","  Attempting uninstall: en-core-web-sm\n","    Found existing installation: en-core-web-sm 3.7.0\n","    Uninstalling en-core-web-sm-3.7.0:\n","      Successfully uninstalled en-core-web-sm-3.7.0\n","Successfully installed en-core-web-sm-3.7.1\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}],"source":["# check the models we have dowloaded in spacy folder\n","!python -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-30T09:05:05.856685Z","iopub.status.busy":"2023-01-30T09:05:05.856487Z","iopub.status.idle":"2023-01-30T09:05:06.232281Z","shell.execute_reply":"2023-01-30T09:05:06.231907Z","shell.execute_reply.started":"2023-01-30T09:05:05.856659Z"},"id":"kI4ZLLkcnQPT","tags":[]},"outputs":[],"source":["# We will load the model -en_core_web_sm\n","nlp = spacy.load('en_core_web_sm')\n"]},{"cell_type":"markdown","metadata":{"id":"07hFKgezHnzW"},"source":["# <font color = 'dodgerblue'>**Pre-processing**"]},{"cell_type":"markdown","source":["## <font color = 'dodgerblue'>**Basic cleaning (remove HTML tags).**"],"metadata":{"id":"Lyz4xHSryVX5"}},{"cell_type":"code","source":["def basic_clean(text: str) -> str:\n","    \"\"\"\n","    This function performs basic text cleaning on an input string by removing HTML tags (if present)\n","    and replacing newline and return characters with a space.\n","\n","    Parameters:\n","    text (str): The input string to be cleaned\n","\n","    Returns:\n","    str: The cleaned string\n","    \"\"\"\n","    # Use BeautifulSoup to remove HTML tags (if present)\n","    soup = BeautifulSoup(text, \"html.parser\")\n","    text = soup.get_text()\n","\n","    # Replace newline and return characters with a space\n","    return re.sub(r'[\\n\\r]',' ', text)"],"metadata":{"id":"Px08vdQRoByY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cleaned_text= [basic_clean(text).encode('utf-8', 'ignore').decode() for text in df['Reviews'].values]"],"metadata":{"id":"okYIY1eYoD5W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pprint(cleaned_text[2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ostbfo3SpTzw","executionInfo":{"status":"ok","timestamp":1706501479935,"user_tz":360,"elapsed":9,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"f6e6601c-8ac2-4c97-b463-c63c5883cd7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["('The concept of regular expressions began in the 1950s, when the American '\n"," 'mathematician Stephen Cole Kleene formalized the description of a regular '\n"," 'language. They came into common use with Unix text-processing utilities. '\n"," 'Different syntaxes for writing regular expressions have existed since the '\n"," '1980s, one being the POSIX standard and another, widely used, being the Perl '\n"," 'syntax. Regular expressions are used in search engines, search and replace '\n"," 'dialogs of word processors and text editors, in text processing utilities '\n"," 'such as sed and AWK and in lexical analysis. Many programming languages '\n"," 'provide regex capabilities either built-in or via libraries, as it has uses '\n"," 'in many situations.  ')\n"]}]},{"cell_type":"markdown","source":["## <font color = 'dodgerblue'>**Understand how to do preprocessing using spaCy**\n","- We will now understand how to do basic pre-processing (like removing stop words, lammetization, remove urls, remove emails etc.) using Spacy.\n","- We will also underatand how Spaxy handles mentions (@keyword) and hashtags (#keyword)."],"metadata":{"id":"JVGRWzXwyrLo"}},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-30T09:14:25.381942Z","iopub.status.busy":"2023-01-30T09:14:25.381624Z","iopub.status.idle":"2023-01-30T09:14:48.930788Z","shell.execute_reply":"2023-01-30T09:14:48.929670Z","shell.execute_reply.started":"2023-01-30T09:14:25.381927Z"},"id":"fIBqaxeJnQVt","tags":[]},"outputs":[],"source":["# initialize an empty list to store processed text\n","processed_text = []\n","\n","# initialize an empty list to store processed text\n","processed_text = []\n","\n","# Use context manager to temporarily disable the named pipes of spaCy NLP processing pipeline\n","with nlp.select_pipes(disable=['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']):\n","  # process multiple documents in parallel using the spaCy NLP library\n","  for doc in nlp.pipe(cleaned_text, batch_size=3, n_process=1):\n","      # filter out URLs and emails, then extract text of each token\n","      tokens = [token.text for token in doc if not token.like_url and not token.like_email]\n","      # join tokens back into a single string\n","      text = ' '.join(tokens)\n","      # append to the processed_text list\n","      processed_text.append(text)"]},{"cell_type":"code","source":["cleaned_text[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C4c9Jqneu19h","executionInfo":{"status":"ok","timestamp":1706502359869,"user_tz":360,"elapsed":9,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"5046321f-40c8-44db-d787-3e8b447f5430"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'  The video of iphone x released. I liked iOS 9 but I like iOS 11 more. You may not like my like @Tech_Guru #Iphone #IOS harpreet@utdallas.edu  https://jindal.utdallas.edu/'"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-01-30T09:14:48.932492Z","iopub.status.busy":"2023-01-30T09:14:48.932344Z","iopub.status.idle":"2023-01-30T09:14:48.966025Z","shell.execute_reply":"2023-01-30T09:14:48.965594Z","shell.execute_reply.started":"2023-01-30T09:14:48.932477Z"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706502361123,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"s1NHXq-iIOZ-","outputId":"4d62beef-7fcb-4408-af58-1c84a7b8cc17","tags":[]},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'The video of iphone x released . I liked iOS 9 but I like iOS 11 more . You may not like my like @Tech_Guru # Iphone # IOS'"]},"metadata":{},"execution_count":49}],"source":["processed_text[1]"]},{"cell_type":"markdown","source":["- From the above we can see that `#` is treated as prefix and hence it sis separated from the keyword. `#' becomes a separate token.. If we remove punctuations then `#` will be removed.\n","\n","- However `@` is not treated as prefix. Hence, `@keyword` is treated as one single token."],"metadata":{"id":"qDhLM6JOzV5K"}},{"cell_type":"markdown","source":["## <font color = 'dodgerblue'>**Understand how to change the default behavior of the tokenizer in spaCy**\n","\n","We will now see how we can change this default behavior. We will modify the default prefixes. You can also modify the default suffixes in a similar manner."],"metadata":{"id":"1Xb38e7c0uS8"}},{"cell_type":"code","source":["# Load the spaCy model for English. This model includes various components like tokenization, lemmatization, part-of-speech tagging, etc.\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Get the default set of prefix characters from the loaded spaCy model.\n","# In tokenization, these prefixes are characters or sets of characters at the beginning of a word\n","# that are treated separately from the word they are attached to.\n","prefixes = list(nlp.Defaults.prefixes)\n","\n","# The following lines adjust the tokenizer behavior for specific characters, often used in social media text or web text.\n","\n","# Add '@' to the list of prefixes.\n","prefixes += ['@']\n","\n","# Remove '#' from the list of prefixes. This is typically done when you want to treat hashtags as a single token rather\n","# than separating the '#' symbol from the following text.\n","# It allows the hashtag to remain intact for further analysis or feature extraction.\n","prefixes.remove(r'#')\n","\n","\n","# Compile prefix regex based on selected prefixes\n","prefix_regex = spacy.util.compile_prefix_regex(prefixes)\n","nlp.tokenizer.prefix_search = prefix_regex.search"],"metadata":{"id":"VARff10yuBCS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize an empty list to store processed text\n","processed_text = []\n","\n","# Use context manager to temporarily disable the named pipes of spaCy NLP processing pipeline\n","with nlp.select_pipes(disable=['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']):\n","  # process multiple documents in parallel using the spaCy NLP library\n","  for doc in nlp.pipe(cleaned_text, batch_size=3, n_process=1):\n","      # filter out URLs and emails, then extract text of each token\n","      tokens = [token.text for token in doc if not token.like_url and not token.like_email]\n","      text = ' '.join(tokens)\n","      # join tokens back into a single string and add to the processed_text list\n","      processed_text.append(text)"],"metadata":{"id":"qxgMd4KnwLOb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["processed_text[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"65s4TpATwOKq","executionInfo":{"status":"ok","timestamp":1706423820912,"user_tz":360,"elapsed":618,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"67bbe6f8-27d9-4dc6-fde8-dea3fe46d40e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'   The video of iphone x released . I liked iOS 9 but I like iOS 11 more . You may not like my like @ Tech_Guru #Iphone #IOS  '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["- `@Tech_Guru` is tokenized as `@ Tech_Guru`, indicating that the '@' character is treated as a separate token. This modification is due to adding '@' to the list of prefix characters, making the tokenizer recognize and separate it from the word that follows. The intention might be to isolate mentions or handle them distinctly in subsequent processing steps.\n","\n","- `#Iphone` and `#IOS` appear intact, without separating the '#' from the keywords. This is the result of removing '#' from the list of prefix characters. By doing this, the tokenizer no longer splits hashtags into two tokens ('#' and 'keyword'). Instead, each hashtag is preserved as a single token, which is beneficial for analyses where the integrity of hashtags is important, such as sentiment analysis or trend detection on social media platforms.\n","\n"],"metadata":{"id":"KQAxG0eByR8R"}},{"cell_type":"markdown","source":["## <font color = 'dodgerblue'>**Create a custom pre-processing class**\n","Now, we will create a Custom class for pre-processing in Spacy. This will help us to re-use this code. Once, we test this class, we will create a module (.py file). We can then import the module like other modules which we have been importing.\n","\n","The `SpacyPreprocessor` class is a text preprocessing transformer designed for natural language processing tasks. It extends scikit-learn's `BaseEstimator` and `TransformerMixin` to integrate smoothly with scikit-learn pipelines. The class provides a structured way to clean and preprocess text data using spaCy's powerful NLP tools, along with additional text cleaning steps.\n","\n","Here's a breakdown of its components and functionalities:\n","\n","1. **`__init__` (Constructor):**\n","   - The constructor initializes the transformer with a wide range of parameters that control how the text will be processed. These include options for batch processing, text cleaning (like removing stopwords, punctuation, URLs, emails, etc.), and text transformation (like lemmatization and stemming).\n","   - The `model` parameter specifies the spaCy language model to use.\n","   - A ValueError is raised if both `lemmatize` and `stemming` are set to True, as they are mutually exclusive text normalization techniques.\n","\n","2. **`BaseEstimator`:**\n","   - Inheriting from `BaseEstimator` makes the class compatible with other scikit-learn utilities. It enables functionalities like getting and setting parameters, and it's necessary for any custom estimator.\n","\n","3. **`TransformerMixin`:**\n","   - Inheriting from `TransformerMixin` requires the implementation of a `fit` method and a `transform` method. It also provides a `fit_transform` method that combines `fit` and `transform`.\n","   - This mixin is useful for creating custom transformers in scikit-learn pipelines.\n","\n","4. **`fit`:**\n","   - The `fit` method is a requirement for compatibility with scikit-learn's transformer interface. In this case, it's a placeholder that doesn't learn anything from the data (since text preprocessing doesn't involve fitting to a dataset). It simply returns `self`, allowing the transformer to be used in a pipeline.\n","\n","5. **`transform`:**\n","   - The `transform` method is where the actual text processing happens. It takes an input array of text data and performs the following steps:\n","     - Validates the input format.\n","     - Applies `basic_clean` to each text, handling HTML content and removing extra whitespace and line breaks.\n","     - If `basic_clean_only` is False, it further processes the text using the `spacy_preprocessor` method, which applies the spaCy pipeline and the specified text cleaning and normalization options.\n","     - Returns the preprocessed text data.\n","\n","In summary, `SpacyPreprocessor` is a customizable text preprocessing class that conforms to scikit-learn's interface, making it a flexible tool for integrating NLP preprocessing into machine learning pipelines. The use of `BaseEstimator` and `TransformerMixin` ensures compatibility and ease of use with scikit-learn, while the `fit` and `transform` methods provide the standard interface for a transformer in this ecosystem."],"metadata":{"id":"HuOqyIyt1Eun"}},{"cell_type":"code","source":["from sklearn.base import BaseEstimator, TransformerMixin\n","from bs4 import BeautifulSoup\n","import re\n","import spacy\n","import numpy as np\n","from nltk.stem.porter import PorterStemmer\n","import os\n","\n","class SpacyPreprocessor(BaseEstimator, TransformerMixin):\n","\n","    def __init__(self, model, *, batch_size = 64, lemmatize=True, lower=True, remove_stop=True,\n","                remove_punct=True, remove_email=True, remove_url=True, remove_num=False, stemming = False,\n","                add_user_mention_prefix=True, remove_hashtag_prefix=False, basic_clean_only=False):\n","\n","        self.model = model\n","        self.batch_size = batch_size\n","        self.remove_stop = remove_stop\n","        self.remove_punct = remove_punct\n","        self.remove_num = remove_num\n","        self.remove_url = remove_url\n","        self.remove_email = remove_email\n","        self.lower = lower\n","        self.add_user_mention_prefix = add_user_mention_prefix\n","        self.remove_hashtag_prefix = remove_hashtag_prefix\n","        self.basic_clean_only = basic_clean_only\n","\n","        if lemmatize and stemming:\n","            raise ValueError(\"Only one of 'lemmatize' and 'stemming' can be True.\")\n","\n","        # Validate basic_clean_only option\n","        if self.basic_clean_only and (lemmatize or lower or remove_stop or remove_punct or remove_num or stemming or\n","                                      add_user_mention_prefix or remove_hashtag_prefix):\n","            raise ValueError(\"If 'basic_clean_only' is set to True, other processing options must be set to False.\")\n","\n","        # Assign lemmatize and stemming\n","\n","        self.lemmatize = lemmatize\n","        self.stemming = stemming\n","\n","    def basic_clean(self, text):\n","        soup = BeautifulSoup(text, \"html.parser\")\n","        text = soup.get_text()\n","        text = re.sub(r'[\\n\\r]', ' ', text)\n","        return text.strip()\n","\n","    def spacy_preprocessor(self, texts):\n","        final_result = []\n","        nlp = spacy.load(self.model)\n","\n","        # Disable unnecessary pipelines in spaCy model\n","        if self.lemmatize:\n","            # Disable parser and named entity recognition\n","            disabled_pipes = ['parser', 'ner']\n","        else:\n","            # Disable tagger, parser, attribute ruler, lemmatizer and named entity recognition\n","            disabled_pipes = ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n","\n","        with nlp.select_pipes(disable=disabled_pipes):\n","          # Modify tokenizer behavior based on user_mention_prefix and hashtag_prefix settings\n","          if self.add_user_mention_prefix or self.remove_hashtag_prefix:\n","              prefixes = list(nlp.Defaults.prefixes)\n","              if self.add_user_mention_prefix:\n","                  prefixes += ['@']  # Treat '@' as a separate token\n","              if self.remove_hashtag_prefix:\n","                  prefixes.remove(r'#')  # Don't separate '#' from the following text\n","              prefix_regex = spacy.util.compile_prefix_regex(prefixes)\n","              nlp.tokenizer.prefix_search = prefix_regex.search\n","\n","          # Process text data in parallel using spaCy's nlp.pipe()\n","          for doc in nlp.pipe(texts, batch_size=self.batch_size):\n","              filtered_tokens = []\n","              for token in doc:\n","                  # Check if token should be removed based on specified filters\n","                  if self.remove_stop and token.is_stop:\n","                      continue\n","                  if self.remove_punct and token.is_punct:\n","                      continue\n","                  if self.remove_num and token.like_num:\n","                      continue\n","                  if self.remove_url and token.like_url:\n","                      continue\n","                  if self.remove_email and token.like_email:\n","                      continue\n","\n","                  # Append the token's text, lemma, or stemmed form to the filtered_tokens list\n","                  if self.lemmatize:\n","                      filtered_tokens.append(token.lemma_)\n","                  elif self.stemming:\n","                      filtered_tokens.append(PorterStemmer().stem(token.text))\n","                  else:\n","                      filtered_tokens.append(token.text)\n","\n","              # Join the tokens and apply lowercasing if specified\n","              text = ' '.join(filtered_tokens)\n","              if self.lower:\n","                  text = text.lower()\n","              final_result.append(text.strip())\n","\n","        return final_result\n","\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X, y=None):\n","        try:\n","            if not isinstance(X, (list, np.ndarray)):\n","                raise TypeError(f'Expected list or numpy array, got {type(X)}')\n","\n","            x_clean = [self.basic_clean(text).encode('utf-8', 'ignore').decode() for text in X]\n","\n","            # Check if only basic cleaning is required\n","            if self.basic_clean_only:\n","                return x_clean  # Return the list of basic-cleaned texts\n","\n","            x_clean_final = self.spacy_preprocessor(x_clean)\n","            return x_clean_final\n","\n","        except Exception as error:\n","            print(f'An exception occurred: {repr(error)}')\n"],"metadata":{"id":"AOTSX3QKwQhp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import spacy pre-processor from custom module\n","preprocessor = SpacyPreprocessor(model='en_core_web_sm', batch_size=64, lemmatize=False, lower=False,\n","                                    remove_stop=False, remove_punct=False, remove_email=False,\n","                                    remove_url=False, remove_num=False, stemming=False,\n","                                    add_user_mention_prefix=False, remove_hashtag_prefix=False, basic_clean_only=True)"],"metadata":{"id":"hXSpwrdFzUjs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cleaned_text = preprocessor.fit_transform(df['Reviews'].values)\n","\n","for item in cleaned_text:\n","    print()\n","    pprint(item, width = 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M2J9EjUUznTj","executionInfo":{"status":"ok","timestamp":1706432373508,"user_tz":360,"elapsed":180,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"7e366081-3690-4f03-a358-cef679e97c2f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","('New version of operation system is iOS 11. It is better than iOS 9. The new '\n"," 'version of iPhone X seems cool.')\n","\n","('The video of iphone x released. I liked iOS 9 but I like iOS 11 more. You '\n"," 'may not like my like @Tech_Guru #Iphone #IOS harpreet@utdallas.edu  '\n"," 'https://jindal.utdallas.edu/')\n","\n","('The concept of regular expressions began in the 1950s, when the American '\n"," 'mathematician Stephen Cole Kleene formalized the description of a regular '\n"," 'language. They came into common use with Unix text-processing utilities. '\n"," 'Different syntaxes for writing regular expressions have existed since the '\n"," '1980s, one being the POSIX standard and another, widely used, being the Perl '\n"," 'syntax. Regular expressions are used in search engines, search and replace '\n"," 'dialogs of word processors and text editors, in text processing utilities '\n"," 'such as sed and AWK and in lexical analysis. Many programming languages '\n"," 'provide regex capabilities either built-in or via libraries, as it has uses '\n"," 'in many situations.')\n"]}]},{"cell_type":"markdown","source":["# <font color = 'dodgerblue'>**Feature Extraction**"],"metadata":{"id":"HdzJxD936Zaq"}},{"cell_type":"markdown","source":["## <font color = 'dodgerblue'> **Understand the extraction of POS (Part Of Speech) related features.**"],"metadata":{"id":"U9TfdmapTipw"}},{"cell_type":"code","source":["noun_count = [] # create a list to store the noun count for each document\n","aux_count = [] # create a list to store the auxiliary verb count for each document\n","verb_count = [] # create a list to store the verb count for each document\n","adj_count =[] # create a list to store the adjective count for each document\n","\n","# disable lemmatizer and named entity recognizer\n","disabled_pipes =  ['lemmatizer', 'ner']\n","\n","# iterate over the documents in the dataframe using the spacy pipe method\n","with nlp.select_pipes(disable=disabled_pipes):\n","  for doc in nlp.pipe(cleaned_text, batch_size=3, n_process=1):\n","\n","      # find all nouns and proper nouns in the document and store in a list\n","      nouns = [token.text for token in doc if (token.pos_ in [\"NOUN\",\"PROPN\"])]\n","\n","      # find all auxiliary verbs in the document and store in a list\n","      auxs =  [token.text for token in doc if (token.pos_ in [\"AUX\"])]\n","\n","      # find all verbs in the document and store in a list\n","      verbs =  [token.text for token in doc if (token.pos_ in [\"VERB\"])]\n","      print(verbs)\n","\n","      # find all adjectives in the document and store in a list\n","      adjectives =  [token.text for token in doc if (token.pos_ in [\"ADJ\"])]\n","\n","      # store the count of nouns in the noun_count list\n","      noun_count.append(len(nouns))\n","\n","      # store the count of auxiliary verbs in the aux_count list\n","      aux_count.append(len(auxs))\n","\n","      # store the count of verbs in the verb_count list\n","      verb_count.append(len(verbs))\n","\n","      # store the count of adjectives in the adj_count list\n","      adj_count.append(len(adjectives))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CzYF4yIN3b57","executionInfo":{"status":"ok","timestamp":1706432381856,"user_tz":360,"elapsed":186,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"e82dd887-7ae8-4a64-a641-ceceaa03b48e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['seems']\n","['released', 'liked', 'like', 'like']\n","['began', 'formalized', 'came', 'processing', 'writing', 'existed', 'used', 'replace', 'sed', 'provide', 'built']\n"]}]},{"cell_type":"code","source":["noun_count, verb_count, aux_count, adj_count"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7XC-vq9Q6gGk","executionInfo":{"status":"ok","timestamp":1706432385669,"user_tz":360,"elapsed":312,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"4545c975-1e19-4f5a-edf2-b43d9d928613"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["([6, 8, 40], [1, 4, 11], [2, 1, 5], [4, 1, 12])"]},"metadata":{},"execution_count":71}]},{"cell_type":"markdown","source":["## <font color = 'dodgerblue'> **Understand the extraction of Text Descriptive Features**"],"metadata":{"id":"Dlj7xwQAWq2E"}},{"cell_type":"code","source":["nlp = spacy.load('en_core_web_sm')\n","disabled_pipes = ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n","with nlp.select_pipes(disable=disabled_pipes):\n","  if not nlp.has_pipe('sentencizer'):\n","    nlp.add_pipe('sentencizer')\n","\n","    list_count_words = []\n","    list_count_characters = []\n","    list_count_characters_no_space = []\n","    list_avg_word_length = []\n","    list_count_digits = []\n","    list_count_numbers = []\n","    list_count_sentences = []\n","\n","    for doc in nlp.pipe(cleaned_text, batch_size=3, n_process=1):\n","        # Count words (tokens)\n","        count_word = len([token.text for token in doc if not token.is_punct])\n","\n","        # Count all characters (including spaces)\n","        count_char = len(doc.text)\n","\n","        # Count characters without spaces\n","        count_char_no_space = len(doc.text_with_ws.replace(' ', ''))\n","\n","        # Count characters without spaces and punctuation\n","        # count_char_no_punct = sum(len(token.text) for token in doc if not token.is_punct)\n","\n","\n","        # Calculate average word length\n","        avg_word_length = count_char_no_space / (count_word + 1)\n","\n","        # Count numbers (consecutive digits)\n","        count_numbers = len([token for token in doc if token.is_digit])\n","\n","        # Count sentences\n","        count_sentences = len(list(doc.sents))\n","\n","        list_count_words.append(count_word)\n","        list_count_characters.append(count_char)\n","        list_count_characters_no_space.append(count_char_no_space)\n","        list_avg_word_length.append(avg_word_length)\n","        list_count_numbers.append(count_numbers)\n","        list_count_sentences.append(count_sentences)"],"metadata":{"id":"uEF4f8Qq5gKM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list_count_words,  list_count_characters, list_count_characters_no_space, list_avg_word_length, list_count_numbers, list_count_sentences"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zzem6gow7gG9","executionInfo":{"status":"ok","timestamp":1706434043260,"user_tz":360,"elapsed":473,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"b4e28903-86b5-4c9a-a3d0-e5b4af067487"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["([22, 28, 106],\n"," [107, 170, 687],\n"," [86, 143, 584],\n"," [3.739130434782609, 4.931034482758621, 5.457943925233645],\n"," [2, 2, 0],\n"," [3, 3, 5])"]},"metadata":{},"execution_count":79}]},{"cell_type":"code","source":["for item in cleaned_text:\n","    print()\n","    pprint(item, width = 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OGg_v0jG7tC8","executionInfo":{"status":"ok","timestamp":1706434059878,"user_tz":360,"elapsed":623,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"8114e74a-a943-4399-a3c9-b85819ae1e62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","('New version of operation system is iOS 11. It is better than iOS 9. The new '\n"," 'version of iPhone X seems cool.')\n","\n","('The video of iphone x released. I liked iOS 9 but I like iOS 11 more. You '\n"," 'may not like my like @Tech_Guru #Iphone #IOS harpreet@utdallas.edu  '\n"," 'https://jindal.utdallas.edu/')\n","\n","('The concept of regular expressions began in the 1950s, when the American '\n"," 'mathematician Stephen Cole Kleene formalized the description of a regular '\n"," 'language. They came into common use with Unix text-processing utilities. '\n"," 'Different syntaxes for writing regular expressions have existed since the '\n"," '1980s, one being the POSIX standard and another, widely used, being the Perl '\n"," 'syntax. Regular expressions are used in search engines, search and replace '\n"," 'dialogs of word processors and text editors, in text processing utilities '\n"," 'such as sed and AWK and in lexical analysis. Many programming languages '\n"," 'provide regex capabilities either built-in or via libraries, as it has uses '\n"," 'in many situations.')\n"]}]},{"cell_type":"markdown","source":["## <font color = 'dodgerblue'> **Count of Named Entitites**"],"metadata":{"id":"6g5SQKFHZpy2"}},{"cell_type":"code","source":["nlp = spacy.load('en_core_web_sm')\n","count_ner = []\n","disabled_pipes = ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer']\n","with nlp.select_pipes(disable=disabled_pipes):\n","# Disable the tok2vec, tagger, parser, attribute ruler, and lemmatizer pipelines for improved performance\n","  for doc in nlp.pipe(cleaned_text, batch_size=1000, n_process=-1):\n","      ner_text = [ent.text for ent in doc.ents]\n","      print(ner_text)\n","      ner_labels = [ent.label_ for ent in doc.ents]\n","      print(ner_labels)\n","      count_ner.append(len(ner_labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YN7pr8Jo7ysE","executionInfo":{"status":"ok","timestamp":1706434181450,"user_tz":360,"elapsed":1138,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"e4fcee40-00d6-4172-9972-b2351c40d224"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['11', '9']\n","['CARDINAL', 'CARDINAL']\n","['9', '11', 'IOS']\n","['CARDINAL', 'CARDINAL', 'ORG']\n","['the 1950s', 'American', 'Stephen Cole', 'the 1980s', 'one', 'POSIX', 'Perl', 'AWK']\n","['DATE', 'NORP', 'PERSON', 'DATE', 'CARDINAL', 'ORG', 'ORG', 'ORG']\n"]}]},{"cell_type":"code","source":["count_ner"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JhLpEe8bFtA2","executionInfo":{"status":"ok","timestamp":1706434219321,"user_tz":360,"elapsed":226,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"ae3e7ec9-dbe1-4c3b-a02e-5fe1e81b5f6c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2, 3, 8]"]},"metadata":{},"execution_count":82}]},{"cell_type":"markdown","source":["## <font color = 'dodgerblue'> **Create a custom class for manual feature extraction**"],"metadata":{"id":"VS-OlaEcN3XN"}},{"cell_type":"code","source":["from sklearn.base import TransformerMixin, BaseEstimator\n","import numpy as np\n","import spacy\n","import re\n","import sys\n","import os\n","from pathlib import Path\n","\n","\n","class ManualFeatures(TransformerMixin, BaseEstimator):\n","\n","\n","    def __init__(self, spacy_model, batch_size = 64, pos_features = True, ner_features = True, text_descriptive_features = True):\n","\n","        self.spacy_model = spacy_model\n","        self.batch_size = batch_size\n","        self.pos_features = pos_features\n","        self.ner_features = ner_features\n","        self.text_descriptive_features = text_descriptive_features\n","\n","    def get_cores(self):\n","        \"\"\"\n","        Get the number of CPU cores to use in parallel processing.\n","        \"\"\"\n","        # Get the number of CPU cores available on the system.\n","        num_cores = os.cpu_count()\n","        if num_cores < 3:\n","            use_cores = 1\n","        else:\n","            use_cores = num_cores // 2 + 1\n","        return num_cores\n","\n","    def get_pos_features(self, cleaned_text):\n","\n","        nlp = spacy.load(self.spacy_model)\n","        noun_count = []\n","        aux_count = []\n","        verb_count = []\n","        adj_count =[]\n","\n","        # Disable the lemmatizer and NER pipelines for improved performance\n","        disabled_pipes = ['lemmatizer', 'ner']\n","        with nlp.select_pipes(disable=disabled_pipes):\n","            n_process = self.get_cores()\n","            for doc in nlp.pipe(cleaned_text, batch_size=self.batch_size, n_process=n_process):\n","                # Extract nouns, auxiliaries, verbs, and adjectives from the document\n","                nouns = [token.text for token in doc if token.pos_ in [\"NOUN\",\"PROPN\"]]\n","                auxs =  [token.text for token in doc if token.pos_ in [\"AUX\"]]\n","                verbs =  [token.text for token in doc if token.pos_ in [\"VERB\"]]\n","                adjectives =  [token.text for token in doc if token.pos_ in [\"ADJ\"]]\n","\n","                # Store the count of each type of word in separate lists\n","                noun_count.append(len(nouns))\n","                aux_count.append(len(auxs))\n","                verb_count.append(len(verbs))\n","                adj_count.append(len(adjectives))\n","\n","        # Stack the count lists vertically to form a 2D numpy array\n","        return np.transpose(np.vstack((noun_count, aux_count, verb_count, adj_count)))\n","\n","\n","\n","    def get_ner_features(self, cleaned_text):\n","        nlp = spacy.load(self.spacy_model)\n","        count_ner = []\n","\n","        # Disable the tok2vec, tagger, parser, attribute ruler, and lemmatizer pipelines for improved performance\n","        disabled_pipes = ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer']\n","        with nlp.select_pipes(disable=disabled_pipes):\n","            n_process = self.get_cores()\n","            for doc in nlp.pipe(cleaned_text, batch_size=self.batch_size, n_process=n_process):\n","                ners = [ent.label_ for ent in doc.ents]\n","                count_ner.append(len(ners))\n","\n","        # Convert the list of NER counts to a 2D numpy array\n","        return np.array(count_ner).reshape(-1, 1)\n","\n","\n","    def get_text_descriptive_features(self, cleaned_text):\n","        list_count_words = []\n","        list_count_characters = []\n","        list_count_characters_no_space = []\n","        list_avg_word_length = []\n","        list_count_digits = []\n","        list_count_numbers = []\n","        list_count_sentences = []\n","\n","        nlp = spacy.load(self.spacy_model)\n","        disabled_pipes = ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n","        with nlp.select_pipes(disable=disabled_pipes):\n","            if not nlp.has_pipe('sentencizer'):\n","                nlp.add_pipe('sentencizer')\n","            n_process = self.get_cores()\n","            for doc in nlp.pipe(cleaned_text, batch_size=self.batch_size, n_process=n_process):\n","                count_word = len([token for token in doc if not token.is_punct])\n","                count_char = len(doc.text)\n","                count_char_no_space = len(doc.text_with_ws.replace(' ', ''))\n","                avg_word_length = count_char_no_space / (count_word + 1)\n","                count_numbers = len([token for token in doc if token.is_digit])\n","                count_sentences = len(list(doc.sents))\n","\n","                list_count_words.append(count_word)\n","                list_count_characters.append(count_char)\n","                list_count_characters_no_space.append(count_char_no_space)\n","                list_avg_word_length.append(avg_word_length)\n","                list_count_numbers.append(count_numbers)\n","                list_count_sentences.append(count_sentences)\n","\n","        text_descriptive_features = np.vstack((list_count_words, list_count_characters, list_count_characters_no_space, list_avg_word_length,\n","                                    list_count_numbers, list_count_sentences))\n","        return np.transpose(text_descriptive_features)\n","\n","\n","    def fit(self, X, y=None):\n","\n","        return self\n","\n","\n","    def transform(self, X, y=None):\n","\n","        try:\n","            # Check if the input data is a list or numpy array\n","            if not isinstance(X, (list, np.ndarray)):\n","                raise TypeError(f\"Expected list or numpy array, got {type(X)}\")\n","\n","\n","            feature_names = []\n","\n","            if self.text_descriptive_features:\n","                text_descriptive_features = self.get_text_descriptive_features(X)\n","                feature_names.extend(['count_words', 'count_characters',\n","                                      'count_characters_no_space', 'avg_word_length',\n","                                      'count_numbers', 'count_sentences'])\n","            else:\n","                text_descriptive_features = np.empty(shape=(0, 0))\n","\n","            if self.pos_features:\n","                pos_features = self.get_pos_features(X)\n","                feature_names.extend(['noun_count', 'aux_count', 'verb_count', 'adj_count'])\n","            else:\n","                pos_features = np.empty(shape=(0, 0))\n","\n","            if self.ner_features:\n","                ner_features = self.get_ner_features(X)\n","                feature_names.extend(['ner'])\n","            else:\n","                ner_features = np.empty(shape=(0, 0))\n","\n","            # Stack the feature arrays horizontally to form a single 2D numpy array\n","            return np.hstack((text_descriptive_features, pos_features,  ner_features)), feature_names\n","\n","        except Exception as error:\n","            print(f'An exception occured: {repr(error)}')"],"metadata":{"id":"L3QfePsnJsBN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["featurizer = ManualFeatures(spacy_model='en_core_web_sm', batch_size =3)"],"metadata":{"id":"As7hbiB_NACF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_features, feature_names = featurizer.fit_transform(cleaned_text )"],"metadata":{"id":"18oNzjR5Nie2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(X_train_features, columns=feature_names )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"id":"yztZv1coN4lI","executionInfo":{"status":"ok","timestamp":1706434647697,"user_tz":360,"elapsed":254,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"bc93bb22-63d6-4e5c-efc0-0de5891395e7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   count_words  count_characters  count_characters_no_space  avg_word_length  \\\n","0         22.0             107.0                       86.0         3.739130   \n","1         28.0             170.0                      143.0         4.931034   \n","2        106.0             687.0                      584.0         5.457944   \n","\n","   count_numbers  count_sentences  noun_count  aux_count  verb_count  \\\n","0            2.0              3.0         6.0        2.0         1.0   \n","1            2.0              3.0         8.0        1.0         4.0   \n","2            0.0              5.0        40.0        5.0        11.0   \n","\n","   adj_count  ner  \n","0        4.0  2.0  \n","1        1.0  3.0  \n","2       12.0  8.0  "],"text/html":["\n","  <div id=\"df-29c4d2fd-8179-4587-8845-5c318fc90782\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count_words</th>\n","      <th>count_characters</th>\n","      <th>count_characters_no_space</th>\n","      <th>avg_word_length</th>\n","      <th>count_numbers</th>\n","      <th>count_sentences</th>\n","      <th>noun_count</th>\n","      <th>aux_count</th>\n","      <th>verb_count</th>\n","      <th>adj_count</th>\n","      <th>ner</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>22.0</td>\n","      <td>107.0</td>\n","      <td>86.0</td>\n","      <td>3.739130</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>6.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>28.0</td>\n","      <td>170.0</td>\n","      <td>143.0</td>\n","      <td>4.931034</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>8.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>106.0</td>\n","      <td>687.0</td>\n","      <td>584.0</td>\n","      <td>5.457944</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>40.0</td>\n","      <td>5.0</td>\n","      <td>11.0</td>\n","      <td>12.0</td>\n","      <td>8.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29c4d2fd-8179-4587-8845-5c318fc90782')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-29c4d2fd-8179-4587-8845-5c318fc90782 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-29c4d2fd-8179-4587-8845-5c318fc90782');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-85cb8f79-72f5-4807-bcc6-72885fc729b1\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-85cb8f79-72f5-4807-bcc6-72885fc729b1')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-85cb8f79-72f5-4807-bcc6-72885fc729b1 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":96}]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}