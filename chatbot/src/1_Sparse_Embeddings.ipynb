{"cells":[{"cell_type":"markdown","metadata":{"id":"bzV0Las4fygR"},"source":["# <font color = 'pickle'> **Bag of Words (Sparse Embeddings)**\n"]},{"cell_type":"markdown","metadata":{"id":"aEeEQp3Pf8Kl"},"source":["<font color = 'pickle' size =5 >**What is Bag of Words (BoW)?**</font>\n","\n","A **bag-of-words** is a representation of text that describes the occurrence of words within a document <font color ='indianred'>**disregarding grammar and word order**</font>. It involves two steps:\n","\n","    1. Create Vocabulary. Each word in vocabulary forms feature(independent variable) to represent document.\n","    2. Score words (based on frequency or occurrence) to create Vectors."]},{"cell_type":"markdown","metadata":{"id":"pmUenp0Q4YDU"},"source":["<font color = 'pickle' size =5> **Why do you need to learn Bag of Words?**</font>\n","\n","- Till now we have learnt how to pre-process the text data i.e clean the text data.\n","- Our final goal is to use text data in Machine Learning (ML) models. For example - we want to predict whether e-mail is a spam or not based on the text of the data.\n","- But ML models can understand only numbers. Therefore we need to convert text to vectors (numbers).\n","- The simple method of converting text to numbers is to use 'Bag of Words approach'\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LlYmu-CkHeQb"},"source":["<font color = 'pickle' size =5> **Why do you need Bag of Words in age of LLMs?** </font>\n","\n","<font color = 'indianred'> **Outstanding paper Award ACL 2023: Linear Classifier: An Often-Forgotten Baseline for Text Classification**</font>\n","\n","*Large-scale pre-trained language models such as BERT are popular solutions for text classification. Due to the superior performance of these advanced methods, nowadays, people often directly train them for a few epochs and deploy the obtained model. In this opinion paper, we point out that this way may only sometimes get satisfactory results. We argue the importance of running a simple baseline like linear classifiers on bag-of-words features along with advanced methods.*\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kzKYz9DZ4YDU"},"source":["## <font color = 'pickle' size =5 >**Learning Outcome** </font>\n","After completing this tutorial, you will know\n","\n","1. What the bag-of-words approach is and how you can use it to represent text data.\n","2. What are different techniques to prepare a vocabulary and score words.\n","3. How to implement 'Bag-of-words' approach in python using sklearn."]},{"cell_type":"markdown","metadata":{"id":"uTmQrTrVxCEh"},"source":["# <font color = 'pickle'> **Tutorial Overview**</font>\n"," - Generating Vocab\n"," - Generating vectors using Vocab\n","     - Binary Vectorizer\n","     - Count Vectorizer\n","     - tfidf Vectorizer\n","\n"," - Modifying Vocab\n"," - Example - IMDB Dataset\n"]},{"cell_type":"markdown","metadata":{"id":"xtZUZxRsjFOZ"},"source":["# <font color = 'pickle'> Import/install Libraries"]},{"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AqC3phq7vN33","executionInfo":{"status":"ok","timestamp":1706504876557,"user_tz":360,"elapsed":28,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"a7a3249a-469a-4b2c-e5bb-de50e8d4095c"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}]},{"cell_type":"markdown","source":["The code `%load_ext autoreload` and `%autoreload 2` in a Jupyter notebook enables the autoreload extension. This setup automatically reloads imported modules before executing code cells. Specifically, `%autoreload 2` ensures all modules are reloaded each time before execution, reflecting any changes made to the module files without needing to restart the notebook kernel."],"metadata":{"id":"mmRrU5ZDnfGh"}},{"cell_type":"code","execution_count":70,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1706505454838,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"T0nri2djPi_D"},"outputs":[],"source":["import sys\n","if 'google.colab' in str(get_ipython()):\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    !pip install -U nltk -qq\n","    !pip install -U spacy -qq\n","    !python -m spacy download en_core_web_sm -qq\n","\n","    basepath = '/content/drive/MyDrive/data'\n","    sys.path.append('/content/drive/MyDrive/data/custom-functions')\n","else:\n","    basepath = '/home/harpreet/Insync/google_drive_shaannoor/data'\n","    sys.path.append(\n","    '/home/harpreet/Insync/google_drive_shaannoor/data/custom-functions')\n"]},{"cell_type":"markdown","source":["**Code Explanation**\n","\n","1. **Environment Detection**:\n","The code begins by checking the environment in which it's running. This is done using the statement `if 'google.colab' in str(get_ipython()):`. If this condition is true, it means the code is running in Google Colab, an online interactive development environment. If not, the code assumes it's running in a local environment (like your personal computer).\n","\n","2. **Setup for Google Colab:**\n","When running in Google Colab, the code performs the following actions:\n","\n","- *Mount Google Drive*: `from google.colab import drive` and `drive.mount('/content/drive')` are used to mount the user's Google Drive to the Colab environment, enabling access to files stored there.\n","\n","- *Install Necessary Libraries*: The code installs or updates specific Python libraries (`nltk` and `spacy`) silently without producing unnecessary output (`-qq`).\n","\n","- *Download spaCy Model*: `!python -m spacy download en_core_web_sm -qq` downloads the English language model for spaCy, necessary for NLP tasks.\n","\n","- *Set Basepath and Update sys.path*:\n","  - `basepath` is set to a path in Google Drive where data or relevant files are stored.\n","  - `sys.path.append('/content/drive/MyDrive/data/custom-functions')` adds a directory to the Python search path. This allows Python to import and use custom functions located in that directory in the Colab notebook, promoting modularity and code reusability.\n","\n","3. **Setup for Local Environment:**\n","If the code is not running in Google Colab, it's presumed to be in a local environment. Here, the setup is slightly different:\n","\n","- *Set Basepath*: `basepath` is set to a specific directory on the local machine. This is where the code will look for data files or other resources.\n","\n","- *Update sys.path for Custom Functions*:\n","  - `sys.path.append('/home/harpreet/Insync/google_drive_shaannoor/data/custom-functions')` adds a local directory to the Python search path. Similar to the Colab setup, this step allows the local Python environment to import and use custom functions from the specified directory."],"metadata":{"id":"Jtct-jwpoYBB"}},{"cell_type":"code","execution_count":69,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T20:24:56.540071Z","iopub.status.busy":"2021-09-11T20:24:56.539257Z","iopub.status.idle":"2021-09-11T20:24:56.544797Z","shell.execute_reply":"2021-09-11T20:24:56.544485Z","shell.execute_reply.started":"2021-09-11T20:24:56.539974Z"},"executionInfo":{"elapsed":106,"status":"ok","timestamp":1706505109769,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"NBkcfK5xQkjv","outputId":"f7989af0-8f01-4024-c3d0-b4955464d264","tags":[]},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     /home/harpreet/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["# Import required libraries\n","import pandas as pd  # For data manipulation and analysis\n","import numpy as np   # For numerical operations\n","import spacy         # For NLP preprocessing\n","\n","# Import required nltk packages\n","import nltk\n","nltk.download('stopwords')  # Download the stopwords corpus\n","from nltk.corpus import stopwords as nltk_stopwords  # Stopwords corpus\n","\n","# Import tweet tokenizer from nltk\n","from nltk.tokenize import TweetTokenizer\n","\n","# Import CountVectorizer and TfidfVectorizer from scikit-learn\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","# Import pathlib for managing file paths\n","from pathlib import Path\n","\n","# import custom-preprocessor from python file\n","import CustomPreprocessorSpacy as cp"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1706490346600,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"t2_XksvXxCEj","outputId":"30442954-14ab-4a4d-ed26-250eb8275c5c","tags":[]},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'3.7.2'"]},"metadata":{},"execution_count":5}],"source":["spacy.__version__\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2021-09-11T20:24:59.919129Z","iopub.status.busy":"2021-09-11T20:24:59.918662Z","iopub.status.idle":"2021-09-11T20:25:00.179324Z","shell.execute_reply":"2021-09-11T20:25:00.178886Z","shell.execute_reply.started":"2021-09-11T20:24:59.919114Z"},"id":"PmZmCvuQQezx","tags":[],"executionInfo":{"status":"ok","timestamp":1706490350164,"user_tz":360,"elapsed":730,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["# load spacy model\n","nlp = spacy.load('en_core_web_sm')\n"]},{"cell_type":"markdown","metadata":{"id":"RTlvVAjziFYm"},"source":["## <font color = 'pickle'> **Generating Vocab**"]},{"cell_type":"markdown","metadata":{"id":"whv4QgKPB0Gv"},"source":["###  <font color = 'pickle'> **Dummy Corpus**"]},{"cell_type":"code","execution_count":85,"metadata":{"execution":{"iopub.execute_input":"2021-09-11T20:25:03.583435Z","iopub.status.busy":"2021-09-11T20:25:03.582835Z","iopub.status.idle":"2021-09-11T20:25:03.590454Z","shell.execute_reply":"2021-09-11T20:25:03.589100Z","shell.execute_reply.started":"2021-09-11T20:25:03.583366Z"},"id":"YzXnofA3nF3W","tags":[],"executionInfo":{"status":"ok","timestamp":1706506222115,"user_tz":360,"elapsed":42,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["# Dummy corpus\n","Corpus = [\"Count Vectorizer - for this vectorizer, scoring is done based on frequency. For this vectorizer frequency is key. @vectorizer #frequency @frequency, doesn’t\",\n","          \"tfidf vectorizer - for this vectorizer, scoring is done based on tfidf,  higher tfidf higher score #tfidf @vectorizer \"  ,\n","          \"Binary vectorizer - for this vectorizer, scoring is done based on presence of word. For this vectorizer, dummy is key #dummy @dummy @vectorizer \"]\n"]},{"cell_type":"markdown","metadata":{"id":"lyhgUzi5CNgg"},"source":["### <font color = 'pickle'>**Create an instance of Vectorizer**"]},{"cell_type":"code","execution_count":86,"metadata":{"execution":{"iopub.execute_input":"2021-09-11T20:25:17.155888Z","iopub.status.busy":"2021-09-11T20:25:17.155757Z","iopub.status.idle":"2021-09-11T20:25:17.158014Z","shell.execute_reply":"2021-09-11T20:25:17.157740Z","shell.execute_reply.started":"2021-09-11T20:25:17.155874Z"},"id":"J3ZVqkIHCePq","tags":[],"executionInfo":{"status":"ok","timestamp":1706506223941,"user_tz":360,"elapsed":26,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["vectorizer = CountVectorizer()\n"]},{"cell_type":"markdown","metadata":{"id":"pIo86goBv6sg"},"source":["The above code creates an instance of the `CountVectorizer` class from the `sklearn.feature_extraction.text module`. This class is used to convert a collection of text documents to a matrix of token counts.\n","\n","It accomplishes this by\n","  1. tokenizing the input text\n","  2. creating a vocabulary of all the tokens found in the text\n","  3. encoding the text as a matrix of token counts based on this vocabulary.\n","\n","The created instance vectorizer can then be used to fit the text data to the vocabulary and generate the token count matrix."]},{"cell_type":"code","execution_count":87,"metadata":{"id":"k5fgQ7Kb_9j3","executionInfo":{"status":"ok","timestamp":1706506226021,"user_tz":360,"elapsed":68,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["CountVectorizer??\n"]},{"cell_type":"markdown","metadata":{"id":"u6kX6-YQCSuq"},"source":["### <font color = 'pickle'>**Fit Vectorizer on corpus to generate vocab**"]},{"cell_type":"code","execution_count":88,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1706506230806,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"fnehlHO7RQKx","outputId":"58c651c8-4b80-4ef4-d76b-6c58e76ff7e0","tags":[]},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CountVectorizer()"],"text/html":["<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":88}],"source":["# Fit the vectorizer on corpus\n","vectorizer.fit(Corpus)\n"]},{"cell_type":"markdown","metadata":{"id":"R-8UMn_FSNCF"},"source":["<font color = 'indianred'>**Vectorizer().fit() does the following**:\n","- lowercases your text\n","- uses utf-8 encoding\n","- performs tokenization (converts raw text to smaller units of text)\n","- uses word level tokenization (meaning each word is treated as a separate token) and  ignores single characters during tokenization ( words like ‘a’ and ‘I’ are removed)\n","- By default, the regular expression that is used to split the text and create tokens is : `\"\\b\\w\\w+\\b\"`.\n","  - This means it finds all sequences of characters that consist of at least two letters or numbers(\\w) and that are separated by word boundaries (\\b).\n","  - It does not find single-letter words, and it splits up contractions like “doesn’t” or “bit.ly”, but it matches “h8ter” as a single word.\n","- The CountVectorizer then converts all words to lowercasecharacters, so that “soon”, “Soon”, and “sOon” all correspond to the same token (and therefore feature).\n","- It then creates a dictionary of unique words.\n","- The set of unique words is used as features in the CountVectorizer."]},{"cell_type":"code","execution_count":89,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1706506233001,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"ayvQssZBRTNr","outputId":"89a4ff52-0e8d-40f7-9ec0-4636531be557","tags":[]},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'count': 2,\n"," 'vectorizer': 18,\n"," 'for': 6,\n"," 'this': 17,\n"," 'scoring': 15,\n"," 'is': 9,\n"," 'done': 4,\n"," 'based': 0,\n"," 'on': 12,\n"," 'frequency': 7,\n"," 'key': 10,\n"," 'doesn': 3,\n"," 'tfidf': 16,\n"," 'higher': 8,\n"," 'score': 14,\n"," 'binary': 1,\n"," 'presence': 13,\n"," 'of': 11,\n"," 'word': 19,\n"," 'dummy': 5}"]},"metadata":{},"execution_count":89}],"source":["# Display the mapping of terms to feature indices created by the vectorizer\n","vectorizer.vocabulary_\n"]},{"cell_type":"markdown","source":["We can use `vectorizer.get_feature_names_out()` to obtain the list of unique words that the CountVectorizer has identified from the corpus"],"metadata":{"id":"WAsQM4qer-nx"}},{"cell_type":"code","execution_count":90,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1706506236763,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"4Z1haa0zRkXM","outputId":"d641ab76-3df8-488c-f5c0-955d7dbb0299","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["['based' 'binary' 'count' 'doesn' 'done' 'dummy' 'for' 'frequency'\n"," 'higher' 'is' 'key' 'of' 'on' 'presence' 'score' 'scoring' 'tfidf' 'this'\n"," 'vectorizer' 'word']\n","\n","total number of unique features (words): 20\n"]}],"source":["# Retrieve the list of unique words (tokens) that the CountVectorizer identified in the corpus.\n","# These words serve as features (columns) in the vectorized output.\n","features = vectorizer.get_feature_names_out()\n","print(features)\n","\n","# Print the total number of unique features (words) identified in the corpus\n","print('\\ntotal number of unique features (words):', len(features))\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mP8odLcIC66i"},"source":["## <font color = 'pickle'>**Generate Vectors using Vocab**"]},{"cell_type":"markdown","metadata":{"id":"9DwforYaDNsY"},"source":["### <font color = 'pickle'>**Binary Vectorizer**"]},{"cell_type":"code","execution_count":91,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1706506240137,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"5x5YgOMdDiYC","outputId":"eadcef2a-0ea2-468a-dde6-2ef73c037241","tags":[]},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CountVectorizer(binary=True)"],"text/html":["<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(binary=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(binary=True)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":91}],"source":["binary_vectorizer = CountVectorizer(binary=True)\n","binary_vectorizer.fit(Corpus)\n"]},{"cell_type":"markdown","metadata":{"id":"X4k8F2UTE4SU"},"source":["- We can now call transform() method to transform documents in our corpus to vectors.\n","- <font color = 'dodgerblue'>**Each document**</font> will be represented by <font color = 'dodgerblue'>**vector of length equal to len(dictionary)**.</font>\n","- The vectors are stored in the form of a <font color = 'dodgerblue'>**sparse matrix**.</font>\n","- We can use <font color = 'dodgerblue'>**toarray()**</font> function to get complete matrix.\n","- Number of columns represent the number of features (len(vocab)).\n","- Number of rows represent the number the documents in a corpus.\n","- <font color = 'dodgerblue'>**For each row, the numbers displayed are 0 or 1 - indicating absence or presence of a word in a document.**"]},{"cell_type":"code","execution_count":92,"metadata":{"execution":{"iopub.execute_input":"2021-09-11T20:25:24.312760Z","iopub.status.busy":"2021-09-11T20:25:24.312601Z","iopub.status.idle":"2021-09-11T20:25:24.315437Z","shell.execute_reply":"2021-09-11T20:25:24.315041Z","shell.execute_reply.started":"2021-09-11T20:25:24.312747Z"},"id":"DzwuRnsInKHa","tags":[],"executionInfo":{"status":"ok","timestamp":1706506242654,"user_tz":360,"elapsed":25,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["binary_vectors = binary_vectorizer.transform(Corpus)"]},{"cell_type":"code","execution_count":93,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1706506243314,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"kfXmM7P3nVYy","outputId":"bbafb524-447a-4602-c50e-0b22c6268e6f","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["vectors in sparse format\n","  (0, 0)\t1\n","  (0, 2)\t1\n","  (0, 3)\t1\n","  (0, 4)\t1\n","  (0, 6)\t1\n","  (0, 7)\t1\n","  (0, 9)\t1\n","  (0, 10)\t1\n","  (0, 12)\t1\n","  (0, 15)\t1\n","  (0, 17)\t1\n","  (0, 18)\t1\n","  (1, 0)\t1\n","  (1, 4)\t1\n","  (1, 6)\t1\n","  (1, 8)\t1\n","  (1, 9)\t1\n","  (1, 12)\t1\n","  (1, 14)\t1\n","  (1, 15)\t1\n","  (1, 16)\t1\n","  (1, 17)\t1\n","  (1, 18)\t1\n","  (2, 0)\t1\n","  (2, 1)\t1\n","  (2, 4)\t1\n","  (2, 5)\t1\n","  (2, 6)\t1\n","  (2, 9)\t1\n","  (2, 10)\t1\n","  (2, 11)\t1\n","  (2, 12)\t1\n","  (2, 13)\t1\n","  (2, 15)\t1\n","  (2, 17)\t1\n","  (2, 18)\t1\n","  (2, 19)\t1\n"]}],"source":["print(f'vectors in sparse format')\n","print(binary_vectors)\n"]},{"cell_type":"code","execution_count":94,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1706506245580,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"pr4aHet7Z3t_","outputId":"171feb19-c0ef-4c3d-f03b-672d4307461a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","binary vectors in array(dense) format\n","[[1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0]\n"," [1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0]\n"," [1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1]]\n","\n","The shape of the binary vectors is : (3, 20)\n"]}],"source":["print(f'\\nbinary vectors in array(dense) format')\n","print(binary_vectors.toarray())\n","print(\n","    f'\\nThe shape of the binary vectors is : {binary_vectors.toarray().shape}')\n"]},{"cell_type":"code","execution_count":95,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1706506247403,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"3otH8XphHQJX","outputId":"78b14186-eef5-4e3f-e91a-c79b678b0690"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   based  binary  count  doesn  done  dummy  for  frequency  higher  is  key   \n","0      1       0      1      1     1      0    1          1       0   1    1  \\\n","1      1       0      0      0     1      0    1          0       1   1    0   \n","2      1       1      0      0     1      1    1          0       0   1    1   \n","\n","   of  on  presence  score  scoring  tfidf  this  vectorizer  word  \n","0   0   1         0      0        1      0     1           1     0  \n","1   0   1         0      1        1      1     1           1     0  \n","2   1   1         1      0        1      0     1           1     1  "],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>based</th>\n","      <th>binary</th>\n","      <th>count</th>\n","      <th>doesn</th>\n","      <th>done</th>\n","      <th>dummy</th>\n","      <th>for</th>\n","      <th>frequency</th>\n","      <th>higher</th>\n","      <th>is</th>\n","      <th>key</th>\n","      <th>of</th>\n","      <th>on</th>\n","      <th>presence</th>\n","      <th>score</th>\n","      <th>scoring</th>\n","      <th>tfidf</th>\n","      <th>this</th>\n","      <th>vectorizer</th>\n","      <th>word</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{},"execution_count":95}],"source":["# create dataframe for better visualization\n","df_binary = pd.DataFrame(binary_vectors.toarray(), columns=features)\n","df_binary\n"]},{"cell_type":"markdown","metadata":{"id":"OAPPPILCATX-"},"source":["### <font color = 'pickle'>**Count Vectorizer**\n","-  The vectors are stored in the form of a sparse matrix.\n","- Number of columns represent the number of features (len(vocab))\n","- Number of rows represent the number the documents in a corpus\n","- Thus, each document is represented by a vector of size of length of vocab.\n","- For each row, <font color = 'dodgerblue'>**the numbers displayed are the number of times a particular word has occurred in the document.**"]},{"cell_type":"code","execution_count":96,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1706506269985,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"PuF8IkLUA8zp","outputId":"0a1b28c8-64e2-45c9-92b4-bc5d6de73571","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["count vectors in array (dense) format\n","\n","[[1 0 1 1 1 0 2 4 0 2 1 0 1 0 0 1 0 2 4 0]\n"," [1 0 0 0 1 0 1 0 2 1 0 0 1 0 1 1 4 1 3 0]\n"," [1 1 0 0 1 3 2 0 0 2 1 1 1 1 0 1 0 2 4 1]]\n","\n","The shape of the count vectors is : (3, 20)\n"]}],"source":["term_freq_vectorizer = CountVectorizer(binary=False)\n","# we can combine fit and transform steps into a single step using fit_transform()\n","count_vectors = term_freq_vectorizer.fit_transform(Corpus)\n","print(f'count vectors in array (dense) format\\n')\n","print(count_vectors.toarray())\n","print(f'\\nThe shape of the count vectors is : {count_vectors.toarray().shape}')\n"]},{"cell_type":"code","execution_count":98,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1706506275687,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"DLJCKNMVH0nm","outputId":"c67e93a7-4a73-44b1-a267-c91c77e109b5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   based  binary  count  doesn  done  dummy  for  frequency  higher  is  key   \n","0      1       0      1      1     1      0    2          4       0   2    1  \\\n","1      1       0      0      0     1      0    1          0       2   1    0   \n","2      1       1      0      0     1      3    2          0       0   2    1   \n","\n","   of  on  presence  score  scoring  tfidf  this  vectorizer  word  \n","0   0   1         0      0        1      0     2           4     0  \n","1   0   1         0      1        1      4     1           3     0  \n","2   1   1         1      0        1      0     2           4     1  "],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>based</th>\n","      <th>binary</th>\n","      <th>count</th>\n","      <th>doesn</th>\n","      <th>done</th>\n","      <th>dummy</th>\n","      <th>for</th>\n","      <th>frequency</th>\n","      <th>higher</th>\n","      <th>is</th>\n","      <th>key</th>\n","      <th>of</th>\n","      <th>on</th>\n","      <th>presence</th>\n","      <th>score</th>\n","      <th>scoring</th>\n","      <th>tfidf</th>\n","      <th>this</th>\n","      <th>vectorizer</th>\n","      <th>word</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{},"execution_count":98}],"source":["# create dataframe for better visualization\n","df_count = pd.DataFrame(count_vectors.toarray(),\n","                        columns=term_freq_vectorizer.get_feature_names_out())\n","df_count\n"]},{"cell_type":"markdown","metadata":{"id":"dVlJmMtmhCYM"},"source":["### <font color = 'pickle'>**tf-idf Vectorizer**</font>\n","\n","- One measure of how important a word is term frequency (tf) (how frequently a word occurs in a document). We examined term frequency in previous sections where we used CountVectorizer to get the freqency of each word.\n","- But there may be words in a document, that occur many times but these words also occur in all other documents as well.\n","- Therefore the word might not be a good representation of the document.\n","- We can account for this by  <font color = 'dodgerblue'>giving more importance to words that occur in fewer documents using inverse document frequency </font> ((# Number of documents) / (Number of documents containing the word)).\n","- This can be <font color = 'dodgerblue'>combined with term frequency</font> to calculate a term’s tf-idf (the two quantities multiplied together), the frequency of a term adjusted for how rarely it is used.\n","- The idea of tf-idf is to <font color = 'dodgerblue'>find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents.</font>\n","- tf-idf gives more weight to the the words that are important (i.e., occur more frequently) in a given document, but occur rarely in other documents."]},{"cell_type":"code","execution_count":99,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1706506280432,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"hmb0UrD_xCEo","outputId":"f995b22d-e940-4e23-bac3-8e9ac244867b","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["tfidf vectors in array (dense) format\n","\n","[[0.10829999 0.         0.18336782 0.18336782 0.10829999 0.\n","  0.21659998 0.73347128 0.         0.21659998 0.13945595 0.\n","  0.10829999 0.         0.         0.10829999 0.         0.21659998\n","  0.43319995 0.        ]\n"," [0.11455596 0.         0.         0.         0.11455596 0.\n","  0.11455596 0.         0.3879202  0.11455596 0.         0.\n","  0.11455596 0.         0.1939601  0.11455596 0.77584039 0.11455596\n","  0.34366788 0.        ]\n"," [0.11874019 0.20104462 0.         0.         0.11874019 0.60313387\n","  0.23748039 0.         0.         0.23748039 0.15289962 0.20104462\n","  0.11874019 0.20104462 0.         0.11874019 0.         0.23748039\n","  0.47496077 0.20104462]]\n","\n","The shape of the tfidf vectors is : (3, 20)\n"]}],"source":["tfidf_vectorizer = TfidfVectorizer()\n","# we can combine fit and transform steps into a single step using fit_transform()\n","tfidf_vectors = tfidf_vectorizer.fit_transform(Corpus)\n","print(f'tfidf vectors in array (dense) format\\n')\n","print(tfidf_vectors.toarray())\n","print(f'\\nThe shape of the tfidf vectors is : {tfidf_vectors.toarray().shape}')\n"]},{"cell_type":"code","execution_count":100,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1706506283382,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"rz8SIqG_IZo1","outputId":"1408474d-2384-4b8e-992d-f673106e70c6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["    based  binary   count   doesn    done   dummy     for  frequency  higher   \n","0  0.1083   0.000  0.1834  0.1834  0.1083  0.0000  0.2166     0.7335  0.0000  \\\n","1  0.1146   0.000  0.0000  0.0000  0.1146  0.0000  0.1146     0.0000  0.3879   \n","2  0.1187   0.201  0.0000  0.0000  0.1187  0.6031  0.2375     0.0000  0.0000   \n","\n","       is     key     of      on  presence  score  scoring   tfidf    this   \n","0  0.2166  0.1395  0.000  0.1083     0.000  0.000   0.1083  0.0000  0.2166  \\\n","1  0.1146  0.0000  0.000  0.1146     0.000  0.194   0.1146  0.7758  0.1146   \n","2  0.2375  0.1529  0.201  0.1187     0.201  0.000   0.1187  0.0000  0.2375   \n","\n","   vectorizer   word  \n","0      0.4332  0.000  \n","1      0.3437  0.000  \n","2      0.4750  0.201  "],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>based</th>\n","      <th>binary</th>\n","      <th>count</th>\n","      <th>doesn</th>\n","      <th>done</th>\n","      <th>dummy</th>\n","      <th>for</th>\n","      <th>frequency</th>\n","      <th>higher</th>\n","      <th>is</th>\n","      <th>key</th>\n","      <th>of</th>\n","      <th>on</th>\n","      <th>presence</th>\n","      <th>score</th>\n","      <th>scoring</th>\n","      <th>tfidf</th>\n","      <th>this</th>\n","      <th>vectorizer</th>\n","      <th>word</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.1083</td>\n","      <td>0.000</td>\n","      <td>0.1834</td>\n","      <td>0.1834</td>\n","      <td>0.1083</td>\n","      <td>0.0000</td>\n","      <td>0.2166</td>\n","      <td>0.7335</td>\n","      <td>0.0000</td>\n","      <td>0.2166</td>\n","      <td>0.1395</td>\n","      <td>0.000</td>\n","      <td>0.1083</td>\n","      <td>0.000</td>\n","      <td>0.000</td>\n","      <td>0.1083</td>\n","      <td>0.0000</td>\n","      <td>0.2166</td>\n","      <td>0.4332</td>\n","      <td>0.000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.1146</td>\n","      <td>0.000</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0.1146</td>\n","      <td>0.0000</td>\n","      <td>0.1146</td>\n","      <td>0.0000</td>\n","      <td>0.3879</td>\n","      <td>0.1146</td>\n","      <td>0.0000</td>\n","      <td>0.000</td>\n","      <td>0.1146</td>\n","      <td>0.000</td>\n","      <td>0.194</td>\n","      <td>0.1146</td>\n","      <td>0.7758</td>\n","      <td>0.1146</td>\n","      <td>0.3437</td>\n","      <td>0.000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.1187</td>\n","      <td>0.201</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0.1187</td>\n","      <td>0.6031</td>\n","      <td>0.2375</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0.2375</td>\n","      <td>0.1529</td>\n","      <td>0.201</td>\n","      <td>0.1187</td>\n","      <td>0.201</td>\n","      <td>0.000</td>\n","      <td>0.1187</td>\n","      <td>0.0000</td>\n","      <td>0.2375</td>\n","      <td>0.4750</td>\n","      <td>0.201</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{},"execution_count":100}],"source":["# create dataframe for better visualization\n","df_tfidf = pd.DataFrame(tfidf_vectors.toarray(),\n","                        columns=tfidf_vectorizer.get_feature_names_out())\n","df_tfidf.round(4)\n"]},{"cell_type":"markdown","metadata":{"id":"nq8pvYnkxCEo"},"source":["### <font color = 'pickle'>**Undertstanding tfidf calculations**"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2021-09-11T20:59:13.152712Z","iopub.status.busy":"2021-09-11T20:59:13.152444Z","iopub.status.idle":"2021-09-11T20:59:13.158508Z","shell.execute_reply":"2021-09-11T20:59:13.157542Z","shell.execute_reply.started":"2021-09-11T20:59:13.152683Z"},"id":"wSoWEk3DxCEo","tags":[]},"source":["By default <br>\n","$\\text{tfidf}(w, d) = \\text{tf(w, d)} * \\text{idf(w)}$\n","<br>\n","$\\text{idf(w)} = \\log\\big(\\frac{N + 1}{N_w + 1}\\big) + 1$\n","<br><br>\n","if smooth_idf = False (default is True):\n","<br>\n","$\\text{idf(w)} = \\log\\big(\\frac{N }{N_w}\\big) + 1$\n","<br><br>\n","if sublinear_tfbool = True (default is False)\n","<br>\n","$\\text{tf(w, d)} = \\log(\\text{tf(w, d)} ) + 1$\n","\n","Here:<br>\n","- $\\text{tf}(w, d)$ is number of times word $w$ appears in document $d$\n","<br>\n","- $\\text{idf}(w)$ is inverse document frequency of word $w$\n","- $N$ is total number of documents\n","- $N_w$ is number of documents that contain word w"]},{"cell_type":"code","execution_count":101,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1706506294812,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"fuTBzPljxCEo","outputId":"46fee6b0-813f-4586-8397-c1722e91926b","tags":[]},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.        , 1.69314718, 1.69314718, 1.69314718, 1.        ,\n","       1.69314718, 1.        , 1.69314718, 1.69314718, 1.        ,\n","       1.28768207, 1.69314718, 1.        , 1.69314718, 1.69314718,\n","       1.        , 1.69314718, 1.        , 1.        , 1.69314718])"]},"metadata":{},"execution_count":101}],"source":["# Calculate inverse document frequency for each feature (word)\n","term_idf = tfidf_vectorizer.idf_\n","term_idf\n"]},{"cell_type":"code","execution_count":102,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1706506298053,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"GnwOVSQbJhVf","outputId":"ac3f63d5-8fae-4769-f1ed-6b1dea646eb7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   based  binary   count   doesn  done   dummy  for  frequency  higher   is   \n","0    1.0  1.6931  1.6931  1.6931   1.0  1.6931  1.0     1.6931  1.6931  1.0  \\\n","\n","      key      of   on  presence   score  scoring   tfidf  this  vectorizer   \n","0  1.2877  1.6931  1.0    1.6931  1.6931      1.0  1.6931   1.0         1.0  \\\n","\n","     word  \n","0  1.6931  "],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>based</th>\n","      <th>binary</th>\n","      <th>count</th>\n","      <th>doesn</th>\n","      <th>done</th>\n","      <th>dummy</th>\n","      <th>for</th>\n","      <th>frequency</th>\n","      <th>higher</th>\n","      <th>is</th>\n","      <th>key</th>\n","      <th>of</th>\n","      <th>on</th>\n","      <th>presence</th>\n","      <th>score</th>\n","      <th>scoring</th>\n","      <th>tfidf</th>\n","      <th>this</th>\n","      <th>vectorizer</th>\n","      <th>word</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>1.6931</td>\n","      <td>1.6931</td>\n","      <td>1.6931</td>\n","      <td>1.0</td>\n","      <td>1.6931</td>\n","      <td>1.0</td>\n","      <td>1.6931</td>\n","      <td>1.6931</td>\n","      <td>1.0</td>\n","      <td>1.2877</td>\n","      <td>1.6931</td>\n","      <td>1.0</td>\n","      <td>1.6931</td>\n","      <td>1.6931</td>\n","      <td>1.0</td>\n","      <td>1.6931</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.6931</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{},"execution_count":102}],"source":["# create dataframe for better visualization\n","df_idf = pd.DataFrame(term_idf, index=tfidf_vectorizer.get_feature_names_out())\n","df_idf.round(4).T\n"]},{"cell_type":"markdown","source":["For better understanding, we will now just look at the first document and get the idf and term frequencies for each word in the first document"],"metadata":{"id":"x_tR9I6wzEe9"}},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":676},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1706491209235,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"CGXv36hp6wIk","outputId":"1cf94aee-d416-493b-807a-f04e6d15f8d1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["      features  tf\n","0        based   1\n","1       binary   0\n","2        count   1\n","3        doesn   1\n","4         done   1\n","5        dummy   0\n","6          for   2\n","7    frequency   4\n","8       higher   0\n","9           is   2\n","10         key   1\n","11          of   0\n","12          on   1\n","13    presence   0\n","14       score   0\n","15     scoring   1\n","16       tfidf   0\n","17        this   2\n","18  vectorizer   4\n","19        word   0"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>features</th>\n","      <th>tf</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>based</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>binary</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>count</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>doesn</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>done</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>dummy</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>for</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>frequency</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>higher</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>is</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>key</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>of</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>on</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>presence</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>score</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>scoring</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>tfidf</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>this</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>vectorizer</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>word</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{},"execution_count":25}],"source":["# create dataframe for tf vectors for the first document\n","\n","# Create a dense numpy array from the sparse count vector for the first document\n","first_document_tf = count_vectors[0].toarray().ravel()\n","\n","# Get the feature names for the term frequency vectors\n","feature_names_tf = term_freq_vectorizer.get_feature_names_out()\n","\n","# Create a dataframe from the term frequency feature names and values\n","df_tf = pd.DataFrame({'features': feature_names_tf, 'tf': first_document_tf})\n","df_tf\n"]},{"cell_type":"markdown","metadata":{"id":"8MUwbSGf-C3V"},"source":["Note: The `toarray` method is used to convert the sparse matrix into a dense numpy array, and `ravel` is used to flatten the resulting 2-dimensional array into a 1-dimensional array. This is necessary because pandas dataframes expect 1-dimensional arrays as values for the columns."]},{"cell_type":"code","execution_count":103,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":676},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1706506321984,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"qmWgJ7RP-AgQ","outputId":"5b7b7d97-fe12-4a09-9533-954e1ccb7e0e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["      features  tf       idf  norm_tfidf\n","7    frequency   4  1.693147    0.733471\n","18  vectorizer   4  1.000000    0.433200\n","17        this   2  1.000000    0.216600\n","6          for   2  1.000000    0.216600\n","9           is   2  1.000000    0.216600\n","2        count   1  1.693147    0.183368\n","3        doesn   1  1.693147    0.183368\n","10         key   1  1.287682    0.139456\n","12          on   1  1.000000    0.108300\n","15     scoring   1  1.000000    0.108300\n","0        based   1  1.000000    0.108300\n","4         done   1  1.000000    0.108300\n","11          of   0  1.693147    0.000000\n","1       binary   0  1.693147    0.000000\n","13    presence   0  1.693147    0.000000\n","14       score   0  1.693147    0.000000\n","8       higher   0  1.693147    0.000000\n","16       tfidf   0  1.693147    0.000000\n","5        dummy   0  1.693147    0.000000\n","19        word   0  1.693147    0.000000"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>features</th>\n","      <th>tf</th>\n","      <th>idf</th>\n","      <th>norm_tfidf</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>7</th>\n","      <td>frequency</td>\n","      <td>4</td>\n","      <td>1.693147</td>\n","      <td>0.733471</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>vectorizer</td>\n","      <td>4</td>\n","      <td>1.000000</td>\n","      <td>0.433200</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>this</td>\n","      <td>2</td>\n","      <td>1.000000</td>\n","      <td>0.216600</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>for</td>\n","      <td>2</td>\n","      <td>1.000000</td>\n","      <td>0.216600</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>is</td>\n","      <td>2</td>\n","      <td>1.000000</td>\n","      <td>0.216600</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>count</td>\n","      <td>1</td>\n","      <td>1.693147</td>\n","      <td>0.183368</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>doesn</td>\n","      <td>1</td>\n","      <td>1.693147</td>\n","      <td>0.183368</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>key</td>\n","      <td>1</td>\n","      <td>1.287682</td>\n","      <td>0.139456</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>on</td>\n","      <td>1</td>\n","      <td>1.000000</td>\n","      <td>0.108300</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>scoring</td>\n","      <td>1</td>\n","      <td>1.000000</td>\n","      <td>0.108300</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>based</td>\n","      <td>1</td>\n","      <td>1.000000</td>\n","      <td>0.108300</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>done</td>\n","      <td>1</td>\n","      <td>1.000000</td>\n","      <td>0.108300</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>of</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>binary</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>presence</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>score</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>higher</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>tfidf</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>dummy</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>word</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{},"execution_count":103}],"source":["# create dataframe for tfidf vectors for the first document\n","first_document_tfidf = tfidf_vectors[0].toarray().ravel()\n","feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n","df_tfidf = pd.DataFrame({'features': feature_names_tfidf,\n","                        'idf': term_idf, 'norm_tfidf': first_document_tfidf})\n","\n","# combine dataframes\n","\n","# Merge the tf and tf-idf dataframes on the 'features' column\n","df = pd.merge(left=df_tf, right=df_tfidf)\n","\n","# Sort the combined dataframe by the 'norm_tfidf' column in descending order\n","df.sort_values(by=[\"norm_tfidf\"], ascending=False, inplace=True)\n","\n","df\n"]},{"cell_type":"markdown","metadata":{"id":"Ft1Jj6GfxCEo"},"source":["**Observations from above results**\n","- words 'frequency' and 'vectorizer' occurs 4 times in the documsnt and hence term frequency is 4.\n","- Word 'vectorizer' occurs in every document and hence idf is 1 (log(1) + 1).\n","- norm_tfidf gives higher score to word 'frequency' than 'vectorizer'.\n","- norm_tfidf is not equal to idf * tf\n","\n","Let us know understand how norm_tfidf is calculated:"]},{"cell_type":"code","execution_count":104,"metadata":{"execution":{"iopub.execute_input":"2021-09-11T19:08:38.389005Z","iopub.status.busy":"2021-09-11T19:08:38.388843Z","iopub.status.idle":"2021-09-11T19:08:38.392671Z","shell.execute_reply":"2021-09-11T19:08:38.392237Z","shell.execute_reply.started":"2021-09-11T19:08:38.388992Z"},"id":"-k9PB58_xCEo","tags":[],"executionInfo":{"status":"ok","timestamp":1706506328179,"user_tz":360,"elapsed":39,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["# calculate tfidf (without any normalization)\n","df['tfidf'] = df.eval('tf*idf')\n"]},{"cell_type":"code","execution_count":105,"metadata":{"execution":{"iopub.execute_input":"2021-09-12T03:13:47.640165Z","iopub.status.busy":"2021-09-12T03:13:47.639897Z","iopub.status.idle":"2021-09-12T03:13:47.651782Z","shell.execute_reply":"2021-09-12T03:13:47.651488Z","shell.execute_reply.started":"2021-09-12T03:13:47.640137Z"},"id":"Betig4AjxCEp","tags":[],"executionInfo":{"status":"ok","timestamp":1706506329042,"user_tz":360,"elapsed":33,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["# calculate tfidf - normalized\n","df['sq_tfidf'] = df.eval('tfidf**2')\n","df['norm_tfidf_manually'] = df['tfidf']/np.sqrt(df['sq_tfidf'].sum())"]},{"cell_type":"markdown","metadata":{"id":"z1jKODj4iRNQ"},"source":["## <font color = 'pickle'>**Modifying Vocab**"]},{"cell_type":"markdown","metadata":{"id":"UeTUZZmJiccS"},"source":["### <font color = 'pickle'>**Case sensitive**"]},{"cell_type":"code","execution_count":106,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T06:08:17.486661Z","iopub.status.busy":"2021-09-11T06:08:17.486532Z","iopub.status.idle":"2021-09-11T06:08:17.490721Z","shell.execute_reply":"2021-09-11T06:08:17.490336Z","shell.execute_reply.started":"2021-09-11T06:08:17.486649Z"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1706506331427,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"2omkpfjJnZ_L","outputId":"0de93a81-aba5-44a4-f03a-61700cc96f5a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Count': 1,\n"," 'Vectorizer': 3,\n"," 'for': 8,\n"," 'this': 19,\n"," 'vectorizer': 20,\n"," 'scoring': 17,\n"," 'is': 11,\n"," 'done': 6,\n"," 'based': 4,\n"," 'on': 14,\n"," 'frequency': 9,\n"," 'For': 2,\n"," 'key': 12,\n"," 'doesn': 5,\n"," 'tfidf': 18,\n"," 'higher': 10,\n"," 'score': 16,\n"," 'Binary': 0,\n"," 'presence': 15,\n"," 'of': 13,\n"," 'word': 21,\n"," 'dummy': 7}"]},"metadata":{},"execution_count":106}],"source":["# The lowercase argument is set to False to indicate that the text should\n","# not be converted to lowercase before tokenizing.\n","# The resulting vocab may have same word in upper and lower case\n","vectorizer = CountVectorizer(lowercase=False)\n","\n","# we can use fit_transform to use fit() and transform() in one step\n","vectors = vectorizer.fit_transform(Corpus)\n","vectorizer.vocabulary_\n"]},{"cell_type":"markdown","metadata":{"id":"75w3LP5li7VM"},"source":["### <font color = 'pickle'>**Filtering words based on frequency**"]},{"cell_type":"markdown","metadata":{"id":"RP_IKZG9jKvD"},"source":["The `max_df`, `min_df`, and `max_features` parameters in the `CountVectorizer`` class control the feature selection for the resulting term frequency (tf) vectors.\n","\n","- `max_df`: This parameter sets the maximum threshold for the frequency of a term in the document collection. If a term has a document frequency (i.e., the number of documents that contain the term) higher than max_df, it will be ignored. <font color = 'dodgerblue' >**This parameter is used to filter out stop words (corpus specific) that appear in too many documents.** </font>\n","\n","- min_df: This parameter sets the minimum threshold for the frequency of a term in the document collection. If a term has a document frequency lower than min_df, it will be ignored.  <font color = 'dodgerblue' >**This parameter is used to filter out rare words that appear in too few documents.**\n","\n","- max_features: This parameter sets the maximum number of features (i.e., the maximum number of unique terms) that should be included in the resulting tf vectors. If the number of unique terms in the document collection is larger than max_features, the terms with the highest tf values will be kept and the others will be ignored.  <font color = 'dodgerblue' >**This parameter is used to reduce the dimensionality of the resulting tf vectors, which can help reduce the computational cost of downstream processing.**\n","\n","By using the max_df, min_df, and max_features parameters, you can control the feature selection process and determine the most informative terms to include in the tf vectors."]},{"cell_type":"code","execution_count":107,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T06:08:40.000725Z","iopub.status.busy":"2021-09-11T06:08:40.000235Z","iopub.status.idle":"2021-09-11T06:08:40.007239Z","shell.execute_reply":"2021-09-11T06:08:40.006996Z","shell.execute_reply.started":"2021-09-11T06:08:40.000670Z"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1706506338734,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"YxzFCRGEMCkm","outputId":"fc903ed1-639d-4f54-a289-b69fa1cc1c4f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'vectorizer': 8,\n"," 'for': 2,\n"," 'this': 7,\n"," 'scoring': 6,\n"," 'is': 3,\n"," 'done': 1,\n"," 'based': 0,\n"," 'on': 5,\n"," 'key': 4}"]},"metadata":{},"execution_count":107}],"source":["# remove rare words - remove words which appear in less than 2 documents\n","vectorizer = CountVectorizer(min_df=2)\n","vectorizer.fit(Corpus)\n","vectorizer.vocabulary_\n"]},{"cell_type":"code","execution_count":108,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T06:09:31.316083Z","iopub.status.busy":"2021-09-11T06:09:31.315553Z","iopub.status.idle":"2021-09-11T06:09:31.322432Z","shell.execute_reply":"2021-09-11T06:09:31.322174Z","shell.execute_reply.started":"2021-09-11T06:09:31.316023Z"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1706506341162,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"bHDT20dLxCEq","outputId":"e80ad10b-1d72-4672-f16a-b748fff572d7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'count': 1,\n"," 'frequency': 4,\n"," 'key': 6,\n"," 'doesn': 2,\n"," 'tfidf': 10,\n"," 'higher': 5,\n"," 'score': 9,\n"," 'binary': 0,\n"," 'presence': 8,\n"," 'of': 7,\n"," 'word': 11,\n"," 'dummy': 3}"]},"metadata":{},"execution_count":108}],"source":["# remove words which appear in more than 2 documents - remove corpus specific stop words\n","vectorizer = CountVectorizer(max_df=2)\n","vectorizer.fit(Corpus)\n","vectorizer.vocabulary_\n"]},{"cell_type":"code","execution_count":109,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T06:10:33.933882Z","iopub.status.busy":"2021-09-11T06:10:33.933354Z","iopub.status.idle":"2021-09-11T06:10:33.940572Z","shell.execute_reply":"2021-09-11T06:10:33.940289Z","shell.execute_reply.started":"2021-09-11T06:10:33.933825Z"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1706506343717,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"UW_UErB8xCEq","outputId":"f9024c26-8dc0-4d7b-d1f5-bc072ca86b95"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'vectorizer': 4, 'for': 0, 'this': 3, 'is': 1, 'tfidf': 2}"]},"metadata":{},"execution_count":109}],"source":["# retain most frequent words only - retain top n words based on term frequency across corpus\n","vectorizer = CountVectorizer(max_features=5)\n","vectorizer.fit(Corpus)\n","vectorizer.vocabulary_\n"]},{"cell_type":"markdown","metadata":{"id":"F6mnf0ZHxCEq"},"source":["### <font color = 'pickle'>**Stop Words**"]},{"cell_type":"code","execution_count":110,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T06:14:28.793903Z","iopub.status.busy":"2021-09-11T06:14:28.793490Z","iopub.status.idle":"2021-09-11T06:14:28.800896Z","shell.execute_reply":"2021-09-11T06:14:28.800576Z","shell.execute_reply.started":"2021-09-11T06:14:28.793866Z"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1706506352061,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"XZ4dTc6YxCEq","outputId":"ed6dbe4a-5e35-4aef-cac3-24aa6e41057f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'vectorizer': 4, 'done': 1, 'based': 0, 'frequency': 2, 'tfidf': 3}"]},"metadata":{},"execution_count":110}],"source":["# We can also specify list of stopwords to countvectorizer to get the feature without stopwords\n","\n","# Import libraries\n","nltk_stop_words = nltk_stopwords.words('english')\n","\n","vectorizer = CountVectorizer(max_features=5, stop_words=nltk_stop_words)\n","vectorizer.fit(Corpus)\n","vectorizer.vocabulary_\n"]},{"cell_type":"markdown","metadata":{"id":"OWwFq44kxCEq"},"source":["### <font color = 'pickle'>**Custom Tokenizer and Preprocessor**"]},{"cell_type":"markdown","metadata":{"id":"MrVX1WEtxCEq"},"source":["#### <font color = 'pickle'>**nltk tokenizer**"]},{"cell_type":"code","execution_count":111,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-12T03:16:09.705726Z","iopub.status.busy":"2021-09-12T03:16:09.705542Z","iopub.status.idle":"2021-09-12T03:16:09.711195Z","shell.execute_reply":"2021-09-12T03:16:09.710860Z","shell.execute_reply.started":"2021-09-12T03:16:09.705710Z"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1706506356666,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"XrmIi5seqljd","outputId":"fc7f5ff0-f133-4db5-ea77-0ef3977c22fc","tags":[]},"outputs":[{"output_type":"stream","name":"stderr","text":["/home/harpreet/mambaforge/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["{'count': 11,\n"," 'vectorizer': 28,\n"," '-': 4,\n"," 'for': 15,\n"," 'this': 27,\n"," ',': 3,\n"," 'scoring': 24,\n"," 'is': 18,\n"," 'done': 13,\n"," 'based': 9,\n"," 'on': 21,\n"," 'frequency': 16,\n"," '.': 5,\n"," 'key': 19,\n"," '@vectorizer': 8,\n"," '#frequency': 1,\n"," '@frequency': 7,\n"," 'doesn': 12,\n"," '’': 30,\n"," 't': 25,\n"," 'tfidf': 26,\n"," 'higher': 17,\n"," 'score': 23,\n"," '#tfidf': 2,\n"," 'binary': 10,\n"," 'presence': 22,\n"," 'of': 20,\n"," 'word': 29,\n"," 'dummy': 14,\n"," '#dummy': 0,\n"," '@dummy': 6}"]},"metadata":{},"execution_count":111}],"source":["# We can use custom tokenizer e.g. we can use nltk tweet tokenizer to get each tokens as feature\n","\n","# Create an instance of the TweetTokenizer class\n","tweet_tokenizer = TweetTokenizer()\n","\n","# Initialize the CountVectorizer with the custom tokenizer\n","# only works if analyzer = 'word'\n","vectorizer = CountVectorizer(\n","    analyzer='word', tokenizer=tweet_tokenizer.tokenize)\n","\n","vectorizer.fit_transform(Corpus)\n","vectorizer.vocabulary_"]},{"cell_type":"markdown","metadata":{"id":"nMzV-qHwxCEr"},"source":["#### <font color = 'pickle'>**spacy pre-processor and tokenizer**"]},{"cell_type":"code","execution_count":112,"metadata":{"execution":{"iopub.execute_input":"2021-09-12T03:16:12.204316Z","iopub.status.busy":"2021-09-12T03:16:12.204153Z","iopub.status.idle":"2021-09-12T03:16:12.207192Z","shell.execute_reply":"2021-09-12T03:16:12.206765Z","shell.execute_reply.started":"2021-09-12T03:16:12.204303Z"},"id":"nmyYzK9AVzXw","tags":[],"executionInfo":{"status":"ok","timestamp":1706506361628,"user_tz":360,"elapsed":45,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["def spacy_preprocessor(text):\n","\n","    # Create spacy object\n","    doc = nlp(text)\n","\n","    # remove punctuations and get a list of tokens\n","    filtered_text = [token.text for token in doc if not token.is_punct]\n","\n","    # join the processed tokens in to string\n","    return \" \".join(filtered_text)\n"]},{"cell_type":"code","execution_count":113,"metadata":{"execution":{"iopub.execute_input":"2021-09-12T03:16:12.863124Z","iopub.status.busy":"2021-09-12T03:16:12.862607Z","iopub.status.idle":"2021-09-12T03:16:12.872726Z","shell.execute_reply":"2021-09-12T03:16:12.870950Z","shell.execute_reply.started":"2021-09-12T03:16:12.863065Z"},"id":"YnFartqtWe6X","tags":[],"executionInfo":{"status":"ok","timestamp":1706506366026,"user_tz":360,"elapsed":27,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["# Spacy Tokenizer\n","def spacy_tokenizer(data):\n","    doc = nlp(data)\n","    return [token.text for token in doc]\n"]},{"cell_type":"code","execution_count":114,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T20:36:02.024537Z","iopub.status.busy":"2021-09-11T20:36:02.024028Z","iopub.status.idle":"2021-09-11T20:36:02.043042Z","shell.execute_reply":"2021-09-11T20:36:02.042678Z","shell.execute_reply.started":"2021-09-11T20:36:02.024478Z"},"executionInfo":{"elapsed":51,"status":"ok","timestamp":1706506367193,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"S0GAbq-3WsT_","outputId":"95c958f5-9b17-4551-f8a2-28cc29889cc3","tags":[]},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Count': 5,\n"," 'Vectorizer': 7,\n"," 'for': 12,\n"," 'this': 24,\n"," 'vectorizer': 25,\n"," 'scoring': 22,\n"," 'is': 15,\n"," 'done': 10,\n"," 'based': 8,\n"," 'on': 19,\n"," 'frequency': 13,\n"," 'For': 6,\n"," 'key': 16,\n"," '@vectorizer': 3,\n"," '@frequency': 2,\n"," 'does': 9,\n"," 'n’t': 17,\n"," 'tfidf': 23,\n"," '  ': 0,\n"," 'higher': 14,\n"," 'score': 21,\n"," 'Binary': 4,\n"," 'presence': 20,\n"," 'of': 18,\n"," 'word': 26,\n"," 'dummy': 11,\n"," '@dummy': 1}"]},"metadata":{},"execution_count":114}],"source":["# custom preprocessor and spacy tokenizer\n","vectorizer = CountVectorizer(analyzer='word', preprocessor=spacy_preprocessor,\n","                             tokenizer=spacy_tokenizer, token_pattern=None)\n","vectors = vectorizer.fit(Corpus)\n","vectorizer.vocabulary_\n"]},{"cell_type":"markdown","metadata":{"id":"uSSOPlCRK38o"},"source":["#### <font color = 'pickle'>**custom preprocessor we created earlier**"]},{"cell_type":"code","source":["cp.SpacyPreprocessor??"],"metadata":{"id":"3xWMqbmm1PJl","executionInfo":{"status":"ok","timestamp":1706506376441,"user_tz":360,"elapsed":45,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"execution_count":115,"outputs":[]},{"cell_type":"code","execution_count":116,"metadata":{"id":"EcX3HWLFF2qU","executionInfo":{"status":"ok","timestamp":1706506380630,"user_tz":360,"elapsed":31,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["custom_preprocessor = cp.SpacyPreprocessor('en_core_web_sm')\n"]},{"cell_type":"markdown","source":[],"metadata":{"id":"GTTqFaEEhaQv"}},{"cell_type":"code","execution_count":117,"metadata":{"id":"BKUl2hjTI1Ig","executionInfo":{"status":"ok","timestamp":1706506382093,"user_tz":360,"elapsed":28,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["def spacy_preprocessor(text):\n","    filtered_text = custom_preprocessor.transform([text])\n","    return \" \".join(filtered_text)\n"]},{"cell_type":"code","execution_count":119,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1268,"status":"ok","timestamp":1706506404600,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"iy5XeSb6GHwU","outputId":"7ad37596-d530-420d-b9ac-4a83749c3e19"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'count': 2,\n"," 'vectorizer': 11,\n"," 'scoring': 9,\n"," 'base': 0,\n"," 'frequency': 4,\n"," 'key': 6,\n"," 'tfidf': 10,\n"," 'high': 5,\n"," 'score': 8,\n"," 'binary': 1,\n"," 'presence': 7,\n"," 'word': 12,\n"," 'dummy': 3}"]},"metadata":{},"execution_count":119}],"source":["# custom preprocessor and spacy tokenizer\n","vectorizer = CountVectorizer(analyzer='word', preprocessor=spacy_preprocessor,\n","                            token_pattern=r\"[\\S]+\")\n","vectors = vectorizer.fit(Corpus)\n","vectorizer.vocabulary_\n"]},{"cell_type":"markdown","metadata":{"id":"STiVvP5qxCEr"},"source":["#### <font color = 'pickle'>**token patterns with regular expressions**"]},{"cell_type":"code","execution_count":120,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T06:32:57.889450Z","iopub.status.busy":"2021-09-11T06:32:57.888935Z","iopub.status.idle":"2021-09-11T06:32:57.893152Z","shell.execute_reply":"2021-09-11T06:32:57.892901Z","shell.execute_reply.started":"2021-09-11T06:32:57.889434Z"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1706506407129,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"2xEae96cC5e3","outputId":"f30c3559-5c7e-4314-92e3-e36fdefb90ec"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'count': 9,\n"," 'vectorizer': 28,\n"," '-': 3,\n"," 'for': 13,\n"," 'this': 27,\n"," 'vectorizer,': 29,\n"," 'scoring': 24,\n"," 'is': 17,\n"," 'done': 11,\n"," 'based': 7,\n"," 'on': 21,\n"," 'frequency.': 15,\n"," 'frequency': 14,\n"," 'key.': 19,\n"," '@vectorizer': 6,\n"," '#frequency': 1,\n"," '@frequency,': 5,\n"," 'doesn’t': 10,\n"," 'tfidf': 25,\n"," 'tfidf,': 26,\n"," 'higher': 16,\n"," 'score': 23,\n"," '#tfidf': 2,\n"," 'binary': 8,\n"," 'presence': 22,\n"," 'of': 20,\n"," 'word.': 30,\n"," 'dummy': 12,\n"," 'key': 18,\n"," '#dummy': 0,\n"," '@dummy': 4}"]},"metadata":{},"execution_count":120}],"source":["# We can pass regex to the argument token_pattern to get required pattern\n","# whitespace tokenizer\n","# This can be very useful if we have allready cleaned the text\n","vectorizer = CountVectorizer(analyzer='word', token_pattern=r\"[\\S]+\")\n","\n","# Assign the encoded(transformed) vectors to a variable\n","vectors = vectorizer.fit_transform(Corpus)\n","\n","vectorizer.vocabulary_\n"]},{"cell_type":"markdown","metadata":{"id":"haBuVWSyxCEs"},"source":["### <font color = 'pickle'>**ngrams**</font>\n","\n","- Till now our features consists of single token. However, in some cases we may want to use sequence of tokens as features\n","- Consider the following corpus\n"," 1. This item is good\n"," 2. This item is not good\n","- Now  both the documents will have feature 'good' and 'not' will be an additional feature in document 2.\n","- For applications like sentiment analysis - it might be a good idea to consider 'not good' as a single token.\n","\n","- We can use ngram_range(min_n, max_n) in CountVectorizer to create features that consists of sequence of words.\n","\n","- if we specify min_n = 2 and max_n = 3, we will get bigrams and trigrams as features."]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T06:47:29.553885Z","iopub.status.busy":"2021-09-11T06:47:29.553741Z","iopub.status.idle":"2021-09-11T06:47:29.559538Z","shell.execute_reply":"2021-09-11T06:47:29.559122Z","shell.execute_reply.started":"2021-09-11T06:47:29.553873Z"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1706503595430,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"Ix4jKILgDLxa","outputId":"fc7f4b34-4600-4269-8786-b1511d8d5178","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["Features for text 1\n","\n","is not\n","item is\n","not good\n","this item\n","\n","Features for text 2\n","\n","is terribly\n","item is\n","terribly good\n","this item\n"]}],"source":["min_n = 2\n","max_n = 2\n","\n","# only works if analyzer = 'word'\n","vectorizer1 = CountVectorizer(analyzer='word', ngram_range=(min_n, max_n))\n","vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(min_n, max_n))\n","\n","text1 = [\"This item is not good\"]\n","text2 = [\"This item is terribly good\"]\n","\n","# Fit the vectorizer to text\n","vectorizer1.fit_transform(text1)\n","vectorizer2.fit_transform(text2)\n","\n","features1 = vectorizer1.get_feature_names_out()\n","features2 = vectorizer2.get_feature_names_out()\n","\n","print('Features for text 1\\n')\n","for feature in features1:\n","    print(feature)\n","\n","print(f'\\nFeatures for text 2\\n')\n","for feature in features2:\n","    print(feature)\n"]},{"cell_type":"markdown","metadata":{"id":"7eaVPcHeRXRl"},"source":["## <font color = 'pickle'>**Example : IMDB Data set**"]},{"cell_type":"markdown","metadata":{"id":"wr_e08FxxCEs"},"source":["### <font color = 'pickle'>**Import Data**"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2021-09-11T07:45:50.456908Z","iopub.status.busy":"2021-09-11T07:45:50.456747Z","iopub.status.idle":"2021-09-11T07:45:50.459467Z","shell.execute_reply":"2021-09-11T07:45:50.459100Z","shell.execute_reply.started":"2021-09-11T07:45:50.456895Z"},"id":"TnakQosgOsMY","tags":[],"executionInfo":{"status":"ok","timestamp":1706503607291,"user_tz":360,"elapsed":36,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["# Use train.csv of IMDB movie review data (we downloaded this in the last lecture)\n","base_folder = Path(basepath)\n","data_folder = base_folder/'datasets'\n","train_data = data_folder / 'aclImdb'/'train.csv'\n","test_data = data_folder / 'aclImdb'/'test.csv'\n"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"execution":{"iopub.execute_input":"2021-09-11T07:53:35.249495Z","iopub.status.busy":"2021-09-11T07:53:35.249349Z","iopub.status.idle":"2021-09-11T07:53:35.584630Z","shell.execute_reply":"2021-09-11T07:53:35.584212Z","shell.execute_reply.started":"2021-09-11T07:53:35.249483Z"},"executionInfo":{"elapsed":413,"status":"ok","timestamp":1706503617189,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"2CgPhuOixCEs","outputId":"5f3f0b4c-f674-4bf1-f3b9-8ea84e9d7d51","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of Training data set is : (25000, 2)\n","Shape of Test data set is : (25000, 2)\n","\n","Top five rows of Training data set:\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["                                             Reviews  Labels\n","0  Ever wanted to know just how much Hollywood co...       1\n","1  The movie itself was ok for the kids. But I go...       1\n","2  You could stage a version of Charles Dickens' ...       1\n","3  this was a fantastic episode. i saw a clip fro...       1\n","4  and laugh out loud funny in many scenes.<br />...       1"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Reviews</th>\n","      <th>Labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Ever wanted to know just how much Hollywood co...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The movie itself was ok for the kids. But I go...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>You could stage a version of Charles Dickens' ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>this was a fantastic episode. i saw a clip fro...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>and laugh out loud funny in many scenes.&lt;br /&gt;...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{},"execution_count":50}],"source":["# Reading data\n","train_df = pd.read_csv(train_data, index_col=0)\n","test_df = pd.read_csv(test_data, index_col=0)\n","print(f'Shape of Training data set is : {train_df.shape}')\n","print(f'Shape of Test data set is : {test_df.shape}')\n","print(f'\\nTop five rows of Training data set:\\n')\n","train_df.head()\n"]},{"cell_type":"markdown","metadata":{"id":"FDh2M0G9xCEs"},"source":["### <font color = 'pickle'>**Generating Vocab**</font>\n","- <font color = 'indianred'>**Vocab should be created only based on training dataset**</font>\n","- We will generate vocab using CountVectorizer\n","- <font color = 'indianred'>**Use fit_transform() on Training data set**.\n","- **Use only transform() on Test dataset**. This make sures that we generate vocab only based on training dataset."]},{"cell_type":"code","execution_count":121,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":161},"execution":{"iopub.execute_input":"2021-09-11T07:46:54.988708Z","iopub.status.busy":"2021-09-11T07:46:54.988551Z","iopub.status.idle":"2021-09-11T07:46:57.356248Z","shell.execute_reply":"2021-09-11T07:46:57.355769Z","shell.execute_reply.started":"2021-09-11T07:46:54.988695Z"},"executionInfo":{"elapsed":2474,"status":"ok","timestamp":1706506470944,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"rudMW4B_UGVE","outputId":"2f2bd06b-690a-4bbd-cfc8-df72492379b2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CountVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n","                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n","                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n","                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n","                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n","                            'itself', ...])"],"text/html":["<style>#sk-container-id-10 {color: black;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;,\n","                            &#x27;ourselves&#x27;, &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n","                            &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;, &#x27;yourselves&#x27;,\n","                            &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;,\n","                            &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n","                            &#x27;itself&#x27;, ...])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;,\n","                            &#x27;ourselves&#x27;, &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n","                            &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;, &#x27;yourselves&#x27;,\n","                            &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;,\n","                            &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n","                            &#x27;itself&#x27;, ...])</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":121}],"source":["# Initialize vectorizer\n","nltk_stop_words = nltk_stopwords.words('english')\n","bag_of_word = CountVectorizer(stop_words=nltk_stop_words)\n","\n","# Fit on training data\n","bag_of_word.fit(train_df['Reviews'].values)\n"]},{"cell_type":"code","execution_count":122,"metadata":{"execution":{"iopub.execute_input":"2021-09-11T07:47:13.492919Z","iopub.status.busy":"2021-09-11T07:47:13.492761Z","iopub.status.idle":"2021-09-11T07:47:13.527237Z","shell.execute_reply":"2021-09-11T07:47:13.526788Z","shell.execute_reply.started":"2021-09-11T07:47:13.492905Z"},"id":"6DsUyK2LVCg4","tags":[],"executionInfo":{"status":"ok","timestamp":1706506475057,"user_tz":360,"elapsed":65,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["# get feature names\n","features = bag_of_word.get_feature_names_out()\n"]},{"cell_type":"code","execution_count":123,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T07:47:20.015894Z","iopub.status.busy":"2021-09-11T07:47:20.015734Z","iopub.status.idle":"2021-09-11T07:47:20.019068Z","shell.execute_reply":"2021-09-11T07:47:20.018602Z","shell.execute_reply.started":"2021-09-11T07:47:20.015881Z"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1706506475991,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"yIDLMOzoxCEt","outputId":"39e5c394-231a-4c31-e526-557f9ef896eb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["74704"]},"metadata":{},"execution_count":123}],"source":["# check the legth of the vocab\n","len(features)\n"]},{"cell_type":"markdown","metadata":{"id":"MxjTROQkxCEt"},"source":["### <font color = 'pickle'>**Create vectors for reviews**"]},{"cell_type":"code","execution_count":124,"metadata":{"execution":{"iopub.execute_input":"2021-09-11T08:00:01.403847Z","iopub.status.busy":"2021-09-11T08:00:01.403637Z","iopub.status.idle":"2021-09-11T08:00:05.617042Z","shell.execute_reply":"2021-09-11T08:00:05.616588Z","shell.execute_reply.started":"2021-09-11T08:00:01.403825Z"},"id":"sUvgnrjZVJYn","executionInfo":{"status":"ok","timestamp":1706506482643,"user_tz":360,"elapsed":4517,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["# Transform the training and test dataset\n","bow_vector_train = bag_of_word.transform(train_df['Reviews'].values)\n","bow_vector_test = bag_of_word.transform(test_df['Reviews'].values)\n"]},{"cell_type":"code","execution_count":125,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T08:00:08.442306Z","iopub.status.busy":"2021-09-11T08:00:08.442071Z","iopub.status.idle":"2021-09-11T08:00:08.446901Z","shell.execute_reply":"2021-09-11T08:00:08.446266Z","shell.execute_reply.started":"2021-09-11T08:00:08.442282Z"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1706506483975,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"6xk4M3NPxCEt","outputId":"72d83048-915c-44c1-88cc-a8f54bd28fcb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<25000x74704 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 2479678 stored elements in Compressed Sparse Row format>"]},"metadata":{},"execution_count":125}],"source":["# Shape of the matrix for train dataset\n","bow_vector_train\n"]},{"cell_type":"code","execution_count":126,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T08:00:56.304151Z","iopub.status.busy":"2021-09-11T08:00:56.303619Z","iopub.status.idle":"2021-09-11T08:00:56.314758Z","shell.execute_reply":"2021-09-11T08:00:56.312719Z","shell.execute_reply.started":"2021-09-11T08:00:56.304091Z"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1706506486323,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"_LALNShqxCEt","outputId":"71ad1cfb-fda2-4807-de03-7820d57b5fbb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<25000x74704 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 2385031 stored elements in Compressed Sparse Row format>"]},"metadata":{},"execution_count":126}],"source":["# Shape of the matrix for test dataset\n","bow_vector_test\n"]},{"cell_type":"markdown","metadata":{"id":"uaUcb2LBXBhw"},"source":["### <font color = 'pickle'>**Limit vocab using max_features**\n","We got 25k rows with 78k+ features, but what if we want only top 5k features.\n","We can do this by providing max_features parameter."]},{"cell_type":"code","execution_count":127,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"zR0AfavvHeQn","executionInfo":{"status":"ok","timestamp":1706506492872,"user_tz":360,"elapsed":2301,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"4d30eeef-d6b1-48a2-fec7-706a7cef2ccd"},"outputs":[{"output_type":"stream","name":"stderr","text":["/home/harpreet/mambaforge/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["CountVectorizer(max_features=5000,\n","                stop_words=['another', 'might', 'it', 'somewhere', 'more',\n","                            'their', 'out', 'is', 'them', 'beyond', 'with',\n","                            'somehow', 'doing', 'been', 'latterly', 'wherein',\n","                            'nine', 'seemed', 'else', 'seem', 'why', 'sometime',\n","                            'beforehand', 'his', 'besides', 'among', 'fifteen',\n","                            'keep', 'never', 'those', ...])"],"text/html":["<style>#sk-container-id-11 {color: black;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(max_features=5000,\n","                stop_words=[&#x27;another&#x27;, &#x27;might&#x27;, &#x27;it&#x27;, &#x27;somewhere&#x27;, &#x27;more&#x27;,\n","                            &#x27;their&#x27;, &#x27;out&#x27;, &#x27;is&#x27;, &#x27;them&#x27;, &#x27;beyond&#x27;, &#x27;with&#x27;,\n","                            &#x27;somehow&#x27;, &#x27;doing&#x27;, &#x27;been&#x27;, &#x27;latterly&#x27;, &#x27;wherein&#x27;,\n","                            &#x27;nine&#x27;, &#x27;seemed&#x27;, &#x27;else&#x27;, &#x27;seem&#x27;, &#x27;why&#x27;, &#x27;sometime&#x27;,\n","                            &#x27;beforehand&#x27;, &#x27;his&#x27;, &#x27;besides&#x27;, &#x27;among&#x27;, &#x27;fifteen&#x27;,\n","                            &#x27;keep&#x27;, &#x27;never&#x27;, &#x27;those&#x27;, ...])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_features=5000,\n","                stop_words=[&#x27;another&#x27;, &#x27;might&#x27;, &#x27;it&#x27;, &#x27;somewhere&#x27;, &#x27;more&#x27;,\n","                            &#x27;their&#x27;, &#x27;out&#x27;, &#x27;is&#x27;, &#x27;them&#x27;, &#x27;beyond&#x27;, &#x27;with&#x27;,\n","                            &#x27;somehow&#x27;, &#x27;doing&#x27;, &#x27;been&#x27;, &#x27;latterly&#x27;, &#x27;wherein&#x27;,\n","                            &#x27;nine&#x27;, &#x27;seemed&#x27;, &#x27;else&#x27;, &#x27;seem&#x27;, &#x27;why&#x27;, &#x27;sometime&#x27;,\n","                            &#x27;beforehand&#x27;, &#x27;his&#x27;, &#x27;besides&#x27;, &#x27;among&#x27;, &#x27;fifteen&#x27;,\n","                            &#x27;keep&#x27;, &#x27;never&#x27;, &#x27;those&#x27;, ...])</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":127}],"source":["# Limit Vocab size using Max features\n","spacy_stop_words = nlp.Defaults.stop_words\n","bag_of_word = CountVectorizer(\n","    max_features=5000, stop_words=list(spacy_stop_words))  # Max features\n","\n","# Fit on training data\n","bag_of_word.fit(train_df['Reviews'].values)\n"]},{"cell_type":"code","execution_count":128,"metadata":{"execution":{"iopub.execute_input":"2021-09-11T08:02:57.092677Z","iopub.status.busy":"2021-09-11T08:02:57.092551Z","iopub.status.idle":"2021-09-11T08:03:01.788423Z","shell.execute_reply":"2021-09-11T08:03:01.787845Z","shell.execute_reply.started":"2021-09-11T08:02:57.092664Z"},"id":"rEq04frRXq_i","executionInfo":{"status":"ok","timestamp":1706506501281,"user_tz":360,"elapsed":4524,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["# Transform the training and test dataset\n","bow_vector_train = bag_of_word.transform(train_df['Reviews'].values)\n","bow_vector_test = bag_of_word.transform(train_df['Reviews'].values)\n"]},{"cell_type":"code","execution_count":129,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"execution":{"iopub.execute_input":"2021-09-11T08:03:51.691500Z","iopub.status.busy":"2021-09-11T08:03:51.691259Z","iopub.status.idle":"2021-09-11T08:03:51.887702Z","shell.execute_reply":"2021-09-11T08:03:51.887320Z","shell.execute_reply.started":"2021-09-11T08:03:51.691476Z"},"executionInfo":{"elapsed":246,"status":"ok","timestamp":1706506502539,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"6EZxdHGaY2Ha","outputId":"3eb6d350-723e-4d3a-e9cf-a52222f7ee48"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       00  000  10  100  11  12  13  13th  14  15  ...  yesterday  york   \n","0       0    0   0    0   0   0   0     0   0   0  ...          0     0  \\\n","1       0    0   0    0   0   0   0     0   0   0  ...          0     0   \n","2       0    0   0    0   0   0   0     0   0   0  ...          0     1   \n","3       0    0   0    0   0   0   0     0   0   0  ...          0     0   \n","4       0    0   0    0   0   0   0     0   0   0  ...          0     0   \n","...    ..  ...  ..  ...  ..  ..  ..   ...  ..  ..  ...        ...   ...   \n","24995   0    0   0    0   0   0   0     0   0   0  ...          0     0   \n","24996   0    0   0    0   0   0   0     0   0   0  ...          0     0   \n","24997   0    0   0    0   0   0   0     0   0   1  ...          0     0   \n","24998   0    0   1    0   0   0   0     0   0   0  ...          0     0   \n","24999   0    0   0    0   0   0   0     0   0   0  ...          0     0   \n","\n","       young  younger  youth  zero  zizek  zombie  zombies  zone  \n","0          1        0      0     0      0       0        0     0  \n","1          0        0      0     0      0       0        0     0  \n","2          0        0      0     0      0       0        0     0  \n","3          0        0      0     0      0       0        0     0  \n","4          0        0      0     0      0       0        0     0  \n","...      ...      ...    ...   ...    ...     ...      ...   ...  \n","24995      0        0      0     0      0       0        0     0  \n","24996      0        0      0     0      0       0        0     0  \n","24997      0        1      0     0      0       0        0     0  \n","24998      0        0      0     0      0       0        0     0  \n","24999      0        0      0     0      0       0        0     0  \n","\n","[25000 rows x 5000 columns]"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>00</th>\n","      <th>000</th>\n","      <th>10</th>\n","      <th>100</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>13th</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>...</th>\n","      <th>yesterday</th>\n","      <th>york</th>\n","      <th>young</th>\n","      <th>younger</th>\n","      <th>youth</th>\n","      <th>zero</th>\n","      <th>zizek</th>\n","      <th>zombie</th>\n","      <th>zombies</th>\n","      <th>zone</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>24995</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24996</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24997</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24998</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24999</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>25000 rows × 5000 columns</p>\n","</div>"]},"metadata":{},"execution_count":129}],"source":["# Document representation\n","vocab = bag_of_word.get_feature_names_out()\n","pd.DataFrame(bow_vector_train.toarray(), columns=vocab)\n"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}